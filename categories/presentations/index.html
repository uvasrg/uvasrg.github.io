<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Presentations | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
        <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
	      
	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      
<div style="margin-top:16px; margin-left: auto; margin-right: auto; max-width: 800px;">
<div class="row">
  <div class="column small-12">
    
    
    <h2><a href="/robustrepresentations/">Adversarially Robust Representations</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2020-08-14 00:00:00 &#43;0000 UTC" itemprop="datePublished">14 August 2020</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/icml">ICML</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/sicheng-zhu">Sicheng Zhu</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>
    
  </span>
  
  
</div>


<p><em>Post by Sicheng Zhu</em></p>
<p>With the rapid development of deep learning and the explosive growth
of unlabeled data, <a href="https://arxiv.org/abs/1206.5538">representation
learning</a> is becoming increasingly
important. It has made impressive applications such as pre-trained
language models (e.g., <a href="https://arxiv.org/abs/1810.04805">BERT</a> and
<a href="https://github.com/openai/gpt-3">GPT-3</a>).</p>
<p>Popular as it is, representation learning raises concerns about the
robustness of learned representations under adversarial settings. For
example, <em>how can we compare the robustness to different
representations</em>, and <em>how can we build representations that enable
robust downstream classifiers</em>?</p>
<p class="text-right"><a href="/robustrepresentations/">Read More…</a></p>
	

    
    <h2><a href="/intrinsic-robustness-using-conditional-gans/">Intrinsic Robustness using Conditional GANs</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2020-08-14 00:00:00 &#43;0000 UTC" itemprop="datePublished">14 August 2020</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/aistats">AISTATS</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jinghui-chen">Jinghui Chen</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/quanquan-gu">Quanquan Gu</a>
    
  </span>
  
  
</div>


<p>The video of Xiao&rsquo;s presentation for AISTATS 2020 is now available:
<a href="https://slideslive.com/38930305/understanding-the-intrinsic-robustness-of-image-distributions-using-conditional-generative-models"><em>Understanding the Intrinsic Robustness of Image Distributions using Conditional Generative Models</em></a></p>
<p>Starting with Gilmer et al. (2018), several works have demonstrated
the inevitability of adversarial examples based on different
assumptions about the underlying input probability space. It remains
unclear, however, whether these results apply to natural image
distributions. In this work, we assume the underlying data
distribution is captured by some conditional generative model, and
prove intrinsic robustness bounds for a general class of classifiers,
which solves an open problem in Fawzi et al. (2018). Building upon the
state-of-the-art conditional generative models, we study the intrinsic
robustness of two common image benchmarks under <em>l</em><sub>2</sub>
perturbations, and show the existence of a large gap between the
robustness limits implied by our theory and the adversarial robustness
achieved by current state-of-the-art robust models.</p>
<p class="text-right"><a href="/intrinsic-robustness-using-conditional-gans/">Read More…</a></p>
	

    
    <h2><a href="/hybrid-batch-attacks-at-usenix-security-2020/">Hybrid Batch Attacks at USENIX Security 2020</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2020-08-13 00:00:00 &#43;0000 UTC" itemprop="datePublished">13 August 2020</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yuan-tian">Yuan Tian</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jianfeng-chi">Jianfeng Chi</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/usenix-security">USENIX Security</a>
    
  </span>
  
  
</div>


<p>Here&rsquo;s the video for Suya&rsquo;s presentation on <a href="/usenix-security-2020-hybrid-batch-attacks">Hybrid Batch Attacks</a> at USENIX Security 2020:</p>
<center>
  <video width="90%" id="usenix-media-video-1" data-setup="{}" poster="" class="video-js vjs-default-skin vjs-big-play-centered" preload="auto" controls>
    <source src='https://2459d6dc103cb5933875-c0245c5c937c5dedcca3f1764ecc9b2f.ssl.cf2.rackcdn.com/sec20/videos/0813/s5_machine_learning_1/3_sec20summer-paper412-presentation-video.mp4' type='video/mp4; codecs="avc1.42E01E, mp4a.40.2"'>
  </video><br> 
<a href="https://2459d6dc103cb5933875-c0245c5c937c5dedcca3f1764ecc9b2f.ssl.cf2.rackcdn.com/sec20/videos/0813/s5_machine_learning_1/3_sec20summer-paper412-presentation-video.mp4">Download Video [mp4]</a></p>
</center>
<p><a href="/usenix-security-2020-hybrid-batch-attacks">Blog Post</a><br>
Paper: [<a href="/docs/hybrid_attack.pdf">PDF</a>] [<a href="https://arxiv.org/abs/1908.07000">arXiv</a>]</p>

	

    
    <h2><a href="/congratulations-dr.-xu/">Congratulations Dr. Xu!</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-04-15 00:00:00 &#43;0000 UTC" itemprop="datePublished">15 April 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/weilin-xu">Weilin Xu</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/alumni">alumni</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/phd-defense">PhD defense</a>
    
  </span>
  
  
</div>


<p>Congratulations to Weilin Xu for successfully defending his PhD Thesis!</p>
<center>
<a href="/images/weilin-defense-IMG_4702.jpg"><img src="/images/weilin-defense-IMG_4702-2.jpg" width="70%"></a>
<div class="caption"><center>
Weilin's Committee: <A href="http://faculty.virginia.edu/alemzadeh/">Homa Alemzadeh</a>, <a href="https://www.cs.virginia.edu/yanjun/">Yanjun Qi</a>, <a href="http://patrickmcdaniel.org/">Patrick McDaniel</a> (on screen)</a>, <a href="https://www.cs.virginia.edu/evans">David Evans</a>, <a href="http://vicenteordonez.com/">Vicente Ordóñez Román</a></center>
</div>
</center>
<center>
<em>Improving Robustness of Machine Learning Models using Domain Knowledge</em>
</center>
<p>Although machine learning techniques have achieved great success in
many areas, such as computer vision, natural language processing, and
computer security, recent studies have shown that they are not robust
under attack. A motivated adversary is often able to craft input
samples that force a machine learning model to produce incorrect
predictions, even if the target model achieves high accuracy on normal
test inputs. This raises great concern when machine learning models
are deployed for security-sensitive tasks.</p>
<p class="text-right"><a href="/congratulations-dr.-xu/">Read More…</a></p>
	

    
    <div class="row">
  <div class="column small-12">
    <ul class="pagination" role="navigation" aria-label="Pagination">
      
      <li><span>Page 1 of 1</span></li>      
      
    </ul>    
    All Posts by <a href="//uvasrg.github.io/categories">Category</a> or <a href="//uvasrg.github.io/tags">Tags</a>.

  </div>
</div>

  </div>
</div>
</div>

    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
