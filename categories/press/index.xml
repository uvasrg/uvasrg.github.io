<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Press on Security Research Group</title>
    <link>//uvasrg.github.io/categories/press/</link>
    <description>Recent content in Press on Security Research Group</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Tue, 26 Nov 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/categories/press/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Reassessing EMNLP 2024’s Best Paper: Does Divergence-Based Calibration for Membership Inference Attacks Hold Up?</title>
      <link>//uvasrg.github.io/reassessing-emnlp-2024s-best-paper-does-divergence-based-calibration-for-membership-inference-attacks-hold-up/</link>
      <pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/reassessing-emnlp-2024s-best-paper-does-divergence-based-calibration-for-membership-inference-attacks-hold-up/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.anshumansuri.com/&#34;&gt;Anshuman Suri&lt;/a&gt; and &lt;a href=&#34;https://pratyushmaini.github.io/&#34;&gt;Pratyush Maini&lt;/a&gt; wrote a blog about the EMNLP 2024 best paper award winner: &lt;a href=&#34;https://www.anshumansuri.com/blog/2024/calibrated-mia/&#34;&gt;&lt;em&gt;Reassessing EMNLP 2024’s Best Paper: Does Divergence-Based Calibration for Membership Inference Attacks Hold Up?&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;As we explored in &lt;a href=&#34;https://uvasrg.github.io/do-membership-inference-attacks-work-on-large-language-models/&#34;&gt;&lt;em&gt;Do Membership Inference Attacks Work on Large Language Models?&lt;/em&gt;&lt;/a&gt;, to test a membership inference attack it is essentail to have a candidate set where the members and non-members are from the same distribution. If the distributions are different, the ability of an attack to distinguish members and non-members is indicative of distribution inference, not necessarily membership inference.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Common Way To Test for Leaks in Large Language Models May Be Flawed</title>
      <link>//uvasrg.github.io/common-way-to-test-for-leaks-in-large-language-models-may-be-flawed/</link>
      <pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/common-way-to-test-for-leaks-in-large-language-models-may-be-flawed/</guid>
      <description>&lt;p&gt;UVA News has an article on our LLM membership inference work:&#xA;&lt;a href=&#34;https://engineering.virginia.edu/news-events/news/common-way-test-leaks-large-language-models-may-be-flawed&#34;&gt;&lt;em&gt;Common Way To Test for Leaks in Large Language Models May Be Flawed: UVA Researchers Collaborated To Study the Effectiveness of Membership Inference Attacks&lt;/em&gt;&lt;/a&gt;, Eric Williamson, 13 November 2024.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Poisoning LLMs</title>
      <link>//uvasrg.github.io/poisoning-llms/</link>
      <pubDate>Wed, 21 Aug 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/poisoning-llms/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m quoted in this story by Rob Lemos about poisoning code models (the &lt;a href=&#34;https://www.usenix.org/conference/usenixsecurity24/presentation/yan&#34;&gt;CodeBreaker&lt;/a&gt; paper in USENIX Security 2024 by Shenao Yan, Shen Wang, Yue Duan, Hanbin Hong, Kiho Lee, Doowon Kim, and Yuan Hong), that considers a similar threat to our &lt;a href=&#34;https://uvasrg.github.io/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/&#34;&gt;TrojanPuzzle&lt;/a&gt; work:&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.darkreading.com/application-security/researchers-turn-code-completion-llms-into-attack-tools&#34;&gt;&lt;em&gt;Researchers Highlight How Poisoned LLMs Can Suggest Vulnerable Code&lt;/em&gt;&lt;/a&gt;&lt;br&gt;&#xA;Dark Reading, 20 August 2024&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;CodeBreaker uses code transformations to create vulnerable code that continues to function as expected, but that will not be detected by major static analysis security testing. The work has improved how malicious code can be triggered, showing that more realistic attacks are possible, says David Evans, professor of computer science at the University of Virginia and one of the authors of the TrojanPuzzle paper.&#xA;...&#xA;Developers can take more care as well, viewing code suggestions — whether from an AI or from the Internet — with a critical eye. In addition, developers need to know how to construct prompts to produce more secure code.  &#xA;&lt;p&gt;Yet, developers need their own tools to detect potentially malicious code, says the University of Virginia&amp;rsquo;s Evans.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Google&#39;s Trail of Crumbs</title>
      <link>//uvasrg.github.io/googles-trail-of-crumbs/</link>
      <pubDate>Mon, 05 Aug 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/googles-trail-of-crumbs/</guid>
      <description>&lt;p&gt;Matt Stoller published &lt;a href=&#34;https://www.thebignewsletter.com/p/googles-trail-of-crumbs&#34;&gt;my essay on Google&amp;rsquo;s decision to abandon its Privacy Sandbox Initiative&lt;/a&gt; in his Big newsletter:&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;https://www.thebignewsletter.com/p/googles-trail-of-crumbs&#34;&gt;&#xA;&lt;div class=&#34;substack-post-embed&#34;&gt;&lt;p lang=&#34;en&#34;&gt;Google&#39;s Trail of Crumbs by Matt Stoller&lt;/p&gt;&lt;p&gt;Google is too big to get rid of cookies. Even when it wants to protect users, it can&#39;t.&lt;/p&gt;&lt;a data-post-link href=&#34;https://www.thebignewsletter.com/p/googles-trail-of-crumbs&#34;&gt;Read on Substack&lt;/a&gt;&lt;/div&gt;&lt;script async src=&#34;https://substack.com/embedjs/embed.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/a&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;For more technical background on this, see Minjun&amp;rsquo;s paper: &lt;a href=&#34;https://arxiv.org/abs/2405.08102&#34;&gt;&lt;em&gt;Evaluating Google&amp;rsquo;s Protected Audience Protocol&lt;/em&gt;&lt;/a&gt; in PETS 2024.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Technology: US authorities survey AI ecosystem through antitrust lens</title>
      <link>//uvasrg.github.io/technology-us-authorities-survey-ai-ecosystem-through-antitrust-lens/</link>
      <pubDate>Sun, 04 Aug 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/technology-us-authorities-survey-ai-ecosystem-through-antitrust-lens/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m quoted in this article for the International Bar Association:&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.ibanet.org/technology-us-authorities-survey-ai-ecosystem-through-antitrust-lens&#34;&gt;&lt;em&gt;Technology: US authorities survey AI ecosystem through antitrust lens&lt;/em&gt;&lt;/a&gt;&lt;br&gt;&#xA;William Roberts, IBA US Correspondent&lt;br&gt;&#xA;Friday 2 August 2024&lt;/p&gt;&#xA;&lt;/center&gt;&#xA;&lt;blockquote&gt;&#xA;Antitrust authorities in the US are targeting the new frontier of artificial intelligence (AI) for potential enforcement action.&#xA;&lt;p&gt;&amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt;Jonathan Kanter, Assistant Attorney General for the Antitrust Division of the DoJ, warns that the government sees ‘structures and trends in AI that should give us pause’. He says that AI relies on massive amounts of data and computing power, which can give already dominant companies a substantial advantage. ‘Powerful network and feedback effects’ may enable dominant companies to control these new markets, Kanter adds.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Voice of America interview on ChatGPT</title>
      <link>//uvasrg.github.io/voice-of-america-interview-on-chatgpt/</link>
      <pubDate>Fri, 10 Feb 2023 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/voice-of-america-interview-on-chatgpt/</guid>
      <description>&lt;p&gt;I was interviewed for a Voice of America story (in Russian) on the impact of chatGPT and similar tools.&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube-nocookie.com/embed/dFuunAFX9y4?start=319&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen&gt;&lt;/iframe&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;Full story: &lt;a href=&#34;https://youtu.be/dFuunAFX9y4&#34;&gt;https://youtu.be/dFuunAFX9y4&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Uh-oh, there&#39;s a new way to poison code models</title>
      <link>//uvasrg.github.io/uh-oh-theres-a-new-way-to-poison-code-models/</link>
      <pubDate>Mon, 16 Jan 2023 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/uh-oh-theres-a-new-way-to-poison-code-models/</guid>
      <description>&lt;p&gt;Jack Clark&amp;rsquo;s &lt;a href=&#34;https://mailchi.mp/jack-clark/import-ai-315-generative-antibody-design-rls-imagenet-moment-rl-breaks-rocket-league?e=545365c0e9&#34;&gt;Import AI, 16 Jan 2023&lt;/a&gt; includes a nice description of our work on TrojanPuzzle:&lt;/p&gt;&#xA;&lt;p style=&#34;margin: 10px 0;padding: 0;mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #202020;font-family: Helvetica;font-size: 16px;line-height: 150%;text-align: left;&#34;&gt;&lt;strong&gt;####################################################&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;!-- /wp:paragraph --&gt;&lt;!-- wp:paragraph --&gt;&#xA;&lt;p style=&#34;margin: 10px 0;padding: 0;mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #202020;font-family: Helvetica;font-size: 16px;line-height: 150%;text-align: left;&#34;&gt;&lt;strong&gt;Uh-oh, there&#39;s a new way to poison code models - and it&#39;s really hard to detect:&lt;/strong&gt;&lt;br&gt;&#xA;&lt;em&gt;…TROJANPUZZLE is a clever way to trick your code model into betraying you - if you can poison the undelrying dataset…&lt;/em&gt;&lt;br&gt;&#xA;Researchers with the University of California, Santa Barbara, Microsoft Corporation, and the University of Virginia have come up with some clever, subtle ways to poison the datasets used to train code models. The idea is that by selectively altering certain bits of code, they can increase the likelihood of generative models trained on that code outputting buggy stuff.&amp;nbsp;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Trojan Puzzle attack trains AI assistants into suggesting malicious code</title>
      <link>//uvasrg.github.io/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/</link>
      <pubDate>Tue, 10 Jan 2023 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/</guid>
      <description>&lt;p&gt;Bleeping Computer has a &lt;a href=&#34;https://www.bleepingcomputer.com/news/security/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/&#34;&gt;story on our work&lt;/a&gt; (in collaboration with Microsoft Research) on poisoning code suggestion models:&lt;/p&gt;&#xA;&lt;h1&gt;Trojan Puzzle attack trains AI assistants into suggesting malicious code&lt;/h1&gt;&#xA;&lt;p&gt;By &lt;b&gt;Bill Toulas&lt;/b&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;img alt=&#34;Person made of jigsaw puzzle pieces&#34; height=&#34;900&#34; src=&#34;https://www.bleepstatic.com/content/hl-images/2022/10/09/mystery-hacker.jpg&#34; width=&#34;80%&#34;&gt;&lt;/img&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;Researchers at the universities of California, Virginia, and Microsoft have devised a new poisoning attack that could trick AI-based coding assistants into suggesting dangerous code.&lt;/p&gt;&#xA;&lt;p&gt;Named &amp;lsquo;Trojan Puzzle,&amp;rsquo; the attack stands out for bypassing static detection and signature-based dataset cleansing models, resulting in the AI models being trained to learn how to reproduce dangerous payloads.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to Hide a Backdoor</title>
      <link>//uvasrg.github.io/how-to-hide-a-backdoor/</link>
      <pubDate>Thu, 06 May 2021 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/how-to-hide-a-backdoor/</guid>
      <description>&lt;p&gt;The Register has an article on our recent work on &lt;a href=&#34;https://arxiv.org/abs/2104.15129&#34;&gt;&lt;em&gt;Stealthy Backdoors as Compression Artifacts&lt;/em&gt;&lt;/a&gt;:&#xA;Thomas Claburn, &lt;a href=&#34;https://www.theregister.com/AMP/2021/05/05/ai_backdoors/&#34;&gt;&lt;em&gt;How to hide a backdoor in AI software — Neural networks can be aimed to misbehave when squeezed&lt;/em&gt;&lt;/a&gt;, The Register, 5 May 2021.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Fact-checking Donald Trump’s tweet firing Christopher Krebs</title>
      <link>//uvasrg.github.io/fact-checking-donald-trumps-tweet-firing-christopher-krebs/</link>
      <pubDate>Sat, 14 Nov 2020 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/fact-checking-donald-trumps-tweet-firing-christopher-krebs/</guid>
      <description>&lt;p&gt;I was a source for thie &amp;ldquo;Pants on Fire!&amp;rdquo; fact check by PolitiFact on&#xA;Donald Trump&amp;rsquo;s tweet that fired Christopher Krebs claiming that &amp;ldquo;The&#xA;recent statement by Chris Krebs on the security of the 2020 Election&#xA;was highly inaccurate, in that there were massive improprieties and&#xA;fraud - including dead people voting, Poll Watchers not allowed into&#xA;polling locations, “glitches” in the voting machines which changed&amp;hellip;&amp;rdquo;&lt;/p&gt;&#xA;&lt;img src=&#34;https://static.politifact.com/politifact/rulings/tom_ruling_pof.png&#34; align=&#34;right&#34; width=200&gt;&#xA;&lt;p&gt;PolitiFact: &lt;a href=&#34;https://www.politifact.com/factchecks/2020/nov/18/donald-trump/fact-checking-donald-trumps-tweet-firing-christoph/&#34;&gt;&lt;em&gt;Fact-checking Donald Trump’s tweet firing Christopher Krebs&lt;/em&gt;&lt;/a&gt;, 18 November 2020&lt;/p&gt;</description>
    </item>
    <item>
      <title>Voting Security</title>
      <link>//uvasrg.github.io/voting-security/</link>
      <pubDate>Sat, 31 Oct 2020 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/voting-security/</guid>
      <description>&lt;p&gt;I was interviewed for a local news story by Daniel Grimes on election&#xA;security: &lt;a href=&#34;https://www.nbc29.com/2020/10/21/virginia-is-one-safer-states-cast-ballot-says-uva-cybersecurity-expert/&#34;&gt;&lt;em&gt;UVA cybersecurity expert: Virginia is one of the safer states to cast a ballot&lt;/em&gt;&lt;/a&gt;, NBC 29 News, 21 October 2020.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How AI could save lives without spilling medical secrets</title>
      <link>//uvasrg.github.io/how-ai-could-save-lives-without-spilling-medical-secrets/</link>
      <pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/how-ai-could-save-lives-without-spilling-medical-secrets/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m quoted in this article by Will Knight focused on the work Oasis Labs (Dawn Song&amp;rsquo;s company) is doing on privacy-preserving medical data analysis: &lt;a href=&#34;https://www.technologyreview.com/s/613520/how-ai-could-save-lives-without-spilling-secrets/&#34;&gt;&lt;em&gt;How AI could save lives without spilling medical secrets&lt;/em&gt;&lt;/a&gt;, MIT Technology Review, 14 May 2019.&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&amp;ldquo;The whole notion of doing computation while keeping data secret is an incredibly powerful one,&amp;rdquo; says David Evans, who specializes in machine learning and security at the University of Virginia. When applied across hospitals and patient populations, for instance, machine learning might unlock completely new ways of tying disease to genomics, test results, and other patient information.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Plan to Eradicate Stalkerware</title>
      <link>//uvasrg.github.io/a-plan-to-eradicate-stalkerware/</link>
      <pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/a-plan-to-eradicate-stalkerware/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.cs.cornell.edu/~havron/&#34;&gt;Sam Havron&lt;/a&gt; (BSCS 2017) is quoted in &lt;a href=&#34;https://www.wired.com/story/eva-galperin-stalkerware-kaspersky-antivirus/&#34;&gt;an article in Wired&lt;/a&gt; on eradicating stalkerware:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;The full extent of that stalkerware crackdown will only prove out with time and testing, says Sam Havron, a Cornell researcher who worked on last year&amp;rsquo;s spyware study. Much more work remains. He notes that domestic abuse victims can also be tracked with dual-use apps often overlooked by antivirus firms, like antitheft software Cerberus. Even innocent tools like Apple&amp;rsquo;s Find My Friends and Google Maps&amp;rsquo; location-sharing features can be abused if they don&amp;rsquo;t better communicate to users that they may have been secretly configured to share their location. &amp;ldquo;This is really exciting news,&amp;rdquo; Havron says of Kaspersky&amp;rsquo;s stalkerware change. &amp;ldquo;Hopefully it will spur the rest of the industry to follow suit. But it&amp;rsquo;s just the very first thing.&amp;rdquo;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deep Fools</title>
      <link>//uvasrg.github.io/deep-fools/</link>
      <pubDate>Mon, 21 Jan 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/deep-fools/</guid>
      <description>&lt;p&gt;&lt;em&gt;New Electronics&lt;/em&gt; has an article that includes my &lt;a href=&#34;//uvasrg.github.io/dls-keynote-is-adversarial-examples-an-adversarial-example/&#34;&gt;&lt;em&gt;Deep Learning and Security Workshop&lt;/em&gt; talk&lt;/a&gt;: &lt;a href=&#34;http://www.newelectronics.co.uk/electronics-technology/deep-fools/205133/&#34;&gt;&lt;em&gt;Deep fools&lt;/em&gt;&lt;/a&gt;, 21 January 2019.&lt;/p&gt;&#xA;&lt;p&gt;A better version of the image Mainuddin Jonas produced that they use&#xA;(which they screenshot from the talk video) is below:&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;A href=&#34;//uvasrg.github.io/images/adversarialperturbations.png&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/adversarialperturbations.png&#34; width=80%&gt;&lt;/a&gt;&#xA;&lt;/center&gt;</description>
    </item>
    <item>
      <title>Center for Trustworthy Machine Learning</title>
      <link>//uvasrg.github.io/center-for-trustworthy-machine-learning/</link>
      <pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/center-for-trustworthy-machine-learning/</guid>
      <description>&lt;img src=&#34;//uvasrg.github.io/images/nsf_logo-1h9wdoa.png&#34; align=&#34;right&#34; width=120&gt;&#xA;&lt;p&gt;The National Science Foundation announced the &lt;em&gt;Center for Trustworthy&#xA;Machine Learning&lt;/em&gt; today, a new five-year SaTC Frontier Center &amp;ldquo;to&#xA;develop a rigorous understanding of the security risks of the use of&#xA;machine learning and to devise the tools, metrics and methods to&#xA;manage and mitigate security vulnerabilities.&amp;rdquo;&lt;/p&gt;&#xA;&lt;img src=&#34;//uvasrg.github.io/images/ctmllogos.png&#34; align=&#34;left&#34; style=&#34;padding-right: 1em;padding-top: .5em&#34; width=250&gt;&#xA;&lt;p&gt;The Center is lead by Patrick McDaniel at Penn State University, and&#xA;in addition to our group, includes Dan Boneh and Percy Liang (Stanford&#xA;University), Kamalika Chaudhuri (University of California San Diego),&#xA;Somesh Jha (University of Wisconsin) and Dawn Song (University of&#xA;California Berkeley).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Artificial intelligence: the new ghost in the machine</title>
      <link>//uvasrg.github.io/artificial-intelligence-the-new-ghost-in-the-machine/</link>
      <pubDate>Sat, 13 Oct 2018 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/artificial-intelligence-the-new-ghost-in-the-machine/</guid>
      <description>&lt;p&gt;&lt;em&gt;Engineering and Technology&lt;/em&gt; Magazine (a publication of the British&#xA;[Institution of Engineering and Technology](&lt;a&#xA;href=&#34;https://www.theiet.org/&#34;&gt;) has an article that highlights&#xA;adversarial machine learning research: &lt;a href=&#34;https://eandt.theiet.org/content/articles/2018/10/artificial-intelligence-the-new-ghost-in-the-machine/&#34;&gt;&lt;em&gt;Artificial intelligence: the&#xA;new ghost in the&#xA;machine&lt;/em&gt;&lt;/a&gt;,&#xA;10 October 2018, by Chris Edwards.&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;img src=&#34;https://eandt.theiet.org/media/7065/feature_501008906335871317828.jpg?anchor=center&amp;amp;mode=crop&amp;amp;width=400&amp;amp;height=267&amp;amp;rnd=131836400900000000&#34;&gt;&lt;br /&gt;&#xA;&lt;/center&gt;&#xA;&lt;div class=&#34;excerpt&#34;&gt;&#xA;&lt;p&gt;Although researchers such as David Evans of the University of Virginia see a full explanation being a little way off in the future, the massive number of parameters encoded by DNNs and the avoidance of overtraining due to SGD may have an answer to why the networks can hallucinate images and, as a result, see things that are not there and ignore those that are.&lt;br /&gt;&#xA;…&lt;br /&gt;&#xA;He points to work by PhD student Mainuddin Jonas that shows how adversarial examples can push the output away from what we would see as the correct answer. “It could be just one layer [that makes the mistake]. But from our experience it seems more gradual. It seems many of the layers are being exploited, each one just a little bit. The biggest differences may not be apparent until the very last layer.”&lt;br /&gt;&#xA;…&lt;br /&gt;&#xA;Researchers such as Evans predict a lengthy arms race in attacks and countermeasures that may on the way reveal a lot more about the nature of machine learning and its relationship with reality.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Violations of Children’s Privacy Laws</title>
      <link>//uvasrg.github.io/violations-of-childrens-privacy-laws/</link>
      <pubDate>Sun, 16 Sep 2018 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/violations-of-childrens-privacy-laws/</guid>
      <description>&lt;p&gt;The New York Times has an article, &lt;a href=&#34;https://www.nytimes.com/interactive/2018/09/12/technology/kids-apps-data-privacy-google-twitter.html&#34;&gt;&lt;em&gt;How Game Apps That Captivate Kids Have Been Collecting Their Data&lt;/em&gt;&lt;/a&gt; about a lawsuit the state of New Mexico is bringing against app markets (including Google) that allow apps presented as being for children in the Play store to violate COPPA rules and mislead users into tracking children. The lawsuit stems from a study led by Serge Egleman’s group at UC Berkeley that analyzed COPPA violations in children’s apps. Serge was an undergraduate student here (back in the early 2000s) – one of the things he did as a undergraduate was successfully sue a spammer.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cybersecurity Summer Camp</title>
      <link>//uvasrg.github.io/cybersecurity-summer-camp/</link>
      <pubDate>Sat, 07 Jul 2018 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/cybersecurity-summer-camp/</guid>
      <description>&lt;p&gt;I helped organize a &lt;a href=&#34;https://www.ahmed.ai/cyberwars2018&#34;&gt;summer camp for high school teachers focused on cybersecurity&lt;/a&gt;, led by Ahmed Ibrahim.  Some of the materials from the camp on cryptography, including the Jefferson Wheel and visual cryptography are here: &lt;a href=&#34;https://github.com/evansuva/cipherschool&#34;&gt;&lt;em&gt;Cipher School for Muggles&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;&#xA;&lt;center&gt;&lt;br /&gt;&#xA;&lt;img src=&#34;https://web.archive.org/web/20180707220753im_/https://news.virginia.edu/sites/default/files/cyber_security_class_da_inline_01.jpg&#34; width=&#34;90%&#34;&gt;&lt;/img&gt;&lt;br /&gt;&#xA;&lt;/center&gt;&lt;br /&gt;&#xA;&lt;a href=&#34;https://news.virginia.edu/content/cybersecurity-goes-summer-camp&#34;&gt;&lt;em&gt;Cybersecurity Goes to Summer Camp&lt;/em&gt;&lt;/a&gt;. UVA Today. 22 July 2018. [&lt;a href=&#34;https://web.archive.org/web/20180707220753/https://news.virginia.edu/content/cybersecurity-goes-summer-camp&#34;&gt;archive.org&lt;/a&gt;]&lt;/p&gt;&#xA;&lt;blockquote&gt;&lt;p&gt;&#xA;Earlier this week, 25 high school teachers – including 21 from Virginia – filled a glass-walled room in Rice Hall, sitting in high adjustable chairs at wheeled work tables, their laptops open, following a lecture with graphics about the dangers that lurk in cyberspace and trying to figure out how to pass the information on to a generation that seems to share the most intimate details of life online. &amp;#8220;I think understanding privacy is important to that generation that uses Facebook and Snapchat,&amp;#8221; said David Evans, a computer science professor who helped organize the camp. &amp;#8220;We hope to give teachers some ideas and tools to get their students excited about learning about cryptography, privacy and cybersecurity, and how these things can impact them.&amp;#8221;&#xA;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
