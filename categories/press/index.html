<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Press | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
        <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
	      
	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      
<div style="margin-top:16px; margin-left: auto; margin-right: auto; max-width: 800px;">
<div class="row">
  <div class="column small-12">
    
    
    <h2><a href="/reassessing-emnlp-2024s-best-paper-does-divergence-based-calibration-for-membership-inference-attacks-hold-up/">Reassessing EMNLP 2024’s Best Paper: Does Divergence-Based Calibration for Membership Inference Attacks Hold Up?</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-11-26 00:00:00 &#43;0000 UTC" itemprop="datePublished">26 November 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/membership-inference">membership inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/llm">LLM</a>
    
  </span>
  
  
</div>


Anshuman Suri and Pratyush Maini wrote a blog about the EMNLP 2024 best paper award winner: Reassessing EMNLP 2024’s Best Paper: Does Divergence-Based Calibration for Membership Inference Attacks Hold Up?.
As we explored in Do Membership Inference Attacks Work on Large Language Models?, to test a membership inference attack it is essentail to have a candidate set where the members and non-members are from the same distribution. If the distributions are different, the ability of an attack to distinguish members and non-members is indicative of distribution inference, not necessarily membership inference.
<p class="text-right"><a href="/reassessing-emnlp-2024s-best-paper-does-divergence-based-calibration-for-membership-inference-attacks-hold-up/">Read More…</a></p>
	

    
    <h2><a href="/common-way-to-test-for-leaks-in-large-language-models-may-be-flawed/">Common Way To Test for Leaks in Large Language Models May Be Flawed</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-11-13 00:00:00 &#43;0000 UTC" itemprop="datePublished">13 November 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/membership-inference">membership inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/llm">LLM</a>
    
  </span>
  
  
</div>


<p>UVA News has an article on our LLM membership inference work:
<a href="https://engineering.virginia.edu/news-events/news/common-way-test-leaks-large-language-models-may-be-flawed"><em>Common Way To Test for Leaks in Large Language Models May Be Flawed: UVA Researchers Collaborated To Study the Effectiveness of Membership Inference Attacks</em></a>, Eric Williamson, 13 November 2024.</p>

	

    
    <h2><a href="/poisoning-llms/">Poisoning LLMs</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-08-21 00:00:00 &#43;0000 UTC" itemprop="datePublished">21 August 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/large-language-models">large language models</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/copilot">copilot</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning">poisoning</a>
    
  </span>
  
  
</div>


I&rsquo;m quoted in this story by Rob Lemos about poisoning code models (the CodeBreaker paper in USENIX Security 2024 by Shenao Yan, Shen Wang, Yue Duan, Hanbin Hong, Kiho Lee, Doowon Kim, and Yuan Hong), that considers a similar threat to our TrojanPuzzle work:
Researchers Highlight How Poisoned LLMs Can Suggest Vulnerable Code
Dark Reading, 20 August 2024
CodeBreaker uses code transformations to create vulnerable code that continues to function as expected, but that will not be detected by major static analysis security testing.
<p class="text-right"><a href="/poisoning-llms/">Read More…</a></p>
	

    
    <h2><a href="/googles-trail-of-crumbs/">Google&#39;s Trail of Crumbs</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-08-05 00:00:00 &#43;0000 UTC" itemprop="datePublished">5 August 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/google">Google</a>
    
  </span>
  
  
</div>


<p>Matt Stoller published <a href="https://www.thebignewsletter.com/p/googles-trail-of-crumbs">my essay on Google&rsquo;s decision to abandon its Privacy Sandbox Initiative</a> in his Big newsletter:</p>
<center>
<a href="https://www.thebignewsletter.com/p/googles-trail-of-crumbs">
<div class="substack-post-embed"><p lang="en">Google's Trail of Crumbs by Matt Stoller</p><p>Google is too big to get rid of cookies. Even when it wants to protect users, it can't.</p><a data-post-link href="https://www.thebignewsletter.com/p/googles-trail-of-crumbs">Read on Substack</a></div><script async src="https://substack.com/embedjs/embed.js" charset="utf-8"></script></a>
</center>
<p>For more technical background on this, see Minjun&rsquo;s paper: <a href="https://arxiv.org/abs/2405.08102"><em>Evaluating Google&rsquo;s Protected Audience Protocol</em></a> in PETS 2024.</p>

	

    
    <h2><a href="/technology-us-authorities-survey-ai-ecosystem-through-antitrust-lens/">Technology: US authorities survey AI ecosystem through antitrust lens</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-08-04 00:00:00 &#43;0000 UTC" itemprop="datePublished">4 August 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/artificial-intelligence">artificial intelligence</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/machine-learning">machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anti-trust">anti-trust</a>
    
  </span>
  
  
</div>


I&rsquo;m quoted in this article for the International Bar Association:
Technology: US authorities survey AI ecosystem through antitrust lens
William Roberts, IBA US Correspondent
Friday 2 August 2024
Antitrust authorities in the US are targeting the new frontier of artificial intelligence (AI) for potential enforcement action. &hellip;
Jonathan Kanter, Assistant Attorney General for the Antitrust Division of the DoJ, warns that the government sees ‘structures and trends in AI that should give us pause’.
<p class="text-right"><a href="/technology-us-authorities-survey-ai-ecosystem-through-antitrust-lens/">Read More…</a></p>
	

    
    <h2><a href="/voice-of-america-interview-on-chatgpt/">Voice of America interview on ChatGPT</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-02-10 00:00:00 &#43;0000 UTC" itemprop="datePublished">10 February 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/large-language-models">large language models</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/nlp">nlp</a>
    
  </span>
  
  
</div>


<p>I was interviewed for a Voice of America story (in Russian) on the impact of chatGPT and similar tools.</p>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/dFuunAFX9y4?start=319" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</center>
<p>Full story: <a href="https://youtu.be/dFuunAFX9y4">https://youtu.be/dFuunAFX9y4</a></p>

	

    
    <h2><a href="/uh-oh-theres-a-new-way-to-poison-code-models/">Uh-oh, there&#39;s a new way to poison code models</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-01-16 00:00:00 &#43;0000 UTC" itemprop="datePublished">16 January 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/large-language-models">large language models</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/copilot">copilot</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning">poisoning</a>
    
  </span>
  
  
</div>


Jack Clark&rsquo;s Import AI, 16 Jan 2023 includes a nice description of our work on TrojanPuzzle:
####################################################
Uh-oh, there's a new way to poison code models - and it's really hard to detect:
…TROJANPUZZLE is a clever way to trick your code model into betraying you - if you can poison the undelrying dataset…
Researchers with the University of California, Santa Barbara, Microsoft Corporation, and the University of Virginia have come up with some clever, subtle ways to poison the datasets used to train code models.
<p class="text-right"><a href="/uh-oh-theres-a-new-way-to-poison-code-models/">Read More…</a></p>
	

    
    <h2><a href="/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/">Trojan Puzzle attack trains AI assistants into suggesting malicious code</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-01-10 00:00:00 &#43;0000 UTC" itemprop="datePublished">10 January 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/large-language-models">large language models</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/copilot">copilot</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning">poisoning</a>
    
  </span>
  
  
</div>


Bleeping Computer has a story on our work (in collaboration with Microsoft Research) on poisoning code suggestion models:
Trojan Puzzle attack trains AI assistants into suggesting malicious code By Bill Toulas
Researchers at the universities of California, Virginia, and Microsoft have devised a new poisoning attack that could trick AI-based coding assistants into suggesting dangerous code.
Named &lsquo;Trojan Puzzle,&rsquo; the attack stands out for bypassing static detection and signature-based dataset cleansing models, resulting in the AI models being trained to learn how to reproduce dangerous payloads.
<p class="text-right"><a href="/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/">Read More…</a></p>
	

    
    <h2><a href="/how-to-hide-a-backdoor/">How to Hide a Backdoor</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2021-05-06 00:00:00 &#43;0000 UTC" itemprop="datePublished">6 May 2021</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/research">research</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yulong-tian">Yulong Tian</a>
    
  </span>
  
  
</div>


<p>The Register has an article on our recent work on <a href="https://arxiv.org/abs/2104.15129"><em>Stealthy Backdoors as Compression Artifacts</em></a>:
Thomas Claburn, <a href="https://www.theregister.com/AMP/2021/05/05/ai_backdoors/"><em>How to hide a backdoor in AI software — Neural networks can be aimed to misbehave when squeezed</em></a>, The Register, 5 May 2021.</p>

	

    
    <h2><a href="/fact-checking-donald-trumps-tweet-firing-christopher-krebs/">Fact-checking Donald Trump’s tweet firing Christopher Krebs</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2020-11-14 00:00:00 &#43;0000 UTC" itemprop="datePublished">14 November 2020</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/voting">voting</a>
    
  </span>
  
  
</div>


I was a source for thie &ldquo;Pants on Fire!&rdquo; fact check by PolitiFact on Donald Trump&rsquo;s tweet that fired Christopher Krebs claiming that &ldquo;The recent statement by Chris Krebs on the security of the 2020 Election was highly inaccurate, in that there were massive improprieties and fraud - including dead people voting, Poll Watchers not allowed into polling locations, “glitches” in the voting machines which changed&hellip;&rdquo;
PolitiFact: Fact-checking Donald Trump’s tweet firing Christopher Krebs, 18 November 2020
<p class="text-right"><a href="/fact-checking-donald-trumps-tweet-firing-christopher-krebs/">Read More…</a></p>
	

    
    <div class="row">
  <div class="column small-12">
    <ul class="pagination" role="navigation" aria-label="Pagination">
      
      <li class="arrow" aria-disabled="true"><a href="/categories/press/page/2/">&laquo; <em>Older<span class="show-for-sr"> blog entries</span></em></a></li>
      
      <li><span>Page 1 of 2</span></li>      
      
    </ul>    
    All Posts by <a href="//uvasrg.github.io/categories">Category</a> or <a href="//uvasrg.github.io/tags">Tags</a>.

  </div>
</div>

  </div>
</div>
</div>

    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
