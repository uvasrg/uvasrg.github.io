<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Conferences on Security Research Group</title>
    <link>//uvasrg.github.io/categories/conferences/</link>
    <description>Recent content in Conferences on Security Research Group</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Fri, 16 Dec 2022 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/categories/conferences/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Dissecting Distribution Inference</title>
      <link>//uvasrg.github.io/dissecting-distribution-inference/</link>
      <pubDate>Fri, 16 Dec 2022 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/dissecting-distribution-inference/</guid>
      <description>&lt;p&gt;(Cross-post by &lt;a href=&#34;https://www.anshumansuri.com/post/dissecting&#34;&gt;Anshuman Suri&lt;/a&gt;)&lt;/p&gt;&#xA;&lt;p&gt;Distribution inference attacks aims to infer statistical properties of data used to train machine learning models.&#xA;These attacks are sometimes surprisingly potent, as we demonstrated in&#xA;&lt;a href=&#34;https://uvasrg.github.io/on-the-risks-of-distribution-inference/&#34;&gt;previous work&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;!-- However, the factors that impact this inference risk are not well understood, and demonstrated attacks often&#xA;rely on strong and unrealistic assumptions such as full knowledge of training environments&#xA;even in supposedly black-box threat scenarios. --&gt;&#xA;&lt;!-- In this work, we develop a new black-box attack, the KL Divergence Attack (KL), and use it to evaluate inference risk while relaxing&#xA;a number of implicit assumptions based on the adversary&#39;s knowledge in black-box scenarios. We also evaluate several noise-based defenses, a&#xA;standard approach while trying to preserve privacy in machine learning, along with some intuitive defenses based on resampling. --&gt;&#xA;&lt;h2 id=&#34;kl-divergence-attack&#34;&gt;KL Divergence Attack&lt;/h2&gt;&#xA;&lt;p&gt;Most attacks against distribution inference involve training a meta-classifier, either using model parameters in white-box settings (Ganju et al., &lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3243734.3243834&#34;&gt;Property Inference Attacks on Fully Connected Neural Networks using Permutation Invariant Representations&lt;/a&gt;, CCS 2018), or using model&#xA;predictions in black-box scenarios (Zhang et al., &lt;a href=&#34;https://www.usenix.org/system/files/sec21-zhang-wanrong.pdf&#34;&gt;Leakage of Dataset Properties in Multi-Party Machine Learning&lt;/a&gt;, USENIX 2021). While other black-box were proposed in our prior work, they are not as accurate as meta-classifier-based methods, and require training shadow models nonetheless (Suri and Evans, &lt;a href=&#34;https://arxiv.org/pdf/2109.06024.pdf&#34;&gt;Formalizing and Estimating Distribution Inference Risks&lt;/a&gt;, PETS 2022).&lt;/p&gt;</description>
    </item>
    <item>
      <title>On the Risks of Distribution Inference</title>
      <link>//uvasrg.github.io/on-the-risks-of-distribution-inference/</link>
      <pubDate>Thu, 24 Jun 2021 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/on-the-risks-of-distribution-inference/</guid>
      <description>&lt;p&gt;(Cross-post by &lt;a href=&#34;https://www.anshumansuri.com/post/distr_infer&#34;&gt;Anshuman Suri&lt;/a&gt;)&lt;/p&gt;&#xA;&lt;p&gt;Inference attacks seek to infer sensitive information about the training process of a revealed machine-learned model, most often about the training data.&lt;/p&gt;&#xA;&lt;p&gt;Standard inference attacks (which we call “dataset inference attacks”)&#xA;aim to learn something about a particular record that may have been in&#xA;that training data. For example, in a membership inference attack&#xA;(Reza Shokri et al., &lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7958568&#34;&gt;&lt;em&gt;Membership Inference Attacks Against Machine&#xA;Learning&#xA;Models&lt;/em&gt;&lt;/a&gt;, IEEE S&amp;amp;P 2017),&#xA;the adversary aims to infer whether or not a particular record was&#xA;included in the training data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Oakland Test-of-Time Awards</title>
      <link>//uvasrg.github.io/oakland-test-of-time-awards/</link>
      <pubDate>Sat, 23 May 2020 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/oakland-test-of-time-awards/</guid>
      <description>&lt;p&gt;I chaired the committee to select Test-of-Time Awards for the IEEE Symposium on Security and Privacy symposia from 1995-2006, which were presented at the Opening Section of the &lt;a href=&#34;https://www.ieee-security.org/TC/SP2020/awards.html&#34;&gt;41&lt;sup&gt;st&lt;/sup&gt; &lt;em&gt;IEEE Symposium on Security and Privacy&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube-nocookie.com/embed/EwT1tnyin1M&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&#xA;&lt;/center&gt;</description>
    </item>
    <item>
      <title>NeurIPS 2019</title>
      <link>//uvasrg.github.io/neurips2019/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/neurips2019/</guid>
      <description>&lt;p&gt;&#xA;Here&#39;s a video of Xiao Zhang&#39;s presentation at NeurIPS 2019: &lt;br&gt;&#xA;&lt;a href=&#34;https://slideslive.com/38921718/track-2-session-1&#34;&gt;&lt;em&gt;https://slideslive.com/38921718/track-2-session-1&lt;/em&gt;&lt;/a&gt; (starting at 26:50)&#xA;&lt;/p&gt;&#xA;&lt;p&gt;&#xA;See &lt;A href=&#34;//uvasrg.github.io/neurips-2019-empirically-measuring-concentration/&#34;&gt;this post&lt;/a&gt; for info on the paper.&#xA;&lt;/p&gt;&#xA;Here are a few pictures from NeurIPS 2019 (by Sicheng Zhu and Mohammad Mahmoody):&#xA;&lt;p&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/NeurIPS2019/IMG_6759.JPG&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/NeurIPS2019/IMG_6759.JPG&#34; width=&#34;75%&#34;&gt;&lt;/a&gt;&lt;br&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/NeurIPS2019/IMG_6777.JPG&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/NeurIPS2019/IMG_6777.JPG&#34; width=&#34;75%&#34;&gt;&lt;/a&gt;&lt;br&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/NeurIPS2019/xiao-poster.jpg&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/NeurIPS2019/xiao-poster.jpg&#34;&gt;&lt;/a&gt;&lt;br&gt;&#xA;&lt;/center&gt;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>White House Visit</title>
      <link>//uvasrg.github.io/white-house-visit/</link>
      <pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/white-house-visit/</guid>
      <description>&lt;p&gt;I had a chance to visit the White House for a Roundtable on&#xA;&lt;em&gt;Accelerating Responsible Sharing of Federal Data&lt;/em&gt;. The meeting was&#xA;held under &amp;ldquo;Chatham House Rules&amp;rdquo;, so I won&amp;rsquo;t mention the other&#xA;participants here.&lt;/p&gt;&#xA;&lt;div style=&#34;padding-bottom: 2em&#34;&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/whitehouse/IMG_20191030_080936.png&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/whitehouse/IMG_20191030_080936.png&#34; width=75%&gt;&#xA;&lt;/center&gt;&#xA;&lt;/div&gt;&#xA;&lt;p&gt;The meeting was held in the &lt;a href=&#34;https://en.wikipedia.org/wiki/Roosevelt_Room&#34;&gt;Roosevelt&#xA;Room&lt;/a&gt; of the White&#xA;House. We entered through the visitor&amp;rsquo;s side entrance. After a&#xA;security gate (where you put your phone in a lockbox, so no pictures&#xA;inside) with a TV blaring Fox News, there is a pleasant lobby for&#xA;waiting, and then an entrance right into the Roosevelt Room. (We&#xA;didn&amp;rsquo;t get to see the entrance in the opposite corner of the room,&#xA;which is just a hallway across from the Oval Office.)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Research Symposium Posters</title>
      <link>//uvasrg.github.io/research-symposium-posters/</link>
      <pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/research-symposium-posters/</guid>
      <description>&lt;p&gt;Five students from our group presented posters at the department&amp;rsquo;s&#xA;&lt;a href=&#34;https://engineering.virginia.edu/cs-research-symposium-fall-2019&#34;&gt;Fall Research&#xA;Symposium&lt;/a&gt;:&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;iframe src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vQcaahIxPyCHIMJti6tRB9HM_RreRhZkGH4wCN7YjTwiHSqcHod9v3hDFj-ZS1TtXp9OtBEBCV8OPH4/embed?start=false&amp;loop=false&amp;delayms=3000&#34; frameborder=&#34;0&#34; width=&#34;764&#34; height=&#34;453&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;&lt;br&gt;&#xA;Anshuman Suri&#39;s Overview Talk&#xA;&lt;/center&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/evaluatingdpml-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt; &lt;br&gt;&#xA;Bargav Jayaraman, &lt;em&gt;Evaluating Differentially Private Machine Learning In Practice&lt;/em&gt; &#xA;[&lt;a href=&#34;&#xA;&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/evaluatingdpml-poster.pdf&#34;&gt;Poster&lt;/a&gt;]&lt;br&gt;&#xA;[&lt;a href=&#34;https://www.cs.virginia.edu/~evans/pubs/usenix2019/&#34;&gt;Paper&lt;/a&gt; (USENIX Security 2019)]&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/pretrainedvulnerable-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt;&lt;br&gt;&#xA;Hannah Chen [&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/pretrainedvulnerable-poster.pdf&#34;&gt;Poster&lt;/a&gt;]&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/measuringconcentration-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt;&lt;br&gt;&#xA;Xiao Zhang [&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/measuringconcentration-poster.pdf&#34;&gt;Poster&lt;a&gt;]&lt;br&gt;&#xA;[&lt;a href=&#34;https://arxiv.org/abs/1905.12202&#34;&gt;Paper&lt;/a&gt; (NeurIPS 2019)]&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/diversemodels-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt;&lt;br&gt;&#xA;Mainudding Jonas [&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/diversemodels-poster.pdf&#34;&gt;Poster&lt;/a&gt;]&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/hybridbatch-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt;&lt;br&gt;&#xA;Fnu Suya [&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/hybridbatch-poster.pdf&#34;&gt;Poster&lt;a&gt;]&lt;br&gt;&#xA;[&lt;a href=&#34;https://arxiv.org/abs/1908.07000&#34;&gt;Paper&lt;/a&gt; (USENIX Security 2020)]&#xA;&lt;/center&gt;</description>
    </item>
    <item>
      <title>Evaluating Differentially Private Machine Learning in Practice</title>
      <link>//uvasrg.github.io/evaluating-differentially-private-machine-learning-in-practice/</link>
      <pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/evaluating-differentially-private-machine-learning-in-practice/</guid>
      <description>&lt;p&gt;(Cross-post by &lt;a href=&#34;https://bargavjayaraman.github.io/post/evaluating-dpml-results/&#34;&gt;Bargav Jayaraman&lt;/a&gt;)&lt;/p&gt;&#xA;&lt;p&gt;With the recent advances in composition of differential private&#xA;mechanisms, the research community has been able to achieve meaningful&#xA;deep learning with privacy budgets in single digits. Rènyi&#xA;differential privacy (RDP) is one mechanism that provides tighter&#xA;composition which is widely used because of its implementation in&#xA;TensorFlow Privacy (recently, Gaussian differential privacy (GDP) has&#xA;shown a tighter analysis for low privacy budgets, but it was not yet&#xA;available when we did this work). But the central question that&#xA;remains to be answered is: &lt;em&gt;how private are these methods in&#xA;practice?&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>USENIX Security Symposium 2019</title>
      <link>//uvasrg.github.io/usenix-security-symposium-2019/</link>
      <pubDate>Mon, 26 Aug 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/usenix-security-symposium-2019/</guid>
      <description>&lt;p&gt;Bargav Jayaraman presented our paper on &lt;a href=&#34;https://arxiv.org/abs/1902.08874&#34;&gt;&lt;em&gt;Evaluating Differentially Private Machine Learning in Practice&lt;/em&gt;&lt;/a&gt; at the &lt;a&#xA;href=&#34;https://www.usenix.org/conference/usenixsecurity19&#34;&gt;28&lt;sup&gt;th&lt;/sup&gt;&#xA;USENIX Security Symposium&lt;/em&gt;&lt;/a&gt; in Santa Clara, California.&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube-nocookie.com/embed/JAGhqbY_U50&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&#xA;&lt;/center&gt;&#xA;&lt;center&gt;&#xA;&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;dfdd40e4ba2b46e1baee68219df82de7&#34; data-ratio=&#34;1.29456384323641&#34; src=&#34;//speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;&#xA;&lt;/center&gt;&#xA;&lt;center&gt;&lt;img src=&#34;//uvasrg.github.io/images/usenix2019/bargav.jpg&#34; width=80%&#34;&gt;&lt;/center&gt;&#xA;&lt;p&gt;Summary by Lea Kissner:&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Hey it&amp;#39;s the results! &lt;a href=&#34;https://t.co/ru1FbkESho&#34;&gt;pic.twitter.com/ru1FbkESho&lt;/a&gt;&lt;/p&gt;&amp;mdash; Lea Kissner (@LeaKissner) &lt;a href=&#34;https://twitter.com/LeaKissner/status/1162518239177371648?ref_src=twsrc%5Etfw&#34;&gt;August 17, 2019&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;Also, great to see several UVA folks at the conference including:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://havron.dev&#34;&gt;Sam Havron&lt;/a&gt; (BSCS 2017, now a PhD student at&#xA;Cornell) presented a paper on the work he and his colleagues have&#xA;done on &lt;a href=&#34;https://havron.dev/pubs/clinicalsec.pdf&#34;&gt;computer security for victims of intimate partner violence&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;center&gt;&lt;img src=&#34;//uvasrg.github.io/images/usenix2019/havron.jpg&#34; width=80%&#34;&gt;&lt;/center&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.guanotronic.com/~serge/&#34;&gt;Serge Egelman&lt;/a&gt; (BSCS 2004) was an author on the paper &lt;a href=&#34;https://www.usenix.org/conference/usenixsecurity19/presentation/reardon&#34;&gt;&lt;em&gt;50 Ways to Leak Your Data: An&#xA;Exploration of Apps&amp;rsquo; Circumvention of the Android Permissions&#xA;System&lt;/em&gt;&lt;/a&gt;&#xA;(which was recognized by a Distinguished Paper Award). His paper in&#xA;SOUPS on &lt;a href=&#34;https://www.usenix.org/system/files/soups2019-frik.pdf&#34;&gt;&lt;em&gt;Privacy and Security Threat Models and Mitigation&#xA;Strategies of Older&#xA;Adults&lt;/em&gt;&lt;/a&gt; was highlighted in Alex Stamos&amp;rsquo; excellent talk.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Google Security and Privacy Workshop</title>
      <link>//uvasrg.github.io/google-security-and-privacy-workshop/</link>
      <pubDate>Sun, 25 Aug 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/google-security-and-privacy-workshop/</guid>
      <description>&lt;p&gt;I presented a short talk at a workshop at Google on &lt;em&gt;Adversarial ML: Closing Gaps between Theory and Practice&lt;/em&gt; (mostly fun for the &lt;a href=&#34;https://www.youtube.com/watch?v=TVmjjfTvnFs&#34;&gt;movie of me trying to solve Google&amp;rsquo;s CAPTCHA&lt;/a&gt; on the last slide):&lt;/p&gt;&#xA;&lt;iframe src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vTKeMueFNQPz0Pms4EGKoYOXVEg92IBi55babPKG5WRrhHRR2PmIYwZIyLsZ11ucKSahqjjp3Zxd5i3/embed?start=true&amp;loop=false&amp;delayms=3000&#34; frameborder=&#34;0&#34; width=&#34;960&#34; height=&#34;569&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;&#xA;&lt;p&gt;Getting the actual screencast to fit into the limited time for this talk challenged the limits of my video editing skills.&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;img src=&#34;//uvasrg.github.io/images/googledonuts.jpg&#34; width=&#34;90%&#34;&gt;&lt;br&gt;&#xA;I can say with some confidence, Google does donuts much better than they &lt;a href=&#34;https://freedom-to-tinker.com/2019/08/23/deconstructing-googles-excuses-on-tracking-protection/&#34;&gt;do cookies&lt;/a&gt;!&#xA;&lt;/center&gt;</description>
    </item>
    <item>
      <title>NeurIPS 2018: Distributed Learning without Distress</title>
      <link>//uvasrg.github.io/neurips-2018-distributed-learning-without-distress/</link>
      <pubDate>Sat, 08 Dec 2018 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/neurips-2018-distributed-learning-without-distress/</guid>
      <description>&lt;p&gt;Bargav Jayaraman presented our work on privacy-preserving machine learning at the &lt;a href=&#34;https://nips.cc/Conferences/2018/&#34;&gt;32&lt;sup&gt;nd&lt;/sup&gt; &lt;em&gt;Conference on Neural Information Processing Systems&lt;/em&gt;&lt;/a&gt; (NeurIPS 2018) in Montreal.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;Distributed learning&lt;/em&gt; (sometimes known as &lt;em&gt;federated learning&lt;/em&gt;)&#xA;allows a group of independent data owners to collaboratively learn a&#xA;model over their data sets without exposing their private data.  Our&#xA;approach combines &lt;em&gt;differential privacy&lt;/em&gt; with secure &lt;em&gt;multi-party&#xA;computation&lt;/em&gt; to both protect the data during training and produce a&#xA;model that provides privacy against inference attacks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>USENIX Security 2018</title>
      <link>//uvasrg.github.io/usenix-security-2018/</link>
      <pubDate>Sun, 19 Aug 2018 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/usenix-security-2018/</guid>
      <description>&lt;p&gt;Three SRG posters were presented at &lt;a&#xA;href=&#34;https://www.usenix.org/conference/usenixsecurity18/poster-session&#34;&gt;USENIX&#xA;Security Symposium 2018&lt;/a&gt; in Baltimore, Maryland:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Nathaniel Grevatt (&lt;em&gt;GDPR-Compliant Data Processing: Improving&#xA;Pseudonymization with Multi-Party Computation&lt;/em&gt;)&lt;/li&gt;&#xA;&lt;li&gt;Matthew Wallace and Parvesh Samayamanthula (&lt;em&gt;Deceiving Privacy Policy Classifiers with Adversarial Examples&lt;/em&gt;)&lt;/li&gt;&#xA;&lt;li&gt;Guy Verrier (&lt;em&gt;How is GDPR Affecting Privacy Policies?&lt;/em&gt;, joint with Haonan Chen and Yuan&#xA;Tian)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;center&gt;&lt;/p&gt;&#xA;&lt;table width=&#34;85%&#34;&gt;&#xA;&lt;tr&gt;&#xA;&lt;td align=&#34;center&#34;&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/usenix2018/IMG_20180816_190616-2.jpg&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;//uvasrg.github.io/images/usenix2018/IMG_20180816_190616.jpg&#34; height=220&gt;&#xA;&lt;/td&gt;&#xA;&lt;td href=&#34;//uvasrg.github.io/images/usenix2018/IMG_20180816_190626-2.jpg&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;//uvasrg.github.io/images/usenix2018/IMG_20180816_190626.jpg&#34; height=220&gt;&#xA;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;tr&gt;&#xA;&lt;td align=&#34;center&#34;&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/usenix2018/IMG_20180816_192620-2.jpg&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;//uvasrg.github.io/images/usenix2018/IMG_20180816_192620.jpg&#34; height=220&gt;&#xA;&lt;/td&gt;&#xA;&lt;td href=&#34;//uvasrg.github.io/images/usenix2018/IMG_20180816_192646-2.jpg&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;//uvasrg.github.io/images/usenix2018/IMG_20180816_192646.jpg&#34; height=220&gt;&#xA;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;There were also a surprising number of appearances by an unidentified unicorn:&lt;br /&gt;&#xA;&lt;center&gt;&lt;/p&gt;&#xA;&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Your poster may have made the cut for the &lt;a href=&#34;https://twitter.com/hashtag/usesec18?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#usesec18&lt;/a&gt; Poster Reception, but has it received the approval of a tiny, adorable unicorn? &lt;a href=&#34;https://twitter.com/UVA?ref_src=twsrc%5Etfw&#34;&gt;@UVA&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/seenatusesec18?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#seenatusesec18&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/girlswhocode?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#girlswhocode&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/futurecomputerscientist?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#futurecomputerscientist&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/dreambig?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#dreambig&lt;/a&gt; &lt;a href=&#34;https://t.co/bZOO6lYLXK&#34;&gt;pic.twitter.com/bZOO6lYLXK&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cybersecurity Summer Camp</title>
      <link>//uvasrg.github.io/cybersecurity-summer-camp/</link>
      <pubDate>Sat, 07 Jul 2018 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/cybersecurity-summer-camp/</guid>
      <description>&lt;p&gt;I helped organize a &lt;a href=&#34;https://www.ahmed.ai/cyberwars2018&#34;&gt;summer camp for high school teachers focused on cybersecurity&lt;/a&gt;, led by Ahmed Ibrahim.  Some of the materials from the camp on cryptography, including the Jefferson Wheel and visual cryptography are here: &lt;a href=&#34;https://github.com/evansuva/cipherschool&#34;&gt;&lt;em&gt;Cipher School for Muggles&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;&#xA;&lt;center&gt;&lt;br /&gt;&#xA;&lt;img src=&#34;https://web.archive.org/web/20180707220753im_/https://news.virginia.edu/sites/default/files/cyber_security_class_da_inline_01.jpg&#34; width=&#34;90%&#34;&gt;&lt;/img&gt;&lt;br /&gt;&#xA;&lt;/center&gt;&lt;br /&gt;&#xA;&lt;a href=&#34;https://news.virginia.edu/content/cybersecurity-goes-summer-camp&#34;&gt;&lt;em&gt;Cybersecurity Goes to Summer Camp&lt;/em&gt;&lt;/a&gt;. UVA Today. 22 July 2018. [&lt;a href=&#34;https://web.archive.org/web/20180707220753/https://news.virginia.edu/content/cybersecurity-goes-summer-camp&#34;&gt;archive.org&lt;/a&gt;]&lt;/p&gt;&#xA;&lt;blockquote&gt;&lt;p&gt;&#xA;Earlier this week, 25 high school teachers – including 21 from Virginia – filled a glass-walled room in Rice Hall, sitting in high adjustable chairs at wheeled work tables, their laptops open, following a lecture with graphics about the dangers that lurk in cyberspace and trying to figure out how to pass the information on to a generation that seems to share the most intimate details of life online. &amp;#8220;I think understanding privacy is important to that generation that uses Facebook and Snapchat,&amp;#8221; said David Evans, a computer science professor who helped organize the camp. &amp;#8220;We hope to give teachers some ideas and tools to get their students excited about learning about cryptography, privacy and cybersecurity, and how these things can impact them.&amp;#8221;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dependable and Secure Machine Learning</title>
      <link>//uvasrg.github.io/dependable-and-secure-machine-learning/</link>
      <pubDate>Sat, 07 Jul 2018 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/dependable-and-secure-machine-learning/</guid>
      <description>&lt;p&gt;I co-organized, with &lt;a&#xA;href=&#34;http://faculty.virginia.edu/alemzadeh/&#34;&gt;Homa Alemzadeh&lt;/a&gt; and&#xA;&lt;a href=&#34;http://blogs.ubc.ca/karthik/&#34;&gt;Karthik Pattabiraman&lt;/a&gt;, a&#xA;workshop on trustworthy machine learning attached to DSN 2018, in&#xA;Luxembourg: &lt;a href=&#34;https://dependablesecureml.github.io/&#34;&gt;DSML:&#xA;Dependable and Secure Machine Learning&lt;/a&gt;.&lt;/p&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;img src=&#34;//uvasrg.github.io/images/dsn2018.jpg&#34; width=&#34;80%&#34;&gt;&#xA;&lt;/center&gt;</description>
    </item>
    <item>
      <title>Wahoos at Oakland</title>
      <link>//uvasrg.github.io/wahoos-at-oakland/</link>
      <pubDate>Tue, 29 May 2018 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/wahoos-at-oakland/</guid>
      <description>&lt;h2 id=&#34;uva-group-dinner-at-ieee-security-and-privacy-2018&#34;&gt;UVA Group Dinner at IEEE Security and Privacy 2018&lt;/h2&gt;&#xA;&lt;p&gt;Including our newest faculty member, &lt;a href=&#34;https://www.cs.purdue.edu/homes/kwon58/#summary&#34;&gt;Yongwhi Kwon&lt;/a&gt;, joining UVA in Fall 2018!&lt;br /&gt;&lt;/p&gt;&#xA;&lt;center&gt;&lt;br /&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/srg2018/ORG_DSC07202.jpg&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/srg2018/ORG_DSC07202.jpg&#34; width=&#34;680&#34;&gt;&lt;/a&gt;&lt;br /&gt;&#xA;&lt;small&gt;Yuan Tian, Fnu Suya, Mainuddin Jonas, Yongwhi Kwon, David Evans, Weihang Wang, Aihua&amp;nbsp;Chen,&amp;nbsp;Weilin&amp;nbsp;Xu&lt;/small&gt;&lt;br /&gt;&#xA;&lt;/center&gt;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;&#xA;## Poster Session&#xA;&lt;table width=&#34;100%&#34;&gt;&#xA;&lt;tr valign=&#34;top&#34;&gt;&#xA;&lt;td width=&#34;50%&#34; align=&#34;center&#34;&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/srg2018/IMG_20180521_193906.jpg&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/srg2018/IMG_20180521_193906-3.jpg&#34; height=&#34;360&#34;&gt;&lt;/a&gt;&lt;br /&gt;&#xA;Fnu Suya (with Yuan Tian and David Evans), &lt;em&gt;Adversaries Don’t Care About Averages: Batch Attacks on Black-Box Classifiers&lt;/em&gt; &lt;a href=&#34;https://www.ieee-security.org/TC/SP2018/poster-abstracts/oakland2018-paper37-poster-abstract.pdf&#34;&gt;[PDF]&lt;/a&gt;&#xA;&lt;/td&gt;&#xA;&lt;td width=&#34;50%&#34; align=&#34;center&#34;&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/srg2018/IMG_20180521_193914.jpg&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/srg2018/IMG_20180521_193914-2.jpg&#34; height=&#34;360&#34;&gt;&lt;/a&gt;&lt;br /&gt;&#xA;Mainuddin Jonas (with David Evans), &lt;em&gt;Enhancing Adversarial Example Defenses Using Internal Layers&lt;/em&gt; &lt;a href=&#34;https://www.ieee-security.org/TC/SP2018/poster-abstracts/oakland2018-paper29-poster-abstract.pdf&#34;&gt;[PDF]&lt;/a&gt;&#xA;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;tr valign=&#34;top&#34;&gt;&#xA;&lt;td width=&#34;50%&#34; align=&#34;center&#34;&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/srg2018/IMG_20180522_153017.jpg&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/srg2018/IMG_20180522_153017-2.jpg&#34; height=&#34;300&#34;&gt;&lt;/a&gt;&#xA;&lt;/td&gt;&#xA;&lt;td width=&#34;50%&#34; align=&#34;center&#34;&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/srg2018/IMG_20180522_153109.jpg&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/srg2018/IMG_20180522_153109-2.jpg&#34; height=&#34;300&#34;&gt;&lt;/a&gt;&#xA;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/table&gt;</description>
    </item>
  </channel>
</rss>
