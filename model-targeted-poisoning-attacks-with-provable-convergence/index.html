<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Model-Targeted Poisoning Attacks with Provable Convergence | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      



	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      
<div style="margin-top:16px; margin-left: auto; margin-right: auto; max-width: 800px;">
    <article class="article" itemscope itemtype="http://schema.org/Article">
      
      <h1 itemprop="name">Model-Targeted Poisoning Attacks with Provable Convergence</h1>
      <div class="post-metadata">
  <span class="post-date">
    <time datetime="2021-06-29 00:00:00 &#43;0000 UTC" itemprop="datePublished">29 June 2021</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/model-targeted-poisoning-attacks">model-targeted poisoning attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/saeed-mahloujifar">Saeed Mahloujifar</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yuan-tian">Yuan Tian</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/icml-2021">ICML 2021</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>(Post by Sean Miller, using images adapted from Suya&rsquo;s talk slides)</p>
<h2 id="data-poisoning-attacks">Data Poisoning Attacks</h2>
<p>Machine learning models are often trained using data from untrusted
sources, leaving them open to poisoning attacks where adversaries use
their control over a small fraction of that training data to poison
the model in a particular way.</p>
<p>Most work on poisoning attacks is directly driven by an attacker&rsquo;s
objective, where the adversary chooses poisoning points that maximize
some target objective. Our work focuses on <em>model-targeted</em> poisoning
attacks, where the adversary splits the attack into choosing a target
model that satisfies the objective and then choosing poisoning points
that induce the target model.</p>
<center>
<a href="/images/model-targeted-poisoning/difference-between-model-types.png"><img src="/images/model-targeted-poisoning/difference-between-model-types.png" width="80%" align="center"></a>
</center><br/>
<p>The advantage of the model-targeted approach is that while
objective-driven attacks must be designed for a specific objective and
tend to result in difficult optimization problems for complex
objectives, model-targeted attacks only depend on the target
model. That model can be selected to incorporate any attacker
objective, allowing the same attack to be easily applied to many
different objectives.</p>
<h2 id="the-attack">The Attack</h2>
<p>Our attack requires the desired target model and the clean training
data. We sequentially train a model on the mixture of the clean
training points and the poisoning points found so far (which at the
start is none) in order to generate an intermediate model. We then
find a point that maximizes the loss difference between the
intermediate model and the target model, and then add that point to
the poisoning data for the next iteration.  The process repeats until
some stopping condition is met (such as the maximum loss difference
between the intermediate and target models being smaller than a
threshold value).</p>
<center>
<a href="/images/model-targeted-poisoning/attack-diagram.png"><img src="/images/model-targeted-poisoning/attack-diagram.png" width="80%" align="center"></a>
</center><br/>
<p>We prove two important features of our attack:</p>
<ol>
<li>
<p>If our loss function is Lipschitz continuous and strongly convex, the induced model converges to the target model. <em>This is the first model-targeted attack with provable convergence</em>.</p>
</li>
<li>
<p>For any loss function, we can empirically find a lower bound on the
number of poisoning points required to produce the target
classifier. This allows us to check the optimality of any
model-targeted attack.</p>
</li>
</ol>
<h2 id="experimental-results">Experimental Results</h2>
<p>To test our attack, we use subpopulation and indiscriminate attack scenarios on SVM and linear regression models for the Adult, MNIST 1-7, and Dogfish datasets. We compare our attack to the state-of-the-art model targeted KKT attack from Pang Wei Koh, Jacob Steinhardt, and Percy Liang, <a href="https://arxiv.org/abs/1811.00741"><em>Stronger data poisoning attacks break data sanitation defenses</em></a>, 2018.</p>
<p>Our attack steadily reduces the Euclidean distance to the target
model, indicating convergence, while the KKT attack does not reliably
converge to the target even as more poisoning points are used:</p>
<center>
<a href="/images/model-targeted-poisoning/attack-convergence.png"><img src="/images/model-targeted-poisoning/attack-convergence.png" width="50%" align="center"></a>
</center><br/>
<p>Next, we compare our attack to the KKT attack based on attack
success. For the subpopulation attack on the left, where the attacker
aims to reduce model accuracy only on a subpopulation of the data, our
attack is significantly more successful in increasing error on the
subpopulation than the KKT attack for the same number of poisoning
points. In the indiscriminate setting (right side of figure), where
the attacker aims to reduce overall model accuracy, our attack is
comparable to the KKT attack.</p>
<center>
<a href="/images/model-targeted-poisoning/success-comparison.png"><img src="/images/model-targeted-poisoning/success-comparison.png" width="90%" align="center"></a>
</center>
<p>Finally, we also compare our computed number of poisoning points to
the theoretical lower bound on points to see the optimality of our
attack. For the Adult dataset on the left, the gap between the lower
bound and the number of points used is small, so our attack is close
to optimal. However, for the other two datasets on the right, there
still is a gap between the lower bound and the number actually used,
indicating that the attack might not be optimal.</p>
<center>
<a href="/images/model-targeted-poisoning/optimality.png"><img src="/images/model-targeted-poisoning/optimality.png" width="90%" align="center"></a>
</center>
<h2 id="summary">Summary</h2>
<p>We propose a model-targeted poisoning attack that is proven to
converge theoretically and empirically, along with a lower bound on
the number of poisoning points needed. Since our attack is
model-targeted, we can select a target model that can achieve any goal
of an adversary and then induce that model through poisoning attacks,
allowing our attack to satisfy any number of objectives.</p>
<h2 id="full-paper">Full Paper</h2>
<p><a href="https://fsuya.org/">Fnu Suya</a>, <a href="https://smahloujifar.github.io/">Saeed Mahloujifar</a>, <a href="https://www.anshumansuri.com/">Anshuman Suri</a>, <a href="https://www.cs.virginia.edu/evans/">David Evans</a>, <a href="https://www.ytian.info/">Yuan Tian</a>. <a
href="https://arxiv.org/abs/2006.16469">Model-Targeted Poisoning
Attacks with Provable Convergence.</a> In <a href="https://icml.cc/Conferences/2021"><em>Thirty-eighth
International Conference on Machine
Learning</em></a> (ICML), July 2021. [<a href="https://arxiv.org/abs/2006.16469">arXiv</a>] [<a href="https://arxiv.org/pdf/2006.16469">PDF</a>]</p>
<h2 id="code">Code</h2>
<p><a href="https://github.com/suyeecav/model-targeted-poisoning">https://github.com/suyeecav/model-targeted-poisoning</a></p>

      </div>

      <meta itemprop="wordCount" content="688">
      <meta itemprop="datePublished" content="2021-06-29">
      <meta itemprop="url" content="//uvasrg.github.io/model-targeted-poisoning-attacks-with-provable-convergence/">
    </article>

    <ul class="pagination" role="navigation" aria-label="Pagination" style="margin-top:32px;">
      
      <li class="arrow" aria-disabled="true"><a href="//uvasrg.github.io/on-the-risks-of-distribution-inference/">&laquo; <em>Previous<span class="show-for-sr"> page</span></em>: On the Risks of Distribution Inference</a></li>
      
      
      <li class="arrow" aria-disabled="true"><a href="//uvasrg.github.io/uva-news-article/"><em>Next<span class="show-for-sr"> page</span></em>: UVA News Article&nbsp;&raquo;</a></li>
      
    </ul>
</div>

    </main>
    
    
<footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>


    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
