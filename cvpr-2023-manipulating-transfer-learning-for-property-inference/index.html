<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>CVPR 2023: Manipulating Transfer Learning for Property Inference | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      



	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      
<div style="margin-top:16px; margin-left: auto; margin-right: auto; max-width: 800px;">
    <article class="article" itemscope itemtype="http://schema.org/Article">
      
      <h1 itemprop="name">CVPR 2023: Manipulating Transfer Learning for Property Inference</h1>
      <div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-05-02 00:00:00 &#43;0000 UTC" itemprop="datePublished">2 May 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yulong-tian">Yulong Tian</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/transfer-learning">transfer learning</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <h1 id="manipulating-transfer-learning-for-property-inference">Manipulating Transfer Learning for Property Inference</h1>
<p>Transfer learning is a popular method to train deep learning models
efficiently. By reusing parameters from upstream pre-trained models,
the downstream trainer can use fewer computing resources to train
downstream models, compared to training models from scratch.</p>
<p>The figure below shows the typical process of transfer learning for
vision tasks:</p>
<center>
<a href="/images/mtlpi/fig1.png"><img src="/images/mtlpi/fig1.png" width="80%"></a>
</center>
<p>However, the nature of transfer learning can be exploited by a
malicious upstream trainer, leading to severe risks to the downstream
trainer.</p>
<p>Here, we consider the risk of amplifying property inference in
transfer learning scenarios. The malicious upstream trainer in this
scenario produces a crafted pre-trained model designed to enable
inference of a particular property of the downstream tuning data used
to train a downstream model.</p>
<p>The attack process is illustrated below:</p>
<center>
<a href="/images/mtlpi/fig2.png"><img src="/images/mtlpi/fig2.png" width="80%"></a>
</center>
<p>The main idea of the attack is to manipulate the upstream model
(<em>feature extractor</em>) to purposefully generate activations in
different distributions for samples with and without the target
property.  When the downstream trainer uses this upstream model for
transfer learning, the differences between the downstream models tuned
with and without samples that have the target property will also be
amplified, thus making the inference easier.</p>
<p>The adversary can then conduct the inference attacks with white-box
(e.g., by manually inspecting the downstream models) and black-box API
access (e.g., using meta-classifiers).</p>
<h3 id="zero-activation-attack">Zero Activation Attack</h3>
<p><strong>Upstream Manipulation.</strong> In this attack, the manipulation is
conducted in a way that certain parameters in the downstream model
will not be updated (e.g., have zero activations from feature
extractors on some <em><strong>secret-secreting parameters</strong></em> and hence zero
gradients in downstream training due to chain rule) if the tuning
data do not have the target property, but will be updated if some
tuning data are with the property (e.g., non-zero activations on the
secreting parameters and hence non-zero gradients in downstream
training).</p>
<center>
<a href="/images/mtlpi/fig3.png"><img src="/images/mtlpi/fig3.png" width="80%"></a>
</center>
<p><strong>Property Inference on Downstream Model.</strong> For the downstream model,
we can use inference attacks to infer sensitive properties of the
downstream training data.</p>
<p>In white-box settings where attacker has complete knowledge of the
model, in addition to evaluating standard white-box meta-classifier
based attacks (<em>white-box meta-classifier</em>), we propose two new
methods by directly comparing the actual values the secreting
parameters before and after downstream training (the <em>Difference</em>
attack) or by analyzing their variance in the final tuned model (the
<em>Variance</em> attack).</p>
<p>In the black-box setting with API access, attackers can employ
existing black-box methods such as black-box meta classifier based
approaches (<em>black-box meta-classifier</em>) and test based on confidence
scores returned for the queried samples (<em>Confidence score</em>).</p>
<center>
<a href="/images/mtlpi/results.jpg"><img src="/images/mtlpi/results.jpg" width="80%"></a>
</center>
<p><strong>Results.</strong> The results are summarize in the above
graphs. <em>Baseline</em> reports the highest inference success from
all existing attacks when the upstream model is trained normally
(i.e., without any manipulation). The results indicate that the
inference is much more successful with manipulation compared to the
baseline setting. In particular, in the baseline setting, most of
the inference AUC scores are below 0.7. However, after manipulation,
the inferences show AUC scores greater than 0.89 even when only 0.1%
(10 out of 10 000) of the downstream samples have the target
property. Moreover, the results achieve perfect scores (AUC score &gt;
0.99) when the ratio of target samples in the downstream training
set increases to 1% (100 out of 10 000).</p>
<p><strong>Stealthier Attack.</strong> Above results are only suitable for settings
where there are no active defenses to inspect the pertained
models. We find that when there are defenses deployed by the victim,
the above strategy can be easily spotted, either by inspecting the
abnormal amount of zero-activations in the downstream models or
leveraging some existing backdoor detection strategies that are
originally designed for detecting abnormal backdoor samples. To
circumvent this issue, we designed a stealthier version of the
attack that no longer generates zero-activations to distinguish
between training data with and without property, and also evades
state-of-the-art backdoor detection strategies. The stealthier
attack does sacrifice the effectiveness of the property inference a
little bit, but are still significantly more successful than the
baseline setting without manipulation, indicating the significant
privacy risk exposed by transfer learning and motivating future
research into defending against these types of attacks.</p>
<h3 id="paper">Paper</h3>
<p>Yulong Tian, Fnu Suya, Anshuman Suri, Fengyuan Xu, David Evans. <a href="https://arxiv.org/abs/2303.11643"><em>Manipulating Transfer Learning for Property Inference</em></a>. In <a href="https://cvpr2023.thecvf.com/">IEEE/CVF Computer Vision and Pattern Recognition Conference</a> (CVPR). Vancouver, 18â€“22 June 2023. <a href="https://arxiv.org/abs/2303.11643">[arXiv]</a></p>
<p>Code: <a href="https://github.com/yulongt23/transfer-inference">https://github.com/yulongt23/transfer-inference</a></p>

      </div>

      <meta itemprop="wordCount" content="713">
      <meta itemprop="datePublished" content="2023-05-02">
      <meta itemprop="url" content="//uvasrg.github.io/cvpr-2023-manipulating-transfer-learning-for-property-inference/">
    </article>

    <ul class="pagination" role="navigation" aria-label="Pagination" style="margin-top:32px;">
      
      <li class="arrow" aria-disabled="true"><a href="//uvasrg.github.io/mico-challenge-in-membership-inference/">&laquo; <em>Previous<span class="show-for-sr"> page</span></em>: MICO Challenge in Membership Inference</a></li>
      
      
      <li class="arrow" aria-disabled="true"><a href="//uvasrg.github.io/sok-let-the-privacy-games-begin-a-unified-treatment-of-data-inference-privacy-in-machine-learning/"><em>Next<span class="show-for-sr"> page</span></em>: SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning&nbsp;&raquo;</a></li>
      
    </ul>
</div>

    </main>
    
    
<footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>


    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
