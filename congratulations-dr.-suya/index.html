<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Congratulations, Dr. Suya! | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      



	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      
<div style="margin-top:16px; margin-left: auto; margin-right: auto; max-width: 800px;">
    <article class="article" itemscope itemtype="http://schema.org/Article">
      
      <h1 itemprop="name">Congratulations, Dr. Suya!</h1>
      <div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-07-11 00:00:00 &#43;0000 UTC" itemprop="datePublished">11 July 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning">poisoning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/phd-defense">PhD defense</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Congratulations to <a href="fsuya.org">Fnu Suya</a> for successfully defending
his PhD thesis!</p>
<p>Suya will join the Unversity of Maryland as a MC2 Postdoctoral Fellow
at the <a href="https://cyber.umd.edu/about">Maryland Cybersecurity Center</a> this fall.</p>
<h2 id="heading"></h2>
<center>
<em>
On the Limits of Data Poisoning Attacks
</em>
</center>
<h2 id="heading-1"></h2>
<p>Current machine learning models require large amounts of labeled training data, which are often collected from untrusted sources. Models trained on these potentially manipulated data points are prone to data poisoning attacks. My research aims to gain a deeper understanding on the limits of two types of data poisoning attacks: indiscriminate poisoning attacks, where the attacker aims to increase the test error on the entire dataset; and subpopulation poisoning attacks, where the attacker aims to increase the test error on a defined subset of the distribution. We first present an empirical poisoning attack that encodes the attack objectives into target models and then generates poisoning points that induce the target models (and hence the encoded objectives) with provable convergence. This attack achieves state-of-the-art performance for a diverse set of attack objectives and quantifies a lower bound to the performance of best possible poisoning attacks. In the broader sense, because the attack guarantees convergence to the target model which encodes the desired attack objective, our attack can also be applied to objectives related to other trustworthy aspects (e.g., privacy, fairness) of machine learning.</p>
<p>Through experiments for the two types of poisoning attacks we consider, we find that some datasets in the indiscriminate setting and subpopulations in the subpopulation setting are highly vulnerable to poisoning attacks even when the poisoning ratio is low. But other datasets and subpopulations resist the best-performing known attacks even without any defensive protections. Motivated by the drastic differences in the attack effectiveness across datasets or subpopulations, we further investigate the possible factors related to the data distribution and learning algorithm that contribute to the disparate effectiveness of poisoning attacks. In the subpopulation setting, for the given learner, we identify the separability of the class-wise distributions and also the difference of the model that misclassifies the subpopulations to the clean model are highly correlated to the empirical performance of state-of-the-art poisoning attacks and demonstrate them through visualizations. In the indiscriminate setting, we conduct a more thorough investigation by first showing under theoretical distributions that there are datasets that inherently resist the best possible poisoning attacks when the class-wise data distributions are well-separated with low variance and the size of the constraint set containing all permissible poisoning points is also small. We then demonstrate that these identified factors are highly correlated to both the different empirical performances of the state-of-the-art attacks (as lower bounds on the limits of poisoning attacks) and the upper bounds on the limits across benchmark datasets. Finally, we discuss how understanding the limits of poisoning attacks might help in complementing existing data sanitization defenses to achieve even stronger defenses against poisoning attacks.</p>
<h1 id="heading-2"></h1>
<p><strong>Committee:</strong><br>
Mohammad Mahmoody, Committee Chair (UVA Computer Science)<br>
David Evans, Co-Advisor (UVA Computer Science)<br>
Yuan Tian, Co-Advisor (UCLA)<br>
Cong Shen (UVA ECE)<br>
Farzad Hassanzadeh (UVA Computer Science/ECE)</p>
<h1 id="heading-3"></h1>
<p>Dissertation in Libra: <a href="https://libraetd.lib.virginia.edu/public_view/2j62s635c"><em>On the Limits of Data Poisoning Attacks</em></a></p>

      </div>

      <meta itemprop="wordCount" content="510">
      <meta itemprop="datePublished" content="2023-07-11">
      <meta itemprop="url" content="//uvasrg.github.io/congratulations-dr.-suya/">
    </article>

    <ul class="pagination" role="navigation" aria-label="Pagination" style="margin-top:32px;">
      
      <li class="arrow" aria-disabled="true"><a href="//uvasrg.github.io/sok-let-the-privacy-games-begin-a-unified-treatment-of-data-inference-privacy-in-machine-learning/">&laquo; <em>Previous<span class="show-for-sr"> page</span></em>: SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning</a></li>
      
      
      <li class="arrow" aria-disabled="true"><a href="//uvasrg.github.io/adjectives-can-reveal-gender-biases-within-nlp-models/"><em>Next<span class="show-for-sr"> page</span></em>: Adjectives Can Reveal Gender Biases Within NLP Models&nbsp;&raquo;</a></li>
      
    </ul>
</div>

    </main>
    
    
<footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>


    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
