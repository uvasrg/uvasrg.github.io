<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bargav Jayaraman on Security Research Group</title>
    <link>//uvasrg.github.io/tags/bargav-jayaraman/</link>
    <description>Recent content in Bargav Jayaraman on Security Research Group</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Wed, 07 Dec 2022 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/tags/bargav-jayaraman/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Attribute Inference attacks are really Imputation</title>
      <link>//uvasrg.github.io/attribute-inference-attacks-are-really-imputation/</link>
      <pubDate>Wed, 07 Dec 2022 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/attribute-inference-attacks-are-really-imputation/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Post by Bargav Jayaraman&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;Attribute inference&lt;/em&gt; attacks have been shown by prior works to pose privacy threat against ML models. However, these works assume the knowledge of the training distribution and we show that in such cases these attacks do no better than a data imputataion attack that does not have access to the model. We explore the attribute inference risks in the cases where the adversary has limited or no prior knowledge of the training distribution and show that our white-box attribute inference attack (that uses neuron activations to infer the unknown sensitive attribute) surpasses imputation in these data constrained cases. This attack uses the training distribution information leaked by the model, and thus poses privacy risk when the distribution is private.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Congratulations, Dr. Jayaraman!</title>
      <link>//uvasrg.github.io/congratulations-dr.-jayaraman/</link>
      <pubDate>Fri, 02 Dec 2022 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/congratulations-dr.-jayaraman/</guid>
      <description>&lt;p&gt;Congratulations to Bargav Jayaraman for successfully &lt;a href=&#34;https://engineering.virginia.edu/events/phd-defense-presentation-bargav-jayaraman&#34;&gt;defending his PhD thesis&lt;/a&gt;!&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;img src=&#34;//uvasrg.github.io/images/bargav-defense.jpg&#34; width=&#34;65%&#34;&gt;&#xA;&lt;div class=&#34;caption&#34;&gt;&lt;center&gt;&#xA;Dr. Jayaraman and his PhD committee: Mohammad&amp;nbsp;Mahmoody, Quanquan&amp;nbsp;Gu (UCLA Department of Computer Science, on screen), Yanjun&amp;nbsp;Qi (Committee Chair, on screen), Denis&amp;nbsp;Nekipelov (Department of Economics, on screen), and David Evans&#xA;&lt;/div&gt;&#xA;&lt;/center&gt;&#xA;&lt;h2 id=&#34;heading&#34;&gt;&lt;/h2&gt;&#xA;&lt;p&gt;Bargav will join the Meta AI Lab in Menlo Park, CA as a post-doctoral researcher.&lt;/p&gt;&#xA;&lt;h2 id=&#34;heading-1&#34;&gt;&lt;/h2&gt;&#xA;&lt;center&gt;&#xA;&lt;em&gt;Analyzing the Leaky Cauldron: Inference Attacks on Machine Learning&lt;/em&gt;&#xA;&lt;/center&gt;&#xA;&lt;h2 id=&#34;heading-2&#34;&gt;&lt;/h2&gt;&#xA;&lt;p&gt;Machine learning models have been shown to leak sensitive information about their training data. An adversary having access to the model can infer different types of sensitive information, such as learning if a particular individual&amp;rsquo;s data is in the training set, extracting sensitive patterns like passwords in the training set, or predicting missing sensitive attribute values for partially known training records. This dissertation quantifies this privacy leakage. We explore inference attacks against machine learning models including membership inference, pattern extraction, and attribute inference. While our attacks give an empirical lower bound on the privacy leakage, we also provide a theoretical upper bound on the privacy leakage metrics. Our experiments across various real-world data sets show that the membership inference attacks can infer a subset of candidate training records with high attack precision, even in challenging cases where the adversary&amp;rsquo;s candidate set is mostly non-training records. In our pattern extraction experiments, we show that an adversary is able to recover email ids, passwords and login credentials from large transformer-based language models. Our attribute inference adversary is able to use underlying training distribution information inferred from the model to confidently identify candidate records with sensitive attribute values. We further evaluate the privacy risk implication to individuals contributing their data for model training. Our findings suggest that different subsets of individuals are vulnerable to different membership inference attacks, and that some individuals are repeatedly identified across multiple runs of an attack. For attribute inference, we find that a subset of candidate records with a sensitive attribute value are correctly predicted by our white-box attribute inference attacks but would be misclassified by an imputation attack that does not have access to the target model. We explore different defense strategies to mitigate the inference risks, including approaches that avoid model overfitting such as early stopping and differential privacy, and approaches that remove sensitive data from the training. We find that differential privacy mechanisms can thwart membership inference and pattern extraction attacks, but even differential privacy fails to mitigate the attribute inference risks since the attribute inference attack relies on the distribution information leaked by the model whereas differential privacy provides no protection against leakage of distribution statistics.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Microsoft Research Summit: Surprising (and unsurprising) Inference Risks in Machine Learning</title>
      <link>//uvasrg.github.io/microsoft-research-summit-surprising-and-unsurprising-inference-risks-in-machine-learning/</link>
      <pubDate>Thu, 21 Oct 2021 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/microsoft-research-summit-surprising-and-unsurprising-inference-risks-in-machine-learning/</guid>
      <description>&lt;p&gt;Here are the slides for my talk at the &lt;a href=&#34;https://www.microsoft.com/en-us/research/theme/confidential-computing/#workshops&#34;&gt;&lt;em&gt;Practical and Theoretical Privacy of Machine Learning Training Pipelines&lt;/em&gt;&lt;/a&gt;&#xA;Workshop at the Microsoft Research Summit (21 October 2021):&lt;/p&gt;&#xA;   &lt;center&gt;&#xA;&lt;a href=&#34;https://www.dropbox.com/s/1mfhbelv7qx4t3u/surprisinginferences.pdf?dl=0&#34;&gt;&lt;b&gt;Surprising (and Unsurprising) Inference Risks in Machine Learning&lt;/b&gt; [PDF]&lt;/a&gt;&#xA;   &lt;/center&gt;&#xA;&lt;h2 id=&#34;heading&#34;&gt;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;heading-1&#34;&gt;&lt;/h2&gt;&#xA;&lt;p&gt;The work by Bargav Jayaraman (with Katherine Knipmeyer, Lingxiao Wang,&#xA;and Quanquan Gu) that I talked about on improving membership inference&#xA;attacks is described in more details here:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Bargav Jayaraman, Lingxiao Wang, Katherine Knipmeyer, Quanquan Gu, David Evans. &lt;a href=&#34;https://arxiv.org/abs/2005.10881&#34;&gt;&lt;em&gt;Revisiting Membership Inference Under Realistic Assumptions&lt;/em&gt;&lt;/a&gt; (PETS 2021).&lt;br&gt;&#xA;[&lt;a href=&#34;//uvasrg.github.io/merlin-morgan-and-the-importance-of-thresholds-and-priors/&#34;&gt;Blog&lt;/a&gt;] [Code: &lt;a href=&#34;https://github.com/bargavj/EvaluatingDPML&#34;&gt;&lt;em&gt;https://github.com/bargavj/EvaluatingDPML&lt;/em&gt;&lt;/a&gt;]&lt;/p&gt;</description>
    </item>
    <item>
      <title>UVA News Article</title>
      <link>//uvasrg.github.io/uva-news-article/</link>
      <pubDate>Tue, 28 Sep 2021 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/uva-news-article/</guid>
      <description>&lt;p&gt;UVA News has an article by Audra Book on our research on security and&#xA;privacy of machine learning (with some very nice quotes from several&#xA;students in the group, and me saying something positive about the&#xA;NSA!): &lt;a href=&#34;https://engineering.virginia.edu/news/2021/09/computer-science-professor-david-evans-and-his-team-conduct-experiments-understand&#34;&gt;&lt;em&gt;Computer science professor David Evans and his team conduct&#xA;experiments to understand security and privacy risks associated with&#xA;machine&#xA;learning&lt;/em&gt;&lt;/a&gt;,&#xA;8 September 2021.&lt;/p&gt;&#xA;&lt;div class=&#34;articletext&#34;&gt;&#xA;&lt;p&gt;David Evans, professor of computer science in the University of Virginia School of Engineering and Applied Science, is leading research to understand how machine learning models can be compromised.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ICLR DPML 2021: Inference Risks for Machine Learning</title>
      <link>//uvasrg.github.io/iclr-dpml-2021-inference-risks-for-machine-learning/</link>
      <pubDate>Fri, 07 May 2021 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/iclr-dpml-2021-inference-risks-for-machine-learning/</guid>
      <description>&lt;p&gt;I gave an invited talk at the &lt;a href=&#34;https://dp-ml.github.io/2021-workshop-ICLR/&#34;&gt;Distributed and Private Machine Learning&lt;/a&gt; (DPML) workshop at ICLR 2021 on &lt;a href=&#34;https://iclr.cc/virtual/2021/workshop/2148#collapse3549&#34;&gt;&lt;em&gt;Inference Risks for Machine Learning&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube-nocookie.com/embed/zgSTsO1LKSs?controls=0&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/center&gt;&#xA;&lt;p&gt;The talk mostly covers work by Bargav Jayaraman on evaluating privacy in&#xA;machine learning and connecting attribute inference and imputation, and recent work by Anshuman Suri on property inference.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Codaspy 2021 Keynote: When Models Learn Too Much</title>
      <link>//uvasrg.github.io/codaspy-2021-keynote-when-models-learn-too-much/</link>
      <pubDate>Mon, 26 Apr 2021 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/codaspy-2021-keynote-when-models-learn-too-much/</guid>
      <description>&lt;p&gt;Here are the slides for my talk at the&#xA;&lt;a href=&#34;http://www.codaspy.org/2021/program.html&#34;&gt;11th ACM Conference on Data and Application Security and Privacy&lt;/a&gt;:&lt;/p&gt;&#xA;   &lt;center&gt;&#xA;&lt;a href=&#34;https://www.dropbox.com/s/6wzloxuai709s0k/codaspy-post.pdf?dl=0)&#34;&gt;&lt;b&gt;When Models Learn Too Much&lt;/b&gt; [PDF]&lt;/a&gt;&#xA;   &lt;/center&gt;&#xA;&lt;h2 id=&#34;heading&#34;&gt;&lt;/h2&gt;&#xA;&lt;p&gt;The talk includes Bargav Jayaraman&amp;rsquo;s work (with Katherine Knipmeyer, Lingxiao Wang, and Quanquan Gu) on evaluating privacy in machine learning, as well as more recent work by Anshuman Suri on property inference attacks, and Bargav on attribute inference and imputation:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;//uvasrg.github.io/merlin-morgan-and-the-importance-of-thresholds-and-priors/&#34;&gt;&lt;em&gt;Merlin, Morgan, and the Importance of Thresholds and Priors&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;//uvasrg.github.io/evaluating-differentially-private-machine-learning-in-practice/&#34;&gt;&lt;em&gt;Evaluating Differentially Private Machine Learning in Practice&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;center&gt;&#xA;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;“When models learn too much. “ Dr. David Evans &lt;a href=&#34;https://twitter.com/UdacityDave?ref_src=twsrc%5Etfw&#34;&gt;@UdacityDave&lt;/a&gt; of University of Virginia gave a keynote talk on different inference risks for machine learning models this morning at &lt;a href=&#34;https://twitter.com/hashtag/codaspy21?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#codaspy21&lt;/a&gt; &lt;a href=&#34;https://t.co/KVgFoUA6sa&#34;&gt;pic.twitter.com/KVgFoUA6sa&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Microsoft Security Data Science Colloquium: Inference Privacy in Theory and Practice</title>
      <link>//uvasrg.github.io/microsoft-security-data-science-colloquium-inference-privacy-in-theory-and-practice/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/microsoft-security-data-science-colloquium-inference-privacy-in-theory-and-practice/</guid>
      <description>&lt;p&gt;Here are the slides for my talk at the Microsoft Security Data Science Colloquium:&lt;br&gt;&#xA;&lt;a href=&#34;https://www.dropbox.com/s/698cuvee81clx1q/inference-privacy-share.pdf?dl=0&#34;&gt;&lt;em&gt;When Models Learn Too Much: Inference Privacy in Theory and Practice&lt;/em&gt; [PDF]&lt;/a&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;https://www.dropbox.com/s/698cuvee81clx1q/inference-privacy-share.pdf?dl=0&#34;&gt;&#xA;&lt;img src=&#34;//uvasrg.github.io/images/inferenceprivacytitle.png&#34; width=65%&gt;&#xA;&lt;/a&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;The talk is mostly about Bargav Jayaraman&amp;rsquo;s work (with Katherine Knipmeyer, Lingxiao Wang, and Quanquan Gu) on evaluating privacy:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;//uvasrg.github.io/merlin-morgan-and-the-importance-of-thresholds-and-priors/&#34;&gt;&lt;em&gt;Merlin, Morgan, and the Importance of Thresholds and Priors&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;//uvasrg.github.io/evaluating-differentially-private-machine-learning-in-practice/&#34;&gt;&lt;em&gt;Evaluating Differentially Private Machine Learning in Practice&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Merlin, Morgan, and the Importance of Thresholds and Priors</title>
      <link>//uvasrg.github.io/merlin-morgan-and-the-importance-of-thresholds-and-priors/</link>
      <pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/merlin-morgan-and-the-importance-of-thresholds-and-priors/</guid>
      <description>&lt;p&gt;&lt;em&gt;Post by Katherine Knipmeyer&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Machine learning poses a substantial risk that adversaries will be&#xA;able to discover information that the model does not intend to&#xA;reveal. One set of methods by which consumers can learn this sensitive&#xA;information, known broadly as &lt;em&gt;membership inference&lt;/em&gt; attacks,&#xA;predicts whether or not a query record belongs to the training set. A&#xA;basic membership inference attack involves an attacker with a given&#xA;record and black-box access to a model who tries to determine whether&#xA;said record was a member of the model’s training set.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Research Symposium Posters</title>
      <link>//uvasrg.github.io/research-symposium-posters/</link>
      <pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/research-symposium-posters/</guid>
      <description>&lt;p&gt;Five students from our group presented posters at the department&amp;rsquo;s&#xA;&lt;a href=&#34;https://engineering.virginia.edu/cs-research-symposium-fall-2019&#34;&gt;Fall Research&#xA;Symposium&lt;/a&gt;:&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;iframe src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vQcaahIxPyCHIMJti6tRB9HM_RreRhZkGH4wCN7YjTwiHSqcHod9v3hDFj-ZS1TtXp9OtBEBCV8OPH4/embed?start=false&amp;loop=false&amp;delayms=3000&#34; frameborder=&#34;0&#34; width=&#34;764&#34; height=&#34;453&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;&lt;br&gt;&#xA;Anshuman Suri&#39;s Overview Talk&#xA;&lt;/center&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/evaluatingdpml-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt; &lt;br&gt;&#xA;Bargav Jayaraman, &lt;em&gt;Evaluating Differentially Private Machine Learning In Practice&lt;/em&gt; &#xA;[&lt;a href=&#34;&#xA;&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/evaluatingdpml-poster.pdf&#34;&gt;Poster&lt;/a&gt;]&lt;br&gt;&#xA;[&lt;a href=&#34;https://www.cs.virginia.edu/~evans/pubs/usenix2019/&#34;&gt;Paper&lt;/a&gt; (USENIX Security 2019)]&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/pretrainedvulnerable-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt;&lt;br&gt;&#xA;Hannah Chen [&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/pretrainedvulnerable-poster.pdf&#34;&gt;Poster&lt;/a&gt;]&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/measuringconcentration-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt;&lt;br&gt;&#xA;Xiao Zhang [&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/measuringconcentration-poster.pdf&#34;&gt;Poster&lt;a&gt;]&lt;br&gt;&#xA;[&lt;a href=&#34;https://arxiv.org/abs/1905.12202&#34;&gt;Paper&lt;/a&gt; (NeurIPS 2019)]&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/diversemodels-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt;&lt;br&gt;&#xA;Mainudding Jonas [&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/diversemodels-poster.pdf&#34;&gt;Poster&lt;/a&gt;]&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/hybridbatch-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt;&lt;br&gt;&#xA;Fnu Suya [&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/hybridbatch-poster.pdf&#34;&gt;Poster&lt;a&gt;]&lt;br&gt;&#xA;[&lt;a href=&#34;https://arxiv.org/abs/1908.07000&#34;&gt;Paper&lt;/a&gt; (USENIX Security 2020)]&#xA;&lt;/center&gt;</description>
    </item>
    <item>
      <title>Evaluating Differentially Private Machine Learning in Practice</title>
      <link>//uvasrg.github.io/evaluating-differentially-private-machine-learning-in-practice/</link>
      <pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/evaluating-differentially-private-machine-learning-in-practice/</guid>
      <description>&lt;p&gt;(Cross-post by &lt;a href=&#34;https://bargavjayaraman.github.io/post/evaluating-dpml-results/&#34;&gt;Bargav Jayaraman&lt;/a&gt;)&lt;/p&gt;&#xA;&lt;p&gt;With the recent advances in composition of differential private&#xA;mechanisms, the research community has been able to achieve meaningful&#xA;deep learning with privacy budgets in single digits. Rènyi&#xA;differential privacy (RDP) is one mechanism that provides tighter&#xA;composition which is widely used because of its implementation in&#xA;TensorFlow Privacy (recently, Gaussian differential privacy (GDP) has&#xA;shown a tighter analysis for low privacy budgets, but it was not yet&#xA;available when we did this work). But the central question that&#xA;remains to be answered is: &lt;em&gt;how private are these methods in&#xA;practice?&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>USENIX Security Symposium 2019</title>
      <link>//uvasrg.github.io/usenix-security-symposium-2019/</link>
      <pubDate>Mon, 26 Aug 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/usenix-security-symposium-2019/</guid>
      <description>&lt;p&gt;Bargav Jayaraman presented our paper on &lt;a href=&#34;https://arxiv.org/abs/1902.08874&#34;&gt;&lt;em&gt;Evaluating Differentially Private Machine Learning in Practice&lt;/em&gt;&lt;/a&gt; at the &lt;a&#xA;href=&#34;https://www.usenix.org/conference/usenixsecurity19&#34;&gt;28&lt;sup&gt;th&lt;/sup&gt;&#xA;USENIX Security Symposium&lt;/em&gt;&lt;/a&gt; in Santa Clara, California.&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube-nocookie.com/embed/JAGhqbY_U50&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&#xA;&lt;/center&gt;&#xA;&lt;center&gt;&#xA;&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;dfdd40e4ba2b46e1baee68219df82de7&#34; data-ratio=&#34;1.29456384323641&#34; src=&#34;//speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;&#xA;&lt;/center&gt;&#xA;&lt;center&gt;&lt;img src=&#34;//uvasrg.github.io/images/usenix2019/bargav.jpg&#34; width=80%&#34;&gt;&lt;/center&gt;&#xA;&lt;p&gt;Summary by Lea Kissner:&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Hey it&amp;#39;s the results! &lt;a href=&#34;https://t.co/ru1FbkESho&#34;&gt;pic.twitter.com/ru1FbkESho&lt;/a&gt;&lt;/p&gt;&amp;mdash; Lea Kissner (@LeaKissner) &lt;a href=&#34;https://twitter.com/LeaKissner/status/1162518239177371648?ref_src=twsrc%5Etfw&#34;&gt;August 17, 2019&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;Also, great to see several UVA folks at the conference including:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://havron.dev&#34;&gt;Sam Havron&lt;/a&gt; (BSCS 2017, now a PhD student at&#xA;Cornell) presented a paper on the work he and his colleagues have&#xA;done on &lt;a href=&#34;https://havron.dev/pubs/clinicalsec.pdf&#34;&gt;computer security for victims of intimate partner violence&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;center&gt;&lt;img src=&#34;//uvasrg.github.io/images/usenix2019/havron.jpg&#34; width=80%&#34;&gt;&lt;/center&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.guanotronic.com/~serge/&#34;&gt;Serge Egelman&lt;/a&gt; (BSCS 2004) was an author on the paper &lt;a href=&#34;https://www.usenix.org/conference/usenixsecurity19/presentation/reardon&#34;&gt;&lt;em&gt;50 Ways to Leak Your Data: An&#xA;Exploration of Apps&amp;rsquo; Circumvention of the Android Permissions&#xA;System&lt;/em&gt;&lt;/a&gt;&#xA;(which was recognized by a Distinguished Paper Award). His paper in&#xA;SOUPS on &lt;a href=&#34;https://www.usenix.org/system/files/soups2019-frik.pdf&#34;&gt;&lt;em&gt;Privacy and Security Threat Models and Mitigation&#xA;Strategies of Older&#xA;Adults&lt;/em&gt;&lt;/a&gt; was highlighted in Alex Stamos&amp;rsquo; excellent talk.&lt;/p&gt;</description>
    </item>
    <item>
      <title>NeurIPS 2018: Distributed Learning without Distress</title>
      <link>//uvasrg.github.io/neurips-2018-distributed-learning-without-distress/</link>
      <pubDate>Sat, 08 Dec 2018 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/neurips-2018-distributed-learning-without-distress/</guid>
      <description>&lt;p&gt;Bargav Jayaraman presented our work on privacy-preserving machine learning at the &lt;a href=&#34;https://nips.cc/Conferences/2018/&#34;&gt;32&lt;sup&gt;nd&lt;/sup&gt; &lt;em&gt;Conference on Neural Information Processing Systems&lt;/em&gt;&lt;/a&gt; (NeurIPS 2018) in Montreal.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;Distributed learning&lt;/em&gt; (sometimes known as &lt;em&gt;federated learning&lt;/em&gt;)&#xA;allows a group of independent data owners to collaboratively learn a&#xA;model over their data sets without exposing their private data.  Our&#xA;approach combines &lt;em&gt;differential privacy&lt;/em&gt; with secure &lt;em&gt;multi-party&#xA;computation&lt;/em&gt; to both protect the data during training and produce a&#xA;model that provides privacy against inference attacks.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
