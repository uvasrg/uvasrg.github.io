<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Copilot on Security Research Group</title>
    <link>//uvasrg.github.io/tags/copilot/</link>
    <description>Recent content in Copilot on Security Research Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Wed, 21 Aug 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/tags/copilot/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Researchers Highlight How Poisoned LLMs Can Suggest Vulnerable Code</title>
      <link>//uvasrg.github.io/researchers-highlight-how-poisoned-llms-can-suggest-vulnerable-code/</link>
      <pubDate>Wed, 21 Aug 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/researchers-highlight-how-poisoned-llms-can-suggest-vulnerable-code/</guid>
      <description>I&amp;rsquo;m quoted in this story by Rob Lemos about poisoning code models (the CodeBreaker paper in USENIX Security 2024 by Shenao Yan, Shen Wang, Yue Duan, Hanbin Hong, Kiho Lee, Doowon Kim, and Yuan Hong), that considers a similar threat to our TrojanPuzzle work:&#xA;Researchers Highlight How Poisoned LLMs Can Suggest Vulnerable Code&#xA;Dark Reading, 20 August 2024&#xA;CodeBreaker uses code transformations to create vulnerable code that continues to function as expected, but that will not be detected by major static analysis security testing.</description>
    </item>
    <item>
      <title>Uh-oh, there&#39;s a new way to poison code models</title>
      <link>//uvasrg.github.io/uh-oh-theres-a-new-way-to-poison-code-models/</link>
      <pubDate>Mon, 16 Jan 2023 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/uh-oh-theres-a-new-way-to-poison-code-models/</guid>
      <description>Jack Clark&amp;rsquo;s Import AI, 16 Jan 2023 includes a nice description of our work on TrojanPuzzle:&#xA;####################################################&#xA;Uh-oh, there&#39;s a new way to poison code models - and it&#39;s really hard to detect:&#xA;…TROJANPUZZLE is a clever way to trick your code model into betraying you - if you can poison the undelrying dataset…&#xA;Researchers with the University of California, Santa Barbara, Microsoft Corporation, and the University of Virginia have come up with some clever, subtle ways to poison the datasets used to train code models.</description>
    </item>
    <item>
      <title>Trojan Puzzle attack trains AI assistants into suggesting malicious code</title>
      <link>//uvasrg.github.io/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/</link>
      <pubDate>Tue, 10 Jan 2023 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/</guid>
      <description>Bleeping Computer has a story on our work (in collaboration with Microsoft Research) on poisoning code suggestion models:&#xA;Trojan Puzzle attack trains AI assistants into suggesting malicious code By Bill Toulas&#xA;Researchers at the universities of California, Virginia, and Microsoft have devised a new poisoning attack that could trick AI-based coding assistants into suggesting dangerous code.&#xA;Named &amp;lsquo;Trojan Puzzle,&amp;rsquo; the attack stands out for bypassing static detection and signature-based dataset cleansing models, resulting in the AI models being trained to learn how to reproduce dangerous payloads.</description>
    </item>
  </channel>
</rss>
