<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Membership Inference on Security Research Group</title>
    <link>//uvasrg.github.io/tags/membership-inference/</link>
    <description>Recent content in Membership Inference on Security Research Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Tue, 26 Nov 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/tags/membership-inference/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Common Way To Test for Leaks in Large Language Models May Be Flawed</title>
      <link>//uvasrg.github.io/common-way-to-test-for-leaks-in-large-language-models-may-be-flawed/</link>
      <pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/common-way-to-test-for-leaks-in-large-language-models-may-be-flawed/</guid>
      <description>Anshuman Suri and Pratyush Maini wrote a blog about the EMNLP 2024 best paper award winner: Reassessing EMNLP 2024â€™s Best Paper: Does Divergence-Based Calibration for Membership Inference Attacks Hold Up?.&#xA;As we explored in Do Membership Inference Attacks Work on Large Language Models?, to test a membership inference attack it is essentail to have a candidate set where the members and non-members are from the same distribution. If the distributions are different, the ability of an attack to distinguish members and non-members is indicative of distribution inference, not necessarily membership inference.</description>
    </item>
    <item>
      <title>Common Way To Test for Leaks in Large Language Models May Be Flawed</title>
      <link>//uvasrg.github.io/common-way-to-test-for-leaks-in-large-language-models-may-be-flawed/</link>
      <pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/common-way-to-test-for-leaks-in-large-language-models-may-be-flawed/</guid>
      <description>UVA News has an article on our LLM membership inference work: Common Way To Test for Leaks in Large Language Models May Be Flawed: UVA Researchers Collaborated To Study the Effectiveness of Membership Inference Attacks, Eric Williamson, 13 November 2024.</description>
    </item>
    <item>
      <title>Congratulations, Dr. Suri!</title>
      <link>//uvasrg.github.io/congratulations-dr.-suri/</link>
      <pubDate>Thu, 18 Jul 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/congratulations-dr.-suri/</guid>
      <description>Congratulations to Anshuman Suri for successfully defending his PhD thesis!&#xA;Tianhao Wang, Dr. Anshuman Suri, Nando Fioretto, Cong Shen&#xA;On Screen: David Evans, Giuseppe Ateniese Inference Privacy in Machine Learning Using machine learning models comes at the risk of leaking information about data used in their training and deployment. This leakage can expose sensitive information about properties of the underlying data distribution, data from participating users, or even individual records in the training data.</description>
    </item>
    <item>
      <title>SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning</title>
      <link>//uvasrg.github.io/sok-let-the-privacy-games-begin-a-unified-treatment-of-data-inference-privacy-in-machine-learning/</link>
      <pubDate>Fri, 26 May 2023 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/sok-let-the-privacy-games-begin-a-unified-treatment-of-data-inference-privacy-in-machine-learning/</guid>
      <description>Our paper on the use of cryptographic-style games to model inference privacy is published in IEEE Symposium on Security and Privacy (Oakland):</description>
    </item>
    <item>
      <title>MICO Challenge in Membership Inference</title>
      <link>//uvasrg.github.io/mico-challenge-in-membership-inference/</link>
      <pubDate>Fri, 10 Feb 2023 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/mico-challenge-in-membership-inference/</guid>
      <description>Anshuman Suri wrote up an interesting post on his experience with the MICO Challenge, a membership inference competition that was part of SaTML. Anshuman placed second in the competition (on the CIFAR data set), where the metric is highest true positive rate at a 0.1 false positive rate over a set of models (some trained using differential privacy and some without).&#xA;Anshuman&amp;rsquo;s post describes the methods he used and his experience in the competition: My submission to the MICO Challenge.</description>
    </item>
  </channel>
</rss>
