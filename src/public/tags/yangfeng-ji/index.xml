<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yangfeng Ji on Security Research Group</title>
    <link>//uvasrg.github.io/tags/yangfeng-ji/</link>
    <description>Recent content in Yangfeng Ji on Security Research Group</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Tue, 04 Mar 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/tags/yangfeng-ji/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Is Taiwan a Country?</title>
      <link>//uvasrg.github.io/is-taiwan-a-country/</link>
      <pubDate>Tue, 04 Mar 2025 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/is-taiwan-a-country/</guid>
      <description>&lt;p&gt;I gave a short talk at an NSF workshop to spark research collaborations between researchers in Taiwan and the United States. My talk was about work &lt;a href=&#34;https://hannahxchen.github.io/&#34;&gt;Hannah Cyberey&lt;/a&gt; is leading on steering the internal representations of LLMs:&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;https://www.dropbox.com/scl/fi/w0h62l88sti26mt43l60t/steering.pdf?rlkey=h8935na1xjt84wxdumifeclyz&amp;dl=0&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/steering.png&#34; width=&#34;70%&#34;&gt;&lt;/a&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.dropbox.com/scl/fi/w0h62l88sti26mt43l60t/steering.pdf?rlkey=h8935na1xjt84wxdumifeclyz&amp;amp;dl=0&#34;&gt;&lt;strong&gt;Steering around Censorship&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&#xA;Taiwan-US Cybersecurity Workshop&lt;br&gt;&#xA;Arlington, Virginia&lt;br&gt;&#xA;3 March 2025&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Mismeasure of Man and Models</title>
      <link>//uvasrg.github.io/the-mismeasure-of-man-and-models/</link>
      <pubDate>Sun, 11 Aug 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/the-mismeasure-of-man-and-models/</guid>
      <description>&lt;h1 id=&#34;evaluating-allocational-harms-in-large-language-models&#34;&gt;Evaluating Allocational Harms in Large Language Models&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;Blog post written by &lt;a href=&#34;https://hannahxchen.github.io/&#34;&gt;Hannah Chen&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Our work considers &lt;i&gt;allocational harms&lt;/i&gt; that arise when model predictions are used to distribute scarce resources or opportunities.&lt;/p&gt;&#xA;&lt;h2 id=&#34;current-bias-metrics-do-not-reliably-reflect-allocation-disparities&#34;&gt;Current Bias Metrics Do Not Reliably Reflect Allocation Disparities&lt;/h2&gt;&#xA;&lt;p&gt;Several methods have been proposed to audit large language models (LLMs) for bias when used in critical decision-making, such as resume screening for hiring. Yet, these methods focus on &lt;i&gt;predictions&lt;/i&gt;, without considering how the predictions are used to make &lt;i&gt;decisions&lt;/i&gt;. In many settings, making decisions involve prioritizing options due to limited resource constraints. We find that prediction-based evaluation methods, which measure bias as the &lt;i&gt;average performance gap&lt;/i&gt; (Î´) in prediction outcomes, do not reliably reflect disparities in allocation decision outcomes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models</title>
      <link>//uvasrg.github.io/balancing-tradeoffs-between-fickleness-and-obstinacy-in-nlp-models/</link>
      <pubDate>Sun, 13 Nov 2022 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/balancing-tradeoffs-between-fickleness-and-obstinacy-in-nlp-models/</guid>
      <description>&lt;p&gt;Post by &lt;a href=&#34;https://hannahxchen.github.io/&#34;&gt;Hannah Chen&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Our work on balanced adversarial training looks at how to train models&#xA;that are robust to two different types of adversarial examples:&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://hannahxchen.github.io/&#34;&gt;Hannah Chen&lt;/a&gt;, &lt;a href=&#34;http://yangfengji.net/&#34;&gt;Yangfeng&#xA;Ji&lt;/a&gt;, &lt;a href=&#34;http://www.cs.virginia.edu/~evans/&#34;&gt;David&#xA;Evans&lt;/a&gt;. &lt;em&gt;Balanced Adversarial&#xA;Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP&#xA;Models&lt;/em&gt;. In &lt;a href=&#34;https://2022.emnlp.org/&#34;&gt;&lt;em&gt;The 2022 Conference on Empirical Methods in Natural&#xA;Language Processing&lt;/em&gt;&lt;/a&gt; (EMNLP), Abu Dhabi,&#xA;7-11 December 2022.  [&lt;a href=&#34;https://arxiv.org/abs/2210.11498&#34;&gt;ArXiv&lt;/a&gt;]&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube-nocookie.com/embed/xQH51lIVDyY&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&#xA;&lt;/center&gt;&#xA;&lt;h2 id=&#34;adversarial-examples&#34;&gt;Adversarial Examples&lt;/h2&gt;&#xA;&lt;p&gt;At the broadest level, an adversarial example is an input crafted intentionally to confuse a model. However, most work focus on the defintion as an input constructed by applying a small perturbation that preserves the ground truth label but changes model&amp;rsquo;s output &lt;a href=&#34;https://arxiv.org/abs/1412.6572&#34;&gt;(Goodfellow et al., 2015)&lt;/a&gt;. We refer it as a &lt;strong&gt;fickle adversarial example&lt;/strong&gt;. On the other hand, attackers can target an opposite objective where the inputs are made with minimal changes that change the ground truth labels but retain model&amp;rsquo;s predictions &lt;a href=&#34;https://arxiv.org/abs/1811.00401&#34;&gt;(Jacobsen et al., 2018)&lt;/a&gt;. We refer these malicious inputs as &lt;strong&gt;obstinate adversarial examples&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pointwise Paraphrase Appraisal is Potentially Problematic</title>
      <link>//uvasrg.github.io/pointwise-paraphrase-appraisal-is-potentially-problematic/</link>
      <pubDate>Tue, 07 Jul 2020 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/pointwise-paraphrase-appraisal-is-potentially-problematic/</guid>
      <description>&lt;p&gt;Hannah Chen presented her paper on &lt;em&gt;Pointwise Paraphrase Appraisal is&#xA;Potentially Problematic&lt;/em&gt; at the &lt;a href=&#34;https://sites.google.com/view/acl20studentresearchworkshop/&#34;&gt;ACL 2020 Student Research&#xA;Workshop&lt;/a&gt;:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;The prevailing approach for training and evaluating paraphrase&#xA;identification models is constructed as a binary classification&#xA;problem: the model is given a pair of sentences, and is judged by how&#xA;accurately it classifies pairs as either paraphrases or&#xA;non-paraphrases. This pointwise-based evaluation method does not match&#xA;well the objective of most real world applications, so the goal of our&#xA;work is to understand how models which perform well under pointwise&#xA;evaluation may fail in practice and find better methods for evaluating&#xA;paraphrase identification models. As a first step towards that goal,&#xA;we show that although the standard way of fine-tuning BERT for&#xA;paraphrase identification by pairing two sentences as one sequence&#xA;results in a model with state-of-the-art performance, that model may&#xA;perform poorly on simple tasks like identifying pairs with two&#xA;identical sentences. Moreover, we show that these models may even&#xA;predict a pair of randomly-selected sentences with higher paraphrase&#xA;score than a pair of identical ones.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
