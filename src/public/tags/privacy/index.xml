<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Privacy on Security Research Group</title>
    <link>//uvasrg.github.io/tags/privacy/</link>
    <description>Recent content in Privacy on Security Research Group</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Tue, 26 Nov 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/tags/privacy/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Reassessing EMNLP 2024’s Best Paper: Does Divergence-Based Calibration for Membership Inference Attacks Hold Up?</title>
      <link>//uvasrg.github.io/reassessing-emnlp-2024s-best-paper-does-divergence-based-calibration-for-membership-inference-attacks-hold-up/</link>
      <pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/reassessing-emnlp-2024s-best-paper-does-divergence-based-calibration-for-membership-inference-attacks-hold-up/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.anshumansuri.com/&#34;&gt;Anshuman Suri&lt;/a&gt; and &lt;a href=&#34;https://pratyushmaini.github.io/&#34;&gt;Pratyush Maini&lt;/a&gt; wrote a blog about the EMNLP 2024 best paper award winner: &lt;a href=&#34;https://www.anshumansuri.com/blog/2024/calibrated-mia/&#34;&gt;&lt;em&gt;Reassessing EMNLP 2024’s Best Paper: Does Divergence-Based Calibration for Membership Inference Attacks Hold Up?&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;As we explored in &lt;a href=&#34;https://uvasrg.github.io/do-membership-inference-attacks-work-on-large-language-models/&#34;&gt;&lt;em&gt;Do Membership Inference Attacks Work on Large Language Models?&lt;/em&gt;&lt;/a&gt;, to test a membership inference attack it is essentail to have a candidate set where the members and non-members are from the same distribution. If the distributions are different, the ability of an attack to distinguish members and non-members is indicative of distribution inference, not necessarily membership inference.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Common Way To Test for Leaks in Large Language Models May Be Flawed</title>
      <link>//uvasrg.github.io/common-way-to-test-for-leaks-in-large-language-models-may-be-flawed/</link>
      <pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/common-way-to-test-for-leaks-in-large-language-models-may-be-flawed/</guid>
      <description>&lt;p&gt;UVA News has an article on our LLM membership inference work:&#xA;&lt;a href=&#34;https://engineering.virginia.edu/news-events/news/common-way-test-leaks-large-language-models-may-be-flawed&#34;&gt;&lt;em&gt;Common Way To Test for Leaks in Large Language Models May Be Flawed: UVA Researchers Collaborated To Study the Effectiveness of Membership Inference Attacks&lt;/em&gt;&lt;/a&gt;, Eric Williamson, 13 November 2024.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Google&#39;s Trail of Crumbs</title>
      <link>//uvasrg.github.io/googles-trail-of-crumbs/</link>
      <pubDate>Mon, 05 Aug 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/googles-trail-of-crumbs/</guid>
      <description>&lt;p&gt;Matt Stoller published &lt;a href=&#34;https://www.thebignewsletter.com/p/googles-trail-of-crumbs&#34;&gt;my essay on Google&amp;rsquo;s decision to abandon its Privacy Sandbox Initiative&lt;/a&gt; in his Big newsletter:&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;https://www.thebignewsletter.com/p/googles-trail-of-crumbs&#34;&gt;&#xA;&lt;div class=&#34;substack-post-embed&#34;&gt;&lt;p lang=&#34;en&#34;&gt;Google&#39;s Trail of Crumbs by Matt Stoller&lt;/p&gt;&lt;p&gt;Google is too big to get rid of cookies. Even when it wants to protect users, it can&#39;t.&lt;/p&gt;&lt;a data-post-link href=&#34;https://www.thebignewsletter.com/p/googles-trail-of-crumbs&#34;&gt;Read on Substack&lt;/a&gt;&lt;/div&gt;&lt;script async src=&#34;https://substack.com/embedjs/embed.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/a&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;For more technical background on this, see Minjun&amp;rsquo;s paper: &lt;a href=&#34;https://arxiv.org/abs/2405.08102&#34;&gt;&lt;em&gt;Evaluating Google&amp;rsquo;s Protected Audience Protocol&lt;/em&gt;&lt;/a&gt; in PETS 2024.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Congratulations, Dr. Suri!</title>
      <link>//uvasrg.github.io/congratulations-dr.-suri/</link>
      <pubDate>Thu, 18 Jul 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/congratulations-dr.-suri/</guid>
      <description>&lt;p&gt;Congratulations to &lt;a href=&#34;https://www.anshumansuri.com/&#34;&gt;Anshuman Suri&lt;/a&gt; for successfully defending his &lt;a href=&#34;https://libraetd.lib.virginia.edu/public_view/2227mr11j&#34;&gt;PhD thesis&lt;/a&gt;!&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;img src=&#34;//uvasrg.github.io/images/suri-phd.png&#34; width=&#34;70%&#34;&gt;&lt;br&gt;&#xA;Tianhao Wang, Dr. Anshuman Suri, Nando Fioretto, Cong Shen&lt;br&gt;&#xA;On Screen: David Evans, Giuseppe Ateniese&#xA;&lt;br&gt;&#xA;&lt;/center&gt;&#xA;&lt;h2 id=&#34;heading&#34;&gt;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;heading-1&#34;&gt;&lt;/h2&gt;&#xA;&lt;center&gt;&#xA;&lt;em&gt;&#xA;Inference Privacy in Machine Learning&#xA;&lt;/em&gt;&#xA;&lt;/center&gt;&#xA;&lt;h2 id=&#34;heading-2&#34;&gt;&lt;/h2&gt;&#xA;&lt;p&gt;Using machine learning models comes at the risk of leaking information about data used in their training and deployment. This leakage can expose sensitive information about properties of the underlying data distribution, data from participating users, or even individual records in the training data. In this dissertation, we develop and evaluate novel methods to quantify and audit such information disclosure at three granularities: distribution, user, and record.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Graduation 2024</title>
      <link>//uvasrg.github.io/graduation-2024/</link>
      <pubDate>Wed, 29 May 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/graduation-2024/</guid>
      <description>&lt;center&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/graduation2024/PXL_20240517_175539174-fish.jpg&#34;&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/graduation2024/PXL_20240517_175539174-fish.jpg&#34; width=&#34;85%&#34;&gt;&lt;/img&gt;&lt;/a&gt;&#xA;&lt;/center&gt;&#xA;&lt;center&gt;&#xA;&lt;h1 id=&#34;congratulations-to-our-two-phd-graduates&#34;&gt;Congratulations to our two PhD graduates!&lt;/h1&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;Suya will be joining the University of Tennessee at Knoxville as an Assistant Professor.&lt;/p&gt;&#xA;&lt;p&gt;Josie will be building a medical analytics research group at Dexcom.&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/graduation2024/IMG_5973.png&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/graduation2024/IMG_5973.png&#34; width=&#34;85%&#34;&gt;&lt;/img&gt;&lt;/a&gt;&#xA;&lt;/center&gt;&#xA;&lt;h2 id=&#34;heading&#34;&gt;&lt;/h2&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/graduation2024/IMG_5941.png&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/graduation2024/IMG_5941.png&#34; width=&#34;60%&#34;&gt;&lt;/img&gt;&lt;/a&gt;&#xA;&amp;nbsp;&lt;a href=&#34;//uvasrg.github.io/images/graduation2024/IMG_5951.png&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/graduation2024/IMG_5951.png&#34; width=&#34;60%&#34;&gt;&lt;/img&gt;&lt;/a&gt;&amp;nbsp;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/graduation2024/IMG_5962.png&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/graduation2024/IMG_5962.png&#34; width=&#34;60%&#34;&gt;&lt;/img&gt;&lt;/a&gt;&#xA;&lt;/center&gt;</description>
    </item>
    <item>
      <title>Congratulations, Dr. Lamp!</title>
      <link>//uvasrg.github.io/congratulations-dr.-lamp/</link>
      <pubDate>Wed, 20 Mar 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/congratulations-dr.-lamp/</guid>
      <description>&lt;center&gt;&#xA;&lt;img src=&#34;//uvasrg.github.io/images/lamp-defense.png&#34; width=&#34;70%&#34;&gt;&lt;br&gt;&#xA;Tianhao Wang (Committee Chair), Miaomiao Zhang, Lu Feng (Co-Advisor), Dr. Josie Lamp, David Evans&lt;br&gt;&#xA;On screen: Sula Mazimba, Rich Nguyen, Tingting Zhu &lt;br&gt;&#xA;&lt;/center&gt;&#xA;&lt;h2 id=&#34;heading&#34;&gt;&lt;/h2&gt;&#xA;&lt;p&gt;Congratulations to &lt;a href=&#34;https://www.josephinelamp.com/&#34;&gt;Josephine Lamp&lt;/a&gt; for successfully defending her &lt;a href=&#34;https://libraetd.lib.virginia.edu/public_view/pv63g149h&#34;&gt;PhD thesis&lt;/a&gt;!&lt;/p&gt;&#xA;&lt;h2 id=&#34;heading-1&#34;&gt;&lt;/h2&gt;&#xA;&lt;center&gt;&#xA;&lt;em&gt;&#xA;Trustworthy Clinical Decision Support Systems for Medical Trajectories&#xA;&lt;/em&gt;&#xA;&lt;/center&gt;&#xA;&lt;h2 id=&#34;heading-2&#34;&gt;&lt;/h2&gt;&#xA;&lt;p&gt;The explosion of medical sensors and wearable devices has resulted in the collection of large amounts of medical trajectories. Medical trajectories are time series that provide a nuanced look into patient conditions and their changes over time, allowing for a more fine-grained understanding of patient health. It is difficult for clinicians and patients to effectively make use of such high dimensional data, especially given the fact that there may be years or even decades worth of data per patient. Clinical Decision Support Systems (CDSS) provide summarized, filtered, and timely information to patients or clinicians to help inform medical decision-making processes. Although CDSS have shown promise for data sources such as tabular and imaging data, e.g., in electronic health records, the opportunities of CDSS using medical trajectories have not yet been realized due to challenges surrounding data use, model trust and interpretability, and privacy and legal concerns.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Microsoft Research Summit: Surprising (and unsurprising) Inference Risks in Machine Learning</title>
      <link>//uvasrg.github.io/microsoft-research-summit-surprising-and-unsurprising-inference-risks-in-machine-learning/</link>
      <pubDate>Thu, 21 Oct 2021 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/microsoft-research-summit-surprising-and-unsurprising-inference-risks-in-machine-learning/</guid>
      <description>&lt;p&gt;Here are the slides for my talk at the &lt;a href=&#34;https://www.microsoft.com/en-us/research/theme/confidential-computing/#workshops&#34;&gt;&lt;em&gt;Practical and Theoretical Privacy of Machine Learning Training Pipelines&lt;/em&gt;&lt;/a&gt;&#xA;Workshop at the Microsoft Research Summit (21 October 2021):&lt;/p&gt;&#xA;   &lt;center&gt;&#xA;&lt;a href=&#34;https://www.dropbox.com/s/1mfhbelv7qx4t3u/surprisinginferences.pdf?dl=0&#34;&gt;&lt;b&gt;Surprising (and Unsurprising) Inference Risks in Machine Learning&lt;/b&gt; [PDF]&lt;/a&gt;&#xA;   &lt;/center&gt;&#xA;&lt;h2 id=&#34;heading&#34;&gt;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;heading-1&#34;&gt;&lt;/h2&gt;&#xA;&lt;p&gt;The work by Bargav Jayaraman (with Katherine Knipmeyer, Lingxiao Wang,&#xA;and Quanquan Gu) that I talked about on improving membership inference&#xA;attacks is described in more details here:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Bargav Jayaraman, Lingxiao Wang, Katherine Knipmeyer, Quanquan Gu, David Evans. &lt;a href=&#34;https://arxiv.org/abs/2005.10881&#34;&gt;&lt;em&gt;Revisiting Membership Inference Under Realistic Assumptions&lt;/em&gt;&lt;/a&gt; (PETS 2021).&lt;br&gt;&#xA;[&lt;a href=&#34;//uvasrg.github.io/merlin-morgan-and-the-importance-of-thresholds-and-priors/&#34;&gt;Blog&lt;/a&gt;] [Code: &lt;a href=&#34;https://github.com/bargavj/EvaluatingDPML&#34;&gt;&lt;em&gt;https://github.com/bargavj/EvaluatingDPML&lt;/em&gt;&lt;/a&gt;]&lt;/p&gt;</description>
    </item>
    <item>
      <title>UVA News Article</title>
      <link>//uvasrg.github.io/uva-news-article/</link>
      <pubDate>Tue, 28 Sep 2021 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/uva-news-article/</guid>
      <description>&lt;p&gt;UVA News has an article by Audra Book on our research on security and&#xA;privacy of machine learning (with some very nice quotes from several&#xA;students in the group, and me saying something positive about the&#xA;NSA!): &lt;a href=&#34;https://engineering.virginia.edu/news/2021/09/computer-science-professor-david-evans-and-his-team-conduct-experiments-understand&#34;&gt;&lt;em&gt;Computer science professor David Evans and his team conduct&#xA;experiments to understand security and privacy risks associated with&#xA;machine&#xA;learning&lt;/em&gt;&lt;/a&gt;,&#xA;8 September 2021.&lt;/p&gt;&#xA;&lt;div class=&#34;articletext&#34;&gt;&#xA;&lt;p&gt;David Evans, professor of computer science in the University of Virginia School of Engineering and Applied Science, is leading research to understand how machine learning models can be compromised.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ICLR DPML 2021: Inference Risks for Machine Learning</title>
      <link>//uvasrg.github.io/iclr-dpml-2021-inference-risks-for-machine-learning/</link>
      <pubDate>Fri, 07 May 2021 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/iclr-dpml-2021-inference-risks-for-machine-learning/</guid>
      <description>&lt;p&gt;I gave an invited talk at the &lt;a href=&#34;https://dp-ml.github.io/2021-workshop-ICLR/&#34;&gt;Distributed and Private Machine Learning&lt;/a&gt; (DPML) workshop at ICLR 2021 on &lt;a href=&#34;https://iclr.cc/virtual/2021/workshop/2148#collapse3549&#34;&gt;&lt;em&gt;Inference Risks for Machine Learning&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube-nocookie.com/embed/zgSTsO1LKSs?controls=0&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/center&gt;&#xA;&lt;p&gt;The talk mostly covers work by Bargav Jayaraman on evaluating privacy in&#xA;machine learning and connecting attribute inference and imputation, and recent work by Anshuman Suri on property inference.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Codaspy 2021 Keynote: When Models Learn Too Much</title>
      <link>//uvasrg.github.io/codaspy-2021-keynote-when-models-learn-too-much/</link>
      <pubDate>Mon, 26 Apr 2021 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/codaspy-2021-keynote-when-models-learn-too-much/</guid>
      <description>&lt;p&gt;Here are the slides for my talk at the&#xA;&lt;a href=&#34;http://www.codaspy.org/2021/program.html&#34;&gt;11th ACM Conference on Data and Application Security and Privacy&lt;/a&gt;:&lt;/p&gt;&#xA;   &lt;center&gt;&#xA;&lt;a href=&#34;https://www.dropbox.com/s/6wzloxuai709s0k/codaspy-post.pdf?dl=0)&#34;&gt;&lt;b&gt;When Models Learn Too Much&lt;/b&gt; [PDF]&lt;/a&gt;&#xA;   &lt;/center&gt;&#xA;&lt;h2 id=&#34;heading&#34;&gt;&lt;/h2&gt;&#xA;&lt;p&gt;The talk includes Bargav Jayaraman&amp;rsquo;s work (with Katherine Knipmeyer, Lingxiao Wang, and Quanquan Gu) on evaluating privacy in machine learning, as well as more recent work by Anshuman Suri on property inference attacks, and Bargav on attribute inference and imputation:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;//uvasrg.github.io/merlin-morgan-and-the-importance-of-thresholds-and-priors/&#34;&gt;&lt;em&gt;Merlin, Morgan, and the Importance of Thresholds and Priors&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;//uvasrg.github.io/evaluating-differentially-private-machine-learning-in-practice/&#34;&gt;&lt;em&gt;Evaluating Differentially Private Machine Learning in Practice&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;center&gt;&#xA;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;“When models learn too much. “ Dr. David Evans &lt;a href=&#34;https://twitter.com/UdacityDave?ref_src=twsrc%5Etfw&#34;&gt;@UdacityDave&lt;/a&gt; of University of Virginia gave a keynote talk on different inference risks for machine learning models this morning at &lt;a href=&#34;https://twitter.com/hashtag/codaspy21?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#codaspy21&lt;/a&gt; &lt;a href=&#34;https://t.co/KVgFoUA6sa&#34;&gt;pic.twitter.com/KVgFoUA6sa&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Virginia Consumer Data Protection Act</title>
      <link>//uvasrg.github.io/virginia-consumer-data-protection-act/</link>
      <pubDate>Fri, 19 Feb 2021 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/virginia-consumer-data-protection-act/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.josephinelamp.com/&#34;&gt;Josephine Lamp&lt;/a&gt; presented on the new data privacy law that is pending in Virginia (it still needs a few steps including expected signing by governor, but likely to go into effect Jan 1, 2023): &lt;a href=&#34;https://www.dropbox.com/s/1epulyhc30wd239/cdpa.pdf?dl=0&#34;&gt;Slides (PDF)&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;This article provides a summary of the law: &lt;a href=&#34;https://www.natlawreview.com/article/virginia-passes-consumer-privacy-law-other-states-may-follow&#34;&gt;&lt;em&gt;Virginia Passes Consumer Privacy Law; Other States May Follow&lt;/em&gt;&lt;/a&gt;, National Law Review, 17 February 2021.&lt;/p&gt;&#xA;&lt;p&gt;The law itself is here: &lt;a href=&#34;https://lis.virginia.gov/cgi-bin/legp604.exe?211+ful+SB1392&#34;&gt;SB 1392: Consumer Data Protection Act&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Microsoft Security Data Science Colloquium: Inference Privacy in Theory and Practice</title>
      <link>//uvasrg.github.io/microsoft-security-data-science-colloquium-inference-privacy-in-theory-and-practice/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/microsoft-security-data-science-colloquium-inference-privacy-in-theory-and-practice/</guid>
      <description>&lt;p&gt;Here are the slides for my talk at the Microsoft Security Data Science Colloquium:&lt;br&gt;&#xA;&lt;a href=&#34;https://www.dropbox.com/s/698cuvee81clx1q/inference-privacy-share.pdf?dl=0&#34;&gt;&lt;em&gt;When Models Learn Too Much: Inference Privacy in Theory and Practice&lt;/em&gt; [PDF]&lt;/a&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;https://www.dropbox.com/s/698cuvee81clx1q/inference-privacy-share.pdf?dl=0&#34;&gt;&#xA;&lt;img src=&#34;//uvasrg.github.io/images/inferenceprivacytitle.png&#34; width=65%&gt;&#xA;&lt;/a&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;The talk is mostly about Bargav Jayaraman&amp;rsquo;s work (with Katherine Knipmeyer, Lingxiao Wang, and Quanquan Gu) on evaluating privacy:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;//uvasrg.github.io/merlin-morgan-and-the-importance-of-thresholds-and-priors/&#34;&gt;&lt;em&gt;Merlin, Morgan, and the Importance of Thresholds and Priors&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;//uvasrg.github.io/evaluating-differentially-private-machine-learning-in-practice/&#34;&gt;&lt;em&gt;Evaluating Differentially Private Machine Learning in Practice&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Merlin, Morgan, and the Importance of Thresholds and Priors</title>
      <link>//uvasrg.github.io/merlin-morgan-and-the-importance-of-thresholds-and-priors/</link>
      <pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/merlin-morgan-and-the-importance-of-thresholds-and-priors/</guid>
      <description>&lt;p&gt;&lt;em&gt;Post by Katherine Knipmeyer&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Machine learning poses a substantial risk that adversaries will be&#xA;able to discover information that the model does not intend to&#xA;reveal. One set of methods by which consumers can learn this sensitive&#xA;information, known broadly as &lt;em&gt;membership inference&lt;/em&gt; attacks,&#xA;predicts whether or not a query record belongs to the training set. A&#xA;basic membership inference attack involves an attacker with a given&#xA;record and black-box access to a model who tries to determine whether&#xA;said record was a member of the model’s training set.&lt;/p&gt;</description>
    </item>
    <item>
      <title>White House Visit</title>
      <link>//uvasrg.github.io/white-house-visit/</link>
      <pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/white-house-visit/</guid>
      <description>&lt;p&gt;I had a chance to visit the White House for a Roundtable on&#xA;&lt;em&gt;Accelerating Responsible Sharing of Federal Data&lt;/em&gt;. The meeting was&#xA;held under &amp;ldquo;Chatham House Rules&amp;rdquo;, so I won&amp;rsquo;t mention the other&#xA;participants here.&lt;/p&gt;&#xA;&lt;div style=&#34;padding-bottom: 2em&#34;&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/whitehouse/IMG_20191030_080936.png&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/whitehouse/IMG_20191030_080936.png&#34; width=75%&gt;&#xA;&lt;/center&gt;&#xA;&lt;/div&gt;&#xA;&lt;p&gt;The meeting was held in the &lt;a href=&#34;https://en.wikipedia.org/wiki/Roosevelt_Room&#34;&gt;Roosevelt&#xA;Room&lt;/a&gt; of the White&#xA;House. We entered through the visitor&amp;rsquo;s side entrance. After a&#xA;security gate (where you put your phone in a lockbox, so no pictures&#xA;inside) with a TV blaring Fox News, there is a pleasant lobby for&#xA;waiting, and then an entrance right into the Roosevelt Room. (We&#xA;didn&amp;rsquo;t get to see the entrance in the opposite corner of the room,&#xA;which is just a hallway across from the Oval Office.)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Research Symposium Posters</title>
      <link>//uvasrg.github.io/research-symposium-posters/</link>
      <pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/research-symposium-posters/</guid>
      <description>&lt;p&gt;Five students from our group presented posters at the department&amp;rsquo;s&#xA;&lt;a href=&#34;https://engineering.virginia.edu/cs-research-symposium-fall-2019&#34;&gt;Fall Research&#xA;Symposium&lt;/a&gt;:&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;iframe src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vQcaahIxPyCHIMJti6tRB9HM_RreRhZkGH4wCN7YjTwiHSqcHod9v3hDFj-ZS1TtXp9OtBEBCV8OPH4/embed?start=false&amp;loop=false&amp;delayms=3000&#34; frameborder=&#34;0&#34; width=&#34;764&#34; height=&#34;453&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;&lt;br&gt;&#xA;Anshuman Suri&#39;s Overview Talk&#xA;&lt;/center&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/evaluatingdpml-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt; &lt;br&gt;&#xA;Bargav Jayaraman, &lt;em&gt;Evaluating Differentially Private Machine Learning In Practice&lt;/em&gt; &#xA;[&lt;a href=&#34;&#xA;&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/evaluatingdpml-poster.pdf&#34;&gt;Poster&lt;/a&gt;]&lt;br&gt;&#xA;[&lt;a href=&#34;https://www.cs.virginia.edu/~evans/pubs/usenix2019/&#34;&gt;Paper&lt;/a&gt; (USENIX Security 2019)]&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/pretrainedvulnerable-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt;&lt;br&gt;&#xA;Hannah Chen [&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/pretrainedvulnerable-poster.pdf&#34;&gt;Poster&lt;/a&gt;]&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/measuringconcentration-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt;&lt;br&gt;&#xA;Xiao Zhang [&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/measuringconcentration-poster.pdf&#34;&gt;Poster&lt;a&gt;]&lt;br&gt;&#xA;[&lt;a href=&#34;https://arxiv.org/abs/1905.12202&#34;&gt;Paper&lt;/a&gt; (NeurIPS 2019)]&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/diversemodels-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt;&lt;br&gt;&#xA;Mainudding Jonas [&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/diversemodels-poster.pdf&#34;&gt;Poster&lt;/a&gt;]&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/hybridbatch-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt;&lt;br&gt;&#xA;Fnu Suya [&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/hybridbatch-poster.pdf&#34;&gt;Poster&lt;a&gt;]&lt;br&gt;&#xA;[&lt;a href=&#34;https://arxiv.org/abs/1908.07000&#34;&gt;Paper&lt;/a&gt; (USENIX Security 2020)]&#xA;&lt;/center&gt;</description>
    </item>
    <item>
      <title>FOSAD Trustworthy Machine Learning Mini-Course</title>
      <link>//uvasrg.github.io/fosad2019/</link>
      <pubDate>Wed, 28 Aug 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/fosad2019/</guid>
      <description>&lt;p&gt;I taught a mini-course on &lt;em&gt;Trustworthy Machine Learning&lt;/em&gt; at the &lt;a href=&#34;http://www.sti.uniurb.it/events/fosad19/&#34;&gt;&lt;em&gt;19th&#xA;International School on Foundations of Security Analysis and&#xA;Design&lt;/em&gt;&lt;/a&gt; in Bertinoro, Italy.&lt;/p&gt;&#xA;&lt;center&gt;&lt;a href=&#34;//uvasrg.github.io/images/bertinoro-big.jpg&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/bertinoro.jpg&#34; width=&#34;90%&#34;&gt;&lt;/a&gt;&lt;/center&gt;&#xA;&lt;p&gt;Slides from my three (two-hour) lectures are posted below, along with&#xA;some links to relevant papers and resources.&lt;/p&gt;&#xA;&lt;h2 id=&#34;class-1-introductionattacks&#34;&gt;Class 1: Introduction/Attacks&lt;/h2&gt;&#xA;&lt;center&gt;&#xA;&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;0ad1775bcc244876ac4df1880a864e78&#34; data-ratio=&#34;1.77777777777778&#34; src=&#34;//speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;The PDF malware evasion attack is described in this paper:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;Weilin Xu, Yanjun Qi, and David Evans. &#xA;&lt;em&gt;&lt;a href=&#34;https://www.cs.virginia.edu/evans/pubs/ndss2016/&#34;&gt;Automatically Evading Classifiers: A Case Study on PDF Malware Classifiers&lt;/a&gt;&lt;/em&gt;.&#xA;&lt;a href=&#34;https://www.internetsociety.org/events/ndss-symposium-2016&#34;&gt;&lt;em&gt;Network and Distributed System Security Symposium&lt;/em&gt;&lt;/a&gt; (NDSS). San Diego, CA. 21-24 February 2016. [&lt;a href=&#34;https://www.cs.virginia.edu/evans/pubs/ndss2016/evademl.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://evademl.org/gpevasion/&#34;&gt;EvadeML.org&lt;/a&gt;]&#xA;&lt;/blockquote&gt;&#xA;&lt;h2 id=&#34;class-2-defenses&#34;&gt;Class 2: Defenses&lt;/h2&gt;&#xA;&lt;center&gt;&#xA;&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;cf560cce9e4b418397d2df3429ddc8f9&#34; data-ratio=&#34;1.77777777777778&#34; src=&#34;//speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;This paper describes the feature squeezing framework:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Evaluating Differentially Private Machine Learning in Practice</title>
      <link>//uvasrg.github.io/evaluating-differentially-private-machine-learning-in-practice/</link>
      <pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/evaluating-differentially-private-machine-learning-in-practice/</guid>
      <description>&lt;p&gt;(Cross-post by &lt;a href=&#34;https://bargavjayaraman.github.io/post/evaluating-dpml-results/&#34;&gt;Bargav Jayaraman&lt;/a&gt;)&lt;/p&gt;&#xA;&lt;p&gt;With the recent advances in composition of differential private&#xA;mechanisms, the research community has been able to achieve meaningful&#xA;deep learning with privacy budgets in single digits. Rènyi&#xA;differential privacy (RDP) is one mechanism that provides tighter&#xA;composition which is widely used because of its implementation in&#xA;TensorFlow Privacy (recently, Gaussian differential privacy (GDP) has&#xA;shown a tighter analysis for low privacy budgets, but it was not yet&#xA;available when we did this work). But the central question that&#xA;remains to be answered is: &lt;em&gt;how private are these methods in&#xA;practice?&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>USENIX Security Symposium 2019</title>
      <link>//uvasrg.github.io/usenix-security-symposium-2019/</link>
      <pubDate>Mon, 26 Aug 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/usenix-security-symposium-2019/</guid>
      <description>&lt;p&gt;Bargav Jayaraman presented our paper on &lt;a href=&#34;https://arxiv.org/abs/1902.08874&#34;&gt;&lt;em&gt;Evaluating Differentially Private Machine Learning in Practice&lt;/em&gt;&lt;/a&gt; at the &lt;a&#xA;href=&#34;https://www.usenix.org/conference/usenixsecurity19&#34;&gt;28&lt;sup&gt;th&lt;/sup&gt;&#xA;USENIX Security Symposium&lt;/em&gt;&lt;/a&gt; in Santa Clara, California.&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube-nocookie.com/embed/JAGhqbY_U50&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&#xA;&lt;/center&gt;&#xA;&lt;center&gt;&#xA;&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;dfdd40e4ba2b46e1baee68219df82de7&#34; data-ratio=&#34;1.29456384323641&#34; src=&#34;//speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;&#xA;&lt;/center&gt;&#xA;&lt;center&gt;&lt;img src=&#34;//uvasrg.github.io/images/usenix2019/bargav.jpg&#34; width=80%&#34;&gt;&lt;/center&gt;&#xA;&lt;p&gt;Summary by Lea Kissner:&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Hey it&amp;#39;s the results! &lt;a href=&#34;https://t.co/ru1FbkESho&#34;&gt;pic.twitter.com/ru1FbkESho&lt;/a&gt;&lt;/p&gt;&amp;mdash; Lea Kissner (@LeaKissner) &lt;a href=&#34;https://twitter.com/LeaKissner/status/1162518239177371648?ref_src=twsrc%5Etfw&#34;&gt;August 17, 2019&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;Also, great to see several UVA folks at the conference including:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://havron.dev&#34;&gt;Sam Havron&lt;/a&gt; (BSCS 2017, now a PhD student at&#xA;Cornell) presented a paper on the work he and his colleagues have&#xA;done on &lt;a href=&#34;https://havron.dev/pubs/clinicalsec.pdf&#34;&gt;computer security for victims of intimate partner violence&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;center&gt;&lt;img src=&#34;//uvasrg.github.io/images/usenix2019/havron.jpg&#34; width=80%&#34;&gt;&lt;/center&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.guanotronic.com/~serge/&#34;&gt;Serge Egelman&lt;/a&gt; (BSCS 2004) was an author on the paper &lt;a href=&#34;https://www.usenix.org/conference/usenixsecurity19/presentation/reardon&#34;&gt;&lt;em&gt;50 Ways to Leak Your Data: An&#xA;Exploration of Apps&amp;rsquo; Circumvention of the Android Permissions&#xA;System&lt;/em&gt;&lt;/a&gt;&#xA;(which was recognized by a Distinguished Paper Award). His paper in&#xA;SOUPS on &lt;a href=&#34;https://www.usenix.org/system/files/soups2019-frik.pdf&#34;&gt;&lt;em&gt;Privacy and Security Threat Models and Mitigation&#xA;Strategies of Older&#xA;Adults&lt;/em&gt;&lt;/a&gt; was highlighted in Alex Stamos&amp;rsquo; excellent talk.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Brink Essay: AI Systems Are Complex and Fragile. Here Are Four Key Risks to Understand.</title>
      <link>//uvasrg.github.io/brink-essay-ai-systems-are-complex-and-fragile.-here-are-four-key-risks-to-understand./</link>
      <pubDate>Tue, 09 Jul 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/brink-essay-ai-systems-are-complex-and-fragile.-here-are-four-key-risks-to-understand./</guid>
      <description>&lt;p&gt;Brink News (a publication of &lt;em&gt;The Atlantic&lt;/em&gt;) published my essay on the risks of deploying AI systems.&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;https://www.brinknews.com/ai-systems-are-complex-and-fragile-here-are-four-key-risks-to-understand/&#34;&gt;&lt;img style=&#34;box-shadow: 10px 10px 5px grey;&#34; src=&#34;//uvasrg.github.io/images/brink.png&#34; width=90%&#34;&gt;&lt;/a&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;Artificial intelligence technologies have the potential to transform society in positive and powerful ways. Recent studies have shown computing systems that can outperform humans at numerous once-challenging tasks, ranging from performing medical diagnoses and reviewing legal contracts to playing Go and recognizing human emotions. &lt;/span&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;span style=&#34;font-weight: 400;&#34;&gt;Despite these successes, AI systems are fundamentally fragile — and the ways they can fail are poorly understood. When AI systems are deployed to make important decisions that impact human safety and well-being, the potential risks of abuse and misbehavior are high and need to be carefully considered and mitigated.&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Google Federated Privacy 2019: The Dragon in the Room</title>
      <link>//uvasrg.github.io/google-federated-privacy-2019-the-dragon-in-the-room/</link>
      <pubDate>Sat, 22 Jun 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/google-federated-privacy-2019-the-dragon-in-the-room/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m back from a very interesting &lt;a href=&#34;https://sites.google.com/view/federated-learning-2019/&#34;&gt;&lt;em&gt;Workshop on Federated Learning and&#xA;Analytics&lt;/em&gt;&lt;/a&gt;&#xA;that was organized by &lt;a href=&#34;https://ai.google/research/people/PeterKairouz&#34;&gt;Peter&#xA;Kairouz&lt;/a&gt; and &lt;a href=&#34;https://ai.google/research/people/author35837&#34;&gt;Brendan&#xA;McMahan&lt;/a&gt; from Google&amp;rsquo;s&#xA;federated learning team and was held at Google Seattle.&lt;/p&gt;&#xA;&lt;p&gt;For the first part of my talk, I covered Bargav&amp;rsquo;s work on &lt;a href=&#34;http://www.cs.virginia.edu/~evans/pubs/usenix2019/&#34;&gt;evaluating&#xA;differentially private machine&#xA;learning&lt;/a&gt;, but I&#xA;reserved the last few minutes of my talk to address the cognitive&#xA;dissonance I felt being at a Google meeting on privacy.&lt;/p&gt;&#xA;&lt;div class=&#34;row&#34;&gt;&#xA;  &lt;div class=&#34;column small-12 medium-6&#34;&gt;&#xA;  &lt;a href=&#34;//uvasrg.github.io/images/googleprivacy2019/slides-full/Slide01.png&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/googleprivacy2019/slides/Slide01.png&#34;&gt;&lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;column small-12 medium-6&#34;&gt;&#xA;&lt;p&gt;&#xA;I don’t want to offend anyone, and want to preface this by saying I&#xA;have lots of friends and former students who work for Google, people&#xA;that I greatly admire and respect – so I want to raise the cognitive&#xA;dissonance I have being at a “privacy” meeting run by Google, in the&#xA;hopes that people at Google actually do think about privacy and will&#xA;able to convince me how wrong I am.&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Violations of Children’s Privacy Laws</title>
      <link>//uvasrg.github.io/violations-of-childrens-privacy-laws/</link>
      <pubDate>Sun, 16 Sep 2018 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/violations-of-childrens-privacy-laws/</guid>
      <description>&lt;p&gt;The New York Times has an article, &lt;a href=&#34;https://www.nytimes.com/interactive/2018/09/12/technology/kids-apps-data-privacy-google-twitter.html&#34;&gt;&lt;em&gt;How Game Apps That Captivate Kids Have Been Collecting Their Data&lt;/em&gt;&lt;/a&gt; about a lawsuit the state of New Mexico is bringing against app markets (including Google) that allow apps presented as being for children in the Play store to violate COPPA rules and mislead users into tracking children. The lawsuit stems from a study led by Serge Egleman’s group at UC Berkeley that analyzed COPPA violations in children’s apps. Serge was an undergraduate student here (back in the early 2000s) – one of the things he did as a undergraduate was successfully sue a spammer.&lt;/p&gt;</description>
    </item>
    <item>
      <title>USENIX Security 2018</title>
      <link>//uvasrg.github.io/usenix-security-2018/</link>
      <pubDate>Sun, 19 Aug 2018 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/usenix-security-2018/</guid>
      <description>&lt;p&gt;Three SRG posters were presented at &lt;a&#xA;href=&#34;https://www.usenix.org/conference/usenixsecurity18/poster-session&#34;&gt;USENIX&#xA;Security Symposium 2018&lt;/a&gt; in Baltimore, Maryland:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Nathaniel Grevatt (&lt;em&gt;GDPR-Compliant Data Processing: Improving&#xA;Pseudonymization with Multi-Party Computation&lt;/em&gt;)&lt;/li&gt;&#xA;&lt;li&gt;Matthew Wallace and Parvesh Samayamanthula (&lt;em&gt;Deceiving Privacy Policy Classifiers with Adversarial Examples&lt;/em&gt;)&lt;/li&gt;&#xA;&lt;li&gt;Guy Verrier (&lt;em&gt;How is GDPR Affecting Privacy Policies?&lt;/em&gt;, joint with Haonan Chen and Yuan&#xA;Tian)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;center&gt;&lt;/p&gt;&#xA;&lt;table width=&#34;85%&#34;&gt;&#xA;&lt;tr&gt;&#xA;&lt;td align=&#34;center&#34;&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/usenix2018/IMG_20180816_190616-2.jpg&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;//uvasrg.github.io/images/usenix2018/IMG_20180816_190616.jpg&#34; height=220&gt;&#xA;&lt;/td&gt;&#xA;&lt;td href=&#34;//uvasrg.github.io/images/usenix2018/IMG_20180816_190626-2.jpg&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;//uvasrg.github.io/images/usenix2018/IMG_20180816_190626.jpg&#34; height=220&gt;&#xA;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;tr&gt;&#xA;&lt;td align=&#34;center&#34;&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/usenix2018/IMG_20180816_192620-2.jpg&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;//uvasrg.github.io/images/usenix2018/IMG_20180816_192620.jpg&#34; height=220&gt;&#xA;&lt;/td&gt;&#xA;&lt;td href=&#34;//uvasrg.github.io/images/usenix2018/IMG_20180816_192646-2.jpg&#34;&gt;&lt;img align=&#34;center&#34; src=&#34;//uvasrg.github.io/images/usenix2018/IMG_20180816_192646.jpg&#34; height=220&gt;&#xA;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;There were also a surprising number of appearances by an unidentified unicorn:&lt;br /&gt;&#xA;&lt;center&gt;&lt;/p&gt;&#xA;&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Your poster may have made the cut for the &lt;a href=&#34;https://twitter.com/hashtag/usesec18?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#usesec18&lt;/a&gt; Poster Reception, but has it received the approval of a tiny, adorable unicorn? &lt;a href=&#34;https://twitter.com/UVA?ref_src=twsrc%5Etfw&#34;&gt;@UVA&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/seenatusesec18?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#seenatusesec18&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/girlswhocode?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#girlswhocode&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/futurecomputerscientist?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#futurecomputerscientist&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/dreambig?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#dreambig&lt;/a&gt; &lt;a href=&#34;https://t.co/bZOO6lYLXK&#34;&gt;pic.twitter.com/bZOO6lYLXK&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
