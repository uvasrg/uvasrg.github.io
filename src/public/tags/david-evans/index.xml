<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>David Evans on Security Research Group</title>
    <link>//uvasrg.github.io/tags/david-evans/</link>
    <description>Recent content in David Evans on Security Research Group</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Tue, 18 Mar 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/tags/david-evans/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>New Classes Explore Promise and Predicaments of Artificial Intelligence</title>
      <link>//uvasrg.github.io/new-classes-explore-promise-and-predicaments-of-artificial-intelligence/</link>
      <pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/new-classes-explore-promise-and-predicaments-of-artificial-intelligence/</guid>
      <description>&lt;p&gt;&lt;em&gt;The Docket&lt;/em&gt; (UVA Law News) has an article about the AI Law class I&amp;rsquo;m helping Tom Nachbar teach:&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.law.virginia.edu/news/202503/new-classes-explore-promise-and-predicaments-artificial-intelligence&#34;&gt;New Classes Explore Promise and Predicaments of Artificial Intelligence&lt;/a&gt;&lt;br&gt;&#xA;&lt;em&gt;Attorneys-in-Training Learn About Prompts, Policies and Governance&lt;/em&gt;&lt;br&gt;&#xA;The Docket, 17 March 2025&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Nachbar teamed up with David Evans, a professor of computer science at UVA, to teach the course, which, he said, is “a big part of what makes this class work.”&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;“This course takes a much more technical approach than typical law school courses do. We have the students actually going in, creating their own chatbots — they’re looking at the technology underlying generative AI,” Nachbar said. Better understanding how AI actually works, Nachbar said, is key in training lawyers to handle AI-related litigation in the future.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Can we explain AI model outputs?</title>
      <link>//uvasrg.github.io/can-we-explain-ai-model-outputs/</link>
      <pubDate>Fri, 21 Feb 2025 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/can-we-explain-ai-model-outputs/</guid>
      <description>&lt;p&gt;I gave a short talk on explanability at the &lt;em&gt;Virginia Journal of Social Policy and the Law Symposium on Artificial Intelligence&lt;/em&gt; at UVA Law School, 21 February 2025.&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.dropbox.com/scl/fi/pnkhi9a8iqx797tp468ic/ailaw-symposium.pdf?rlkey=ahzw9ka13335b7odvpaap0jkp&amp;amp;dl=0&#34;&gt;&lt;em&gt;Can we explain AI model outputs?&lt;/em&gt; (PDF)&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;There&amp;rsquo;s an article about the event in the Virginia Law Weekly:&#xA;&lt;a href=&#34;https://www.lawweekly.org/front-page/2025/2/26/hdt21vnfc7ygs4xf58rrg7slmmjcm8&#34;&gt;&lt;em&gt;Law School Hosts LawTech Events&lt;/em&gt;&lt;/a&gt;, 26 February 2025.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reassessing EMNLP 2024’s Best Paper: Does Divergence-Based Calibration for Membership Inference Attacks Hold Up?</title>
      <link>//uvasrg.github.io/reassessing-emnlp-2024s-best-paper-does-divergence-based-calibration-for-membership-inference-attacks-hold-up/</link>
      <pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/reassessing-emnlp-2024s-best-paper-does-divergence-based-calibration-for-membership-inference-attacks-hold-up/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.anshumansuri.com/&#34;&gt;Anshuman Suri&lt;/a&gt; and &lt;a href=&#34;https://pratyushmaini.github.io/&#34;&gt;Pratyush Maini&lt;/a&gt; wrote a blog about the EMNLP 2024 best paper award winner: &lt;a href=&#34;https://www.anshumansuri.com/blog/2024/calibrated-mia/&#34;&gt;&lt;em&gt;Reassessing EMNLP 2024’s Best Paper: Does Divergence-Based Calibration for Membership Inference Attacks Hold Up?&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;As we explored in &lt;a href=&#34;https://uvasrg.github.io/do-membership-inference-attacks-work-on-large-language-models/&#34;&gt;&lt;em&gt;Do Membership Inference Attacks Work on Large Language Models?&lt;/em&gt;&lt;/a&gt;, to test a membership inference attack it is essentail to have a candidate set where the members and non-members are from the same distribution. If the distributions are different, the ability of an attack to distinguish members and non-members is indicative of distribution inference, not necessarily membership inference.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Common Way To Test for Leaks in Large Language Models May Be Flawed</title>
      <link>//uvasrg.github.io/common-way-to-test-for-leaks-in-large-language-models-may-be-flawed/</link>
      <pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/common-way-to-test-for-leaks-in-large-language-models-may-be-flawed/</guid>
      <description>&lt;p&gt;UVA News has an article on our LLM membership inference work:&#xA;&lt;a href=&#34;https://engineering.virginia.edu/news-events/news/common-way-test-leaks-large-language-models-may-be-flawed&#34;&gt;&lt;em&gt;Common Way To Test for Leaks in Large Language Models May Be Flawed: UVA Researchers Collaborated To Study the Effectiveness of Membership Inference Attacks&lt;/em&gt;&lt;/a&gt;, Eric Williamson, 13 November 2024.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Mismeasure of Man and Models</title>
      <link>//uvasrg.github.io/the-mismeasure-of-man-and-models/</link>
      <pubDate>Sun, 11 Aug 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/the-mismeasure-of-man-and-models/</guid>
      <description>&lt;h1 id=&#34;evaluating-allocational-harms-in-large-language-models&#34;&gt;Evaluating Allocational Harms in Large Language Models&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;Blog post written by &lt;a href=&#34;https://hannahxchen.github.io/&#34;&gt;Hannah Chen&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Our work considers &lt;i&gt;allocational harms&lt;/i&gt; that arise when model predictions are used to distribute scarce resources or opportunities.&lt;/p&gt;&#xA;&lt;h2 id=&#34;current-bias-metrics-do-not-reliably-reflect-allocation-disparities&#34;&gt;Current Bias Metrics Do Not Reliably Reflect Allocation Disparities&lt;/h2&gt;&#xA;&lt;p&gt;Several methods have been proposed to audit large language models (LLMs) for bias when used in critical decision-making, such as resume screening for hiring. Yet, these methods focus on &lt;i&gt;predictions&lt;/i&gt;, without considering how the predictions are used to make &lt;i&gt;decisions&lt;/i&gt;. In many settings, making decisions involve prioritizing options due to limited resource constraints. We find that prediction-based evaluation methods, which measure bias as the &lt;i&gt;average performance gap&lt;/i&gt; (δ) in prediction outcomes, do not reliably reflect disparities in allocation decision outcomes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Google&#39;s Trail of Crumbs</title>
      <link>//uvasrg.github.io/googles-trail-of-crumbs/</link>
      <pubDate>Mon, 05 Aug 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/googles-trail-of-crumbs/</guid>
      <description>&lt;p&gt;Matt Stoller published &lt;a href=&#34;https://www.thebignewsletter.com/p/googles-trail-of-crumbs&#34;&gt;my essay on Google&amp;rsquo;s decision to abandon its Privacy Sandbox Initiative&lt;/a&gt; in his Big newsletter:&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;https://www.thebignewsletter.com/p/googles-trail-of-crumbs&#34;&gt;&#xA;&lt;div class=&#34;substack-post-embed&#34;&gt;&lt;p lang=&#34;en&#34;&gt;Google&#39;s Trail of Crumbs by Matt Stoller&lt;/p&gt;&lt;p&gt;Google is too big to get rid of cookies. Even when it wants to protect users, it can&#39;t.&lt;/p&gt;&lt;a data-post-link href=&#34;https://www.thebignewsletter.com/p/googles-trail-of-crumbs&#34;&gt;Read on Substack&lt;/a&gt;&lt;/div&gt;&lt;script async src=&#34;https://substack.com/embedjs/embed.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/a&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;For more technical background on this, see Minjun&amp;rsquo;s paper: &lt;a href=&#34;https://arxiv.org/abs/2405.08102&#34;&gt;&lt;em&gt;Evaluating Google&amp;rsquo;s Protected Audience Protocol&lt;/em&gt;&lt;/a&gt; in PETS 2024.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Technology: US authorities survey AI ecosystem through antitrust lens</title>
      <link>//uvasrg.github.io/technology-us-authorities-survey-ai-ecosystem-through-antitrust-lens/</link>
      <pubDate>Sun, 04 Aug 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/technology-us-authorities-survey-ai-ecosystem-through-antitrust-lens/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m quoted in this article for the International Bar Association:&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.ibanet.org/technology-us-authorities-survey-ai-ecosystem-through-antitrust-lens&#34;&gt;&lt;em&gt;Technology: US authorities survey AI ecosystem through antitrust lens&lt;/em&gt;&lt;/a&gt;&lt;br&gt;&#xA;William Roberts, IBA US Correspondent&lt;br&gt;&#xA;Friday 2 August 2024&lt;/p&gt;&#xA;&lt;/center&gt;&#xA;&lt;blockquote&gt;&#xA;Antitrust authorities in the US are targeting the new frontier of artificial intelligence (AI) for potential enforcement action.&#xA;&lt;p&gt;&amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt;Jonathan Kanter, Assistant Attorney General for the Antitrust Division of the DoJ, warns that the government sees ‘structures and trends in AI that should give us pause’. He says that AI relies on massive amounts of data and computing power, which can give already dominant companies a substantial advantage. ‘Powerful network and feedback effects’ may enable dominant companies to control these new markets, Kanter adds.&lt;/p&gt;</description>
    </item>
    <item>
      <title>John Guttag Birthday Celebration</title>
      <link>//uvasrg.github.io/john-guttag-birthday-celebration/</link>
      <pubDate>Thu, 01 Aug 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/john-guttag-birthday-celebration/</guid>
      <description>&lt;p&gt;Maggie Makar organized a celebration for the 75th birthday of my PhD advisor, &lt;a href=&#34;https://people.csail.mit.edu/guttag/&#34;&gt;John Guttag&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;I wasn&amp;rsquo;t able to attend in person, unfortunately, but the occasion provided an opportunity to create a poster that looks back on what I&amp;rsquo;ve done since I started working with John over 30 years ago.&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/poster-for-jvg.pdf&#34; style=&#34;border-width:2px; border-color:blue&#34;&gt;&#xA;&lt;img src=&#34;//uvasrg.github.io/images/poster-for-jvg.png&#34; width=&#34;90%&#34; class=&#34;image-shadow&#34;&gt;&#xA;&lt;/a&gt;&#xA;&lt;/center&gt;</description>
    </item>
    <item>
      <title>SaTML Talk: SoK: Pitfalls in Evaluating Black-Box Attacks</title>
      <link>//uvasrg.github.io/satml-talk-sok-pitfalls-in-evaluating-black-box-attacks/</link>
      <pubDate>Mon, 22 Apr 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/satml-talk-sok-pitfalls-in-evaluating-black-box-attacks/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.anshumansuri.com/&#34;&gt;Anshuman Suri&lt;/a&gt;&amp;rsquo;s talk at &lt;a href=&#34;https://satml.org/&#34;&gt;IEEE Conference on Secure and Trustworthy Machine Learning&lt;/a&gt; (SaTML) is now available:&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube-nocookie.com/embed/ui4HMGe3aUs?si=M2A-uD77s4BdhXPR&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; allowfullscreen&gt;&lt;/iframe&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;See the &lt;a href=&#34;https://uvasrg.github.io/sok-pitfalls-in-evaluating-black-box-attacks/&#34;&gt;earlier blog post&lt;/a&gt; for more on the work, and the paper at &lt;a href=&#34;https://arxiv.org/abs/2310.17534&#34;&gt;https://arxiv.org/abs/2310.17534&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SoK: Pitfalls in Evaluating Black-Box Attacks</title>
      <link>//uvasrg.github.io/sok-pitfalls-in-evaluating-black-box-attacks/</link>
      <pubDate>Wed, 20 Dec 2023 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/sok-pitfalls-in-evaluating-black-box-attacks/</guid>
      <description>&lt;p&gt;Post by &lt;strong&gt;&lt;a href=&#34;https://www.anshumansuri.com/&#34;&gt;Anshuman Suri&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&#34;https://fsuya.org/&#34;&gt;Fnu Suya&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Much research has studied black-box attacks on image classifiers,&#xA;where adversaries generate adversarial examples against unknown target&#xA;models without having access to their internal information. Our&#xA;analysis of over 164 attacks (published in 102 major security, machine&#xA;learning and security conferences) shows how these works make&#xA;different assumptions about the adversary’s knowledge.&lt;/p&gt;&#xA;&lt;p&gt;The current literature lacks cohesive organization centered around the&#xA;threat model. Our &lt;a href=&#34;https://arxiv.org/abs/2310.17534&#34;&gt;SoK paper&lt;/a&gt; (to&#xA;appear at &lt;a href=&#34;https://satml.org/&#34;&gt;IEEE SaTML 2024&lt;/a&gt;) introduces a taxonomy&#xA;for systematizing these attacks and demonstrates the importance of&#xA;careful evaluations that consider adversary resources and threat&#xA;models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Model-Targeted Poisoning Attacks with Provable Convergence</title>
      <link>//uvasrg.github.io/model-targeted-poisoning-attacks-with-provable-convergence/</link>
      <pubDate>Tue, 29 Jun 2021 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/model-targeted-poisoning-attacks-with-provable-convergence/</guid>
      <description>&lt;p&gt;(Post by Sean Miller, using images adapted from Suya&amp;rsquo;s talk slides)&lt;/p&gt;&#xA;&lt;h2 id=&#34;data-poisoning-attacks&#34;&gt;Data Poisoning Attacks&lt;/h2&gt;&#xA;&lt;p&gt;Machine learning models are often trained using data from untrusted&#xA;sources, leaving them open to poisoning attacks where adversaries use&#xA;their control over a small fraction of that training data to poison&#xA;the model in a particular way.&lt;/p&gt;&#xA;&lt;p&gt;Most work on poisoning attacks is directly driven by an attacker&amp;rsquo;s&#xA;objective, where the adversary chooses poisoning points that maximize&#xA;some target objective. Our work focuses on &lt;em&gt;model-targeted&lt;/em&gt; poisoning&#xA;attacks, where the adversary splits the attack into choosing a target&#xA;model that satisfies the objective and then choosing poisoning points&#xA;that induce the target model.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
