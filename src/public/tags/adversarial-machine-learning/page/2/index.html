<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Adversarial Machine Learning | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
        <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
	      
	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      
<div style="margin-top:16px; margin-left: auto; margin-right: auto; max-width: 800px;">
<div class="row">
  <div class="column small-12">
    
    
    <h2><a href="/sok-let-the-privacy-games-begin-a-unified-treatment-of-data-inference-privacy-in-machine-learning/">SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-05-26 00:00:00 &#43;0000 UTC" itemprop="datePublished">26 May 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/ahmed-salem">Ahmed Salem</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/giovanni-cherubin">Giovanni Cherubin</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/boris-k%c3%b6pf">Boris K√∂pf</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/andrew-paverd">Andrew Paverd</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/shruti-tople">Shruti Tople</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/santiago-zanella-b%c3%a9guelin">Santiago Zanella-B√©guelin</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/membership-inference">membership inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>
    
  </span>
  
  
</div>


<p>Our paper on the use of cryptographic-style games to model inference privacy is published in <a href="https://www.ieee-security.org/TC/SP2023/"><em>IEEE Symposium on Security and Privacy</em></a> (Oakland):</p>
<blockquote>
<a href="https://www.microsoft.com/en-us/research/people/t-salemahmed/>Ahmed Salem</a>, <a href="https://www.microsoft.com/en-us/research/people/gcherubin/">Giovanni Cherubin</a>, <a href="https://www.cs.virginia.edu/evans"/David Evans</a>, <a href="https://www.microsoft.com/en-us/research/people/bokoepf/">Boris K√∂pf</a>, <a href="https://www.microsoft.com/en-us/research/people/anpaverd/">Andrew Paverd</a>, <a href="https://www.anshumansuri.com/">Anshuman Suri</a>, <a href="https://www.microsoft.com/en-us/research/people/shtople/">Shruti Tople</a>, and <a href="https://www.microsoft.com/en-us/research/people/santiago/">Santiago Zanella-B√©guelin</a>. <em>SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning</em>. IEEE Symposium on Security and Privacy, 2023. [<a href="https://arxiv.org/abs/2212.10986">Arxiv</a>]
</blockquote>
<h2 id="heading"></h2>
<center>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Tired of diverse definitions of machine learning privacy risks? Curious about game-based definitions? In our paper, we present privacy games as a tool for describing and analyzing privacy risks in machine learning. Join us on May 22nd, 11 AM <a href="https://twitter.com/IEEESSP?ref_src=twsrc%5Etfw">@IEEESSP</a> &#39;23 <a href="https://t.co/NbRuTmHyd2">https://t.co/NbRuTmHyd2</a> <a href="https://t.co/CIzsT7UY4b">pic.twitter.com/CIzsT7UY4b</a></p>&mdash; ahmed salem (@AhmedGaSalem) <a href="https://twitter.com/AhmedGaSalem/status/1658210153341001736?ref_src=twsrc%5Etfw">May 15, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

	

    
    <h2><a href="/cvpr-2023-manipulating-transfer-learning-for-property-inference/">CVPR 2023: Manipulating Transfer Learning for Property Inference</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-05-02 00:00:00 &#43;0000 UTC" itemprop="datePublished">2 May 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yulong-tian">Yulong Tian</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/transfer-learning">transfer learning</a>
    
  </span>
  
  
</div>


Manipulating Transfer Learning for Property Inference Transfer learning is a popular method to train deep learning models efficiently. By reusing parameters from upstream pre-trained models, the downstream trainer can use fewer computing resources to train downstream models, compared to training models from scratch.
The figure below shows the typical process of transfer learning for vision tasks:
However, the nature of transfer learning can be exploited by a malicious upstream trainer, leading to severe risks to the downstream trainer.
<p class="text-right"><a href="/cvpr-2023-manipulating-transfer-learning-for-property-inference/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/mico-challenge-in-membership-inference/">MICO Challenge in Membership Inference</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-02-10 00:00:00 &#43;0000 UTC" itemprop="datePublished">10 February 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/membership-inference">membership inference</a>
    
  </span>
  
  
</div>


Anshuman Suri wrote up an interesting post on his experience with the MICO Challenge, a membership inference competition that was part of SaTML. Anshuman placed second in the competition (on the CIFAR data set), where the metric is highest true positive rate at a 0.1 false positive rate over a set of models (some trained using differential privacy and some without).
Anshuman&rsquo;s post describes the methods he used and his experience in the competition: My submission to the MICO Challenge.
<p class="text-right"><a href="/mico-challenge-in-membership-inference/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/voice-of-america-interview-on-chatgpt/">Voice of America interview on ChatGPT</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-02-10 00:00:00 &#43;0000 UTC" itemprop="datePublished">10 February 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/large-language-models">large language models</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/nlp">nlp</a>
    
  </span>
  
  
</div>


<p>I was interviewed for a Voice of America story (in Russian) on the impact of chatGPT and similar tools.</p>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/dFuunAFX9y4?start=319" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</center>
<p>Full story: <a href="https://youtu.be/dFuunAFX9y4">https://youtu.be/dFuunAFX9y4</a></p>

	

    
    <h2><a href="/uh-oh-theres-a-new-way-to-poison-code-models/">Uh-oh, there&#39;s a new way to poison code models</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-01-16 00:00:00 &#43;0000 UTC" itemprop="datePublished">16 January 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/large-language-models">large language models</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/copilot">copilot</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning">poisoning</a>
    
  </span>
  
  
</div>


Jack Clark&rsquo;s Import AI, 16 Jan 2023 includes a nice description of our work on TrojanPuzzle:
####################################################
Uh-oh, there's a new way to poison code models - and it's really hard to detect:
‚Ä¶TROJANPUZZLE is a clever way to trick your code model into betraying you - if you can poison the undelrying dataset‚Ä¶
Researchers with the University of California, Santa Barbara, Microsoft Corporation, and the University of Virginia have come up with some clever, subtle ways to poison the datasets used to train code models.
<p class="text-right"><a href="/uh-oh-theres-a-new-way-to-poison-code-models/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/">Trojan Puzzle attack trains AI assistants into suggesting malicious code</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-01-10 00:00:00 &#43;0000 UTC" itemprop="datePublished">10 January 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/large-language-models">large language models</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/copilot">copilot</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning">poisoning</a>
    
  </span>
  
  
</div>


Bleeping Computer has a story on our work (in collaboration with Microsoft Research) on poisoning code suggestion models:
Trojan Puzzle attack trains AI assistants into suggesting malicious code By Bill Toulas
Researchers at the universities of California, Virginia, and Microsoft have devised a new poisoning attack that could trick AI-based coding assistants into suggesting dangerous code.
Named &lsquo;Trojan Puzzle,&rsquo; the attack stands out for bypassing static detection and signature-based dataset cleansing models, resulting in the AI models being trained to learn how to reproduce dangerous payloads.
<p class="text-right"><a href="/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/best-submission-award-at-visxai-2022/">Best Submission Award at VISxAI 2022</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-10-18 00:00:00 &#43;0000 UTC" itemprop="datePublished">18 October 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/evan-rose">Evan Rose</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-syua">Fnu Syua</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning-attacks">poisoning attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>
    
  </span>
  
  
</div>


Poisoning Attacks and Subpopulation Susceptibility by Evan Rose, Fnu Suya, and David Evans won the Best Submission Award at the 5th Workshop on Visualization for AI Explainability.
Undergraduate student Evan Rose led the work and presented it at VISxAI in Oklahoma City, 17 October 2022.
Congratulations to #VISxAI&#39;s Best Submission Awards:
üèÜ K-Means Clustering: An Explorable Explainer by @yizhe_ang https://t.co/BULW33WPzo
üèÜ Poisoning Attacks and Subpopulation Susceptibility by Evan Rose, @suyafnu, and @UdacityDave https://t.
<p class="text-right"><a href="/best-submission-award-at-visxai-2022/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/visualizing-poisoning/">Visualizing Poisoning</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-07-26 00:00:00 &#43;0000 UTC" itemprop="datePublished">26 July 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/evan-rose">Evan Rose</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning-attacks">poisoning attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>
    
  </span>
  
  
</div>


How does a poisoning attack work and why are some groups more susceptible to being victimized by a poisoning attack?
We&rsquo;ve posted work that helps understand how poisoning attacks work with some engaging visualizations:
Poisoning Attacks and Subpopulation Susceptibility
An Experimental Exploration on the Effectiveness of Poisoning Attacks
Evan Rose, Fnu Suya, and David Evans
Follow the link to try the interactive version! Machine learning is susceptible to poisoning attacks in which adversaries inject maliciously crafted training data into the training set to induce specific model behavior.
<p class="text-right"><a href="/visualizing-poisoning/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/congratulations-dr.-zhang/">Congratulations, Dr. Zhang!</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-07-19 00:00:00 &#43;0000 UTC" itemprop="datePublished">19 July 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/phd-defense">PhD defense</a>
    
  </span>
  
  
</div>


Congratulations to Xiao Zhang for successfully defending his PhD thesis!
Dr. Zhang and his PhD committee: Somesh Jha (University of Wisconsin), David Evans, Tom Fletcher; Tianxi&nbsp;Li&nbsp;(UVA&nbsp;Statistics), David&nbsp;Wu&nbsp;(UT&nbsp;Austin), Mohammad&nbsp;Mahmoody; Xiao&nbsp;Zhang. Xiao will join the CISPA Helmholtz Center for Information Security in Saarbr√ºcken, Germany this fall as a tenure-track faculty member.
From Characterizing Intrinsic Robustness to Adversarially Robust Machine Learning The prevalence of adversarial examples raises questions about the reliability of machine learning systems, especially for their deployment in critical applications.
<p class="text-right"><a href="/congratulations-dr.-zhang/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/iclr-2022-understanding-intrinsic-robustness-using-label-uncertainty/">ICLR 2022: Understanding Intrinsic Robustness Using Label Uncertainty</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-03-24 00:00:00 &#43;0000 UTC" itemprop="datePublished">24 March 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/iclr">ICLR</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/intrinsic-robustness">intrinsic robustness</a>
    
  </span>
  
  
</div>


(Blog post written by Xiao Zhang)
Motivated by the empirical hardness of developing robust classifiers against adversarial perturbations, researchers began asking the question ‚ÄúDoes there even exist a robust classifier?‚Äù. This is formulated as the intrinsic robustness problem (Mahloujifar et al., 2019), where the goal is to characterize the maximum adversarial robustness possible for a given robust classification problem. Building upon the connection between adversarial robustness and classifier‚Äôs error region, it has been shown that if we restrict the search to the set of imperfect classifiers, the intrinsic robustness problem can be reduced to the concentration of measure problem.
<p class="text-right"><a href="/iclr-2022-understanding-intrinsic-robustness-using-label-uncertainty/">Read More‚Ä¶</a></p>
	

    
    <div class="row">
  <div class="column small-12">
    <ul class="pagination" role="navigation" aria-label="Pagination">
      
      <li class="arrow" aria-disabled="true"><a href="/tags/adversarial-machine-learning/page/3/">&laquo; <em>Older<span class="show-for-sr"> blog entries</span></em></a></li>
      
      <li><span>Page 2 of 6</span></li>      
      
      <li class="arrow" aria-disabled="true"><a href="/tags/adversarial-machine-learning/"><em>Newer<span class="show-for-sr"> blog entries</span></em>:&nbsp;&raquo;</a></li>
      
    </ul>    
    All Posts by <a href="//uvasrg.github.io/categories">Category</a> or <a href="//uvasrg.github.io/tags">Tags</a>.

  </div>
</div>

  </div>
</div>
</div>

    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
