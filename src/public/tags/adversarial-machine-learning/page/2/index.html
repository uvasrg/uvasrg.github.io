<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Adversarial Machine Learning | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
        <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
	      
	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      
<div style="margin-top:16px; margin-left: auto; margin-right: auto; max-width: 800px;">
<div class="row">
  <div class="column small-12">
    
    
    <h2><a href="/sok-let-the-privacy-games-begin-a-unified-treatment-of-data-inference-privacy-in-machine-learning/">SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-05-26 00:00:00 &#43;0000 UTC" itemprop="datePublished">26 May 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/ahmed-salem">Ahmed Salem</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/giovanni-cherubin">Giovanni Cherubin</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/boris-k%c3%b6pf">Boris K√∂pf</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/andrew-paverd">Andrew Paverd</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/shruti-tople">Shruti Tople</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/santiago-zanella-b%c3%a9guelin">Santiago Zanella-B√©guelin</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/membership-inference">membership inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>
    
  </span>
  
  
</div>


<p>Our paper on the use of cryptographic-style games to model inference privacy is published in <a href="https://www.ieee-security.org/TC/SP2023/"><em>IEEE Symposium on Security and Privacy</em></a> (Oakland):</p>
<blockquote>
<a href="https://www.microsoft.com/en-us/research/people/t-salemahmed/>Ahmed Salem</a>, <a href="https://www.microsoft.com/en-us/research/people/gcherubin/">Giovanni Cherubin</a>, <a href="https://www.cs.virginia.edu/evans"/David Evans</a>, <a href="https://www.microsoft.com/en-us/research/people/bokoepf/">Boris K√∂pf</a>, <a href="https://www.microsoft.com/en-us/research/people/anpaverd/">Andrew Paverd</a>, <a href="https://www.anshumansuri.com/">Anshuman Suri</a>, <a href="https://www.microsoft.com/en-us/research/people/shtople/">Shruti Tople</a>, and <a href="https://www.microsoft.com/en-us/research/people/santiago/">Santiago Zanella-B√©guelin</a>. <em>SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning</em>. IEEE Symposium on Security and Privacy, 2023. [<a href="https://arxiv.org/abs/2212.10986">Arxiv</a>]
</blockquote>
<h2 id="heading"></h2>
<center>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Tired of diverse definitions of machine learning privacy risks? Curious about game-based definitions? In our paper, we present privacy games as a tool for describing and analyzing privacy risks in machine learning. Join us on May 22nd, 11 AM <a href="https://twitter.com/IEEESSP?ref_src=twsrc%5Etfw">@IEEESSP</a> &#39;23 <a href="https://t.co/NbRuTmHyd2">https://t.co/NbRuTmHyd2</a> <a href="https://t.co/CIzsT7UY4b">pic.twitter.com/CIzsT7UY4b</a></p>
<p class="text-right"><a href="/sok-let-the-privacy-games-begin-a-unified-treatment-of-data-inference-privacy-in-machine-learning/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/cvpr-2023-manipulating-transfer-learning-for-property-inference/">CVPR 2023: Manipulating Transfer Learning for Property Inference</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-05-02 00:00:00 &#43;0000 UTC" itemprop="datePublished">2 May 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yulong-tian">Yulong Tian</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/transfer-learning">transfer learning</a>
    
  </span>
  
  
</div>


<h1 id="manipulating-transfer-learning-for-property-inference">Manipulating Transfer Learning for Property Inference</h1>
<p>Transfer learning is a popular method to train deep learning models
efficiently. By reusing parameters from upstream pre-trained models,
the downstream trainer can use fewer computing resources to train
downstream models, compared to training models from scratch.</p>
<p>The figure below shows the typical process of transfer learning for
vision tasks:</p>
<center>
<a href="/images/mtlpi/fig1.png"><img src="/images/mtlpi/fig1.png" width="80%"></a>
</center>
<p>However, the nature of transfer learning can be exploited by a
malicious upstream trainer, leading to severe risks to the downstream
trainer.</p>
<p class="text-right"><a href="/cvpr-2023-manipulating-transfer-learning-for-property-inference/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/mico-challenge-in-membership-inference/">MICO Challenge in Membership Inference</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-02-10 00:00:00 &#43;0000 UTC" itemprop="datePublished">10 February 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/membership-inference">membership inference</a>
    
  </span>
  
  
</div>


<p><a href="https://www.anshumansuri.com">Anshuman Suri</a> wrote up an interesting
post on his experience with the <a href="https://github.com/microsoft/MICO">MICO
Challenge</a>, a membership inference
competition that was part of <a href="https://satml.org/">SaTML</a>. Anshuman
placed second in the competition (on the CIFAR data set), where the
metric is highest true positive rate at a 0.1 false positive rate over
a set of models (some trained using differential privacy and some
without).</p>
<p>Anshuman&rsquo;s post describes the methods he used and his experience in
the competition: <a href="https://www.anshumansuri.com/post/mico/"><em>My submission to the MICO
Challenge</em></a>.</p>
<p class="text-right"><a href="/mico-challenge-in-membership-inference/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/voice-of-america-interview-on-chatgpt/">Voice of America interview on ChatGPT</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-02-10 00:00:00 &#43;0000 UTC" itemprop="datePublished">10 February 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/large-language-models">large language models</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/nlp">nlp</a>
    
  </span>
  
  
</div>


<p>I was interviewed for a Voice of America story (in Russian) on the impact of chatGPT and similar tools.</p>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/dFuunAFX9y4?start=319" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</center>
<p>Full story: <a href="https://youtu.be/dFuunAFX9y4">https://youtu.be/dFuunAFX9y4</a></p>

	

    
    <h2><a href="/uh-oh-theres-a-new-way-to-poison-code-models/">Uh-oh, there&#39;s a new way to poison code models</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-01-16 00:00:00 &#43;0000 UTC" itemprop="datePublished">16 January 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/large-language-models">large language models</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/copilot">copilot</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning">poisoning</a>
    
  </span>
  
  
</div>


<p>Jack Clark&rsquo;s <a href="https://mailchi.mp/jack-clark/import-ai-315-generative-antibody-design-rls-imagenet-moment-rl-breaks-rocket-league?e=545365c0e9">Import AI, 16 Jan 2023</a> includes a nice description of our work on TrojanPuzzle:</p>
<p style="margin: 10px 0;padding: 0;mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #202020;font-family: Helvetica;font-size: 16px;line-height: 150%;text-align: left;"><strong>####################################################</strong></p>
<!-- /wp:paragraph --><!-- wp:paragraph -->
<p style="margin: 10px 0;padding: 0;mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #202020;font-family: Helvetica;font-size: 16px;line-height: 150%;text-align: left;"><strong>Uh-oh, there's a new way to poison code models - and it's really hard to detect:</strong><br>
<em>‚Ä¶TROJANPUZZLE is a clever way to trick your code model into betraying you - if you can poison the undelrying dataset‚Ä¶</em><br>
Researchers with the University of California, Santa Barbara, Microsoft Corporation, and the University of Virginia have come up with some clever, subtle ways to poison the datasets used to train code models. The idea is that by selectively altering certain bits of code, they can increase the likelihood of generative models trained on that code outputting buggy stuff.&nbsp;</p>
<p class="text-right"><a href="/uh-oh-theres-a-new-way-to-poison-code-models/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/">Trojan Puzzle attack trains AI assistants into suggesting malicious code</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-01-10 00:00:00 &#43;0000 UTC" itemprop="datePublished">10 January 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/large-language-models">large language models</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/copilot">copilot</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning">poisoning</a>
    
  </span>
  
  
</div>


<p>Bleeping Computer has a <a href="https://www.bleepingcomputer.com/news/security/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/">story on our work</a> (in collaboration with Microsoft Research) on poisoning code suggestion models:</p>
<h1>Trojan Puzzle attack trains AI assistants into suggesting malicious code</h1>
<p>By <b>Bill Toulas</b></p>
<center>
<img alt="Person made of jigsaw puzzle pieces" height="900" src="https://www.bleepstatic.com/content/hl-images/2022/10/09/mystery-hacker.jpg" width="80%"></img>
</center>
<p> </p>
<p>Researchers at the universities of California, Virginia, and Microsoft have devised a new poisoning attack that could trick AI-based coding assistants into suggesting dangerous code.</p>
<p>Named &lsquo;Trojan Puzzle,&rsquo; the attack stands out for bypassing static detection and signature-based dataset cleansing models, resulting in the AI models being trained to learn how to reproduce dangerous payloads.</p>
<p class="text-right"><a href="/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/best-submission-award-at-visxai-2022/">Best Submission Award at VISxAI 2022</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-10-18 00:00:00 &#43;0000 UTC" itemprop="datePublished">18 October 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/evan-rose">Evan Rose</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-syua">Fnu Syua</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning-attacks">poisoning attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>
    
  </span>
  
  
</div>


<p><a href="https://uvasrg.github.io/poisoning/"><em>Poisoning Attacks and Subpopulation Susceptibility</em></a> by Evan Rose, Fnu Suya, and David Evans won the Best Submission Award at the <a href="https://visxai.io/">5th Workshop on Visualization for AI Explainability</a>.</p>
<p>Undergraduate student Evan Rose led the work and presented it at VISxAI in Oklahoma City, 17 October 2022.</p>
<center>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Congratulations to <a href="https://twitter.com/hashtag/VISxAI?src=hash&amp;ref_src=twsrc%5Etfw">#VISxAI</a>&#39;s Best Submission Awards:<br><br>üèÜ K-Means Clustering: An Explorable Explainer by <a href="https://twitter.com/yizhe_ang?ref_src=twsrc%5Etfw">@yizhe_ang</a> <a href="https://t.co/BULW33WPzo">https://t.co/BULW33WPzo</a><br><br>üèÜ Poisoning Attacks and Subpopulation Susceptibility by Evan Rose, <a href="https://twitter.com/suyafnu?ref_src=twsrc%5Etfw">@suyafnu</a>, and <a href="https://twitter.com/UdacityDave?ref_src=twsrc%5Etfw">@UdacityDave</a> <a href="https://t.co/Z12D3PvfXu">https://t.co/Z12D3PvfXu</a><a href="https://twitter.com/hashtag/ieeevis?src=hash&amp;ref_src=twsrc%5Etfw">#ieeevis</a></p>&mdash; VISxAI (@VISxAI) <a href="https://twitter.com/VISxAI/status/1582085676857577473?ref_src=twsrc%5Etfw">October 17, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Next up is best submission award üèÖ winner, &quot;Poisoning Attacks and Subpopulation Susceptibility&quot; by Evan Rose, <a href="https://twitter.com/suyafnu?ref_src=twsrc%5Etfw">@suyafnu</a>, and <a href="https://twitter.com/UdacityDave?ref_src=twsrc%5Etfw">@UdacityDave</a>.<br><br>Tune in to learn why some data subpopulations are more vulnerable to attacks than others!<a href="https://t.co/Z12D3PvfXu">https://t.co/Z12D3PvfXu</a><a href="https://twitter.com/hashtag/ieeevis?src=hash&amp;ref_src=twsrc%5Etfw">#ieeevis</a> <a href="https://twitter.com/hashtag/VISxAI?src=hash&amp;ref_src=twsrc%5Etfw">#VISxAI</a> <a href="https://t.co/Gm2JBpWQSP">pic.twitter.com/Gm2JBpWQSP</a></p>
<p class="text-right"><a href="/best-submission-award-at-visxai-2022/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/visualizing-poisoning/">Visualizing Poisoning</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-07-26 00:00:00 &#43;0000 UTC" itemprop="datePublished">26 July 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/evan-rose">Evan Rose</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning-attacks">poisoning attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>
    
  </span>
  
  
</div>


<p><em>How does a poisoning attack work and why are some groups more
susceptible to being victimized by a poisoning attack?</em></p>
<p>We&rsquo;ve posted work that helps understand how poisoning attacks work
with some engaging visualizations:</p>
<p><a href="/poisoning">Poisoning Attacks and Subpopulation Susceptibility</a><br>
<em>An Experimental Exploration on the Effectiveness of Poisoning Attacks</em><br>
Evan Rose, Fnu Suya, and David Evans</p>
<center>
<a href="/poisoning"><img src="/images/visualizingpoisoning.png" width="85%"></a><br>
Follow <a href="/poisoning">the link</a> to try the interactive version!
</center>
<h1 id="heading"></h1>
<p>Machine learning is susceptible to poisoning attacks in which
adversaries inject maliciously crafted training data into the training
set to induce specific model behavior. We focus on subpopulation
attacks, in which the attacker&rsquo;s goal is to induce a model that
produces a targeted and incorrect output (label blue in our demos) for
a particular subset of the input space (colored orange). We study the
question, which subpopulations are the most vulnerable to an attack
and why?</p>
<p class="text-right"><a href="/visualizing-poisoning/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/congratulations-dr.-zhang/">Congratulations, Dr. Zhang!</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-07-19 00:00:00 &#43;0000 UTC" itemprop="datePublished">19 July 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/phd-defense">PhD defense</a>
    
  </span>
  
  
</div>


<p>Congratulations to Xiao Zhang for successfully defending his PhD thesis!</p>
<center>
<img src="/images/xiaozhangphd.png" width="75%">
<div class="caption">
Dr. Zhang and his PhD committee: Somesh Jha (University of Wisconsin), David Evans, Tom Fletcher; Tianxi&nbsp;Li&nbsp;(UVA&nbsp;Statistics), David&nbsp;Wu&nbsp;(UT&nbsp;Austin), Mohammad&nbsp;Mahmoody; Xiao&nbsp;Zhang.
</center>
<h2 id="heading"></h2>
<p>Xiao will join the <a href="https://cispa.de/en">CISPA Helmholtz Center for Information
Security</a> in Saarbr√ºcken, Germany this fall as a
tenure-track faculty member.</p>
<h2 id="heading-1"></h2>
<center>
<em>From Characterizing Intrinsic Robustness to Adversarially Robust Machine Learning</em>
</center>
<h2 id="heading-2"></h2>
<p>The prevalence of adversarial examples raises questions about the
reliability of machine learning systems, especially for their
deployment in critical applications. Numerous defense mechanisms have
been proposed that aim to improve a machine learning system‚Äôs
robustness in the presence of adversarial examples. However, none of
these methods are able to produce satisfactorily robust models, even
for simple classification tasks on benchmarks. In addition to
empirical attempts to build robust models, recent studies have
identified intrinsic limitations for robust learning against
adversarial examples. My research aims to gain a deeper understanding
of why machine learning models fail in the presence of adversaries and
design ways to build better robust systems. In this dissertation, I
develop a concentration estimation framework to characterize the
intrinsic limits of robustness for typical classification tasks of
interest. The proposed framework leads to the discovery that compared
with the concentration of measure which was previously argued to be an
important factor, the existence of uncertain inputs may explain more
fundamentally the vulnerability of state-of-the-art
defenses. Moreover, to further advance our understanding of
adversarial examples, I introduce a notion of representation
robustness based on mutual information, which is shown to be related
to an intrinsic limit of model robustness for downstream
classification tasks. Finally in this dissertation, I advocate for a
need to rethink the current design goal of robustness and shed light
on ways to build better robust machine learning systems, potentially
escaping the intrinsic limits of robustness.</p>
<p class="text-right"><a href="/congratulations-dr.-zhang/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/iclr-2022-understanding-intrinsic-robustness-using-label-uncertainty/">ICLR 2022: Understanding Intrinsic Robustness Using Label Uncertainty</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-03-24 00:00:00 &#43;0000 UTC" itemprop="datePublished">24 March 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/iclr">ICLR</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/intrinsic-robustness">intrinsic robustness</a>
    
  </span>
  
  
</div>


<p>(Blog post written by <a href="https://xiao-zhang.net/">Xiao Zhang</a>)</p>
<p>Motivated by the empirical hardness of developing robust classifiers
against adversarial perturbations, researchers began asking the
question ‚Äú<em>Does there even exist a robust classifier?</em>‚Äù. This is
formulated as the <strong><em>intrinsic robustness problem</em></strong> <a href="https://proceedings.neurips.cc/paper/2019/file/46f76a4bda9a9579eab38a8f6eabcda1-Paper.pdf">(Mahloujifar et
al.,
2019)</a>,
where the goal is to characterize the maximum adversarial robustness
possible for a given robust classification problem. Building upon the
connection between adversarial robustness and classifier‚Äôs error
region, it has been shown that if we restrict the search to the set of
imperfect classifiers, the intrinsic robustness problem can be reduced
to the <strong><em>concentration of measure problem</em></strong>.</p>
<p class="text-right"><a href="/iclr-2022-understanding-intrinsic-robustness-using-label-uncertainty/">Read More‚Ä¶</a></p>
	

    
    <div class="row">
  <div class="column small-12">
    <ul class="pagination" role="navigation" aria-label="Pagination">
      
      <li class="arrow" aria-disabled="true"><a href="/tags/adversarial-machine-learning/page/3/">&laquo; <em>Older<span class="show-for-sr"> blog entries</span></em></a></li>
      
      <li><span>Page 2 of 6</span></li>      
      
      <li class="arrow" aria-disabled="true"><a href="/tags/adversarial-machine-learning/"><em>Newer<span class="show-for-sr"> blog entries</span></em>:&nbsp;&raquo;</a></li>
      
    </ul>    
    All Posts by <a href="//uvasrg.github.io/categories">Category</a> or <a href="//uvasrg.github.io/tags">Tags</a>.

  </div>
</div>

  </div>
</div>
</div>

    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
