<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Adversarial Examples on Security Research Group</title>
    <link>//uvasrg.github.io/tags/adversarial-examples/</link>
    <description>Recent content in Adversarial Examples on Security Research Group</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Sun, 13 Nov 2022 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/tags/adversarial-examples/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models</title>
      <link>//uvasrg.github.io/balancing-tradeoffs-between-fickleness-and-obstinacy-in-nlp-models/</link>
      <pubDate>Sun, 13 Nov 2022 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/balancing-tradeoffs-between-fickleness-and-obstinacy-in-nlp-models/</guid>
      <description>&lt;p&gt;Post by &lt;a href=&#34;https://hannahxchen.github.io/&#34;&gt;Hannah Chen&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Our work on balanced adversarial training looks at how to train models&#xA;that are robust to two different types of adversarial examples:&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://hannahxchen.github.io/&#34;&gt;Hannah Chen&lt;/a&gt;, &lt;a href=&#34;http://yangfengji.net/&#34;&gt;Yangfeng&#xA;Ji&lt;/a&gt;, &lt;a href=&#34;http://www.cs.virginia.edu/~evans/&#34;&gt;David&#xA;Evans&lt;/a&gt;. &lt;em&gt;Balanced Adversarial&#xA;Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP&#xA;Models&lt;/em&gt;. In &lt;a href=&#34;https://2022.emnlp.org/&#34;&gt;&lt;em&gt;The 2022 Conference on Empirical Methods in Natural&#xA;Language Processing&lt;/em&gt;&lt;/a&gt; (EMNLP), Abu Dhabi,&#xA;7-11 December 2022.  [&lt;a href=&#34;https://arxiv.org/abs/2210.11498&#34;&gt;ArXiv&lt;/a&gt;]&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube-nocookie.com/embed/xQH51lIVDyY&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&#xA;&lt;/center&gt;&#xA;&lt;h2 id=&#34;adversarial-examples&#34;&gt;Adversarial Examples&lt;/h2&gt;&#xA;&lt;p&gt;At the broadest level, an adversarial example is an input crafted intentionally to confuse a model. However, most work focus on the defintion as an input constructed by applying a small perturbation that preserves the ground truth label but changes model&amp;rsquo;s output &lt;a href=&#34;https://arxiv.org/abs/1412.6572&#34;&gt;(Goodfellow et al., 2015)&lt;/a&gt;. We refer it as a &lt;strong&gt;fickle adversarial example&lt;/strong&gt;. On the other hand, attackers can target an opposite objective where the inputs are made with minimal changes that change the ground truth labels but retain model&amp;rsquo;s predictions &lt;a href=&#34;https://arxiv.org/abs/1811.00401&#34;&gt;(Jacobsen et al., 2018)&lt;/a&gt;. We refer these malicious inputs as &lt;strong&gt;obstinate adversarial examples&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
