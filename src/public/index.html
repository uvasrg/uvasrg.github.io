<!doctype html>
<html class="no-js" lang="en-us">
  <head>
	<meta name="generator" content="Hugo 0.87.0" />
    <meta charset="utf-8">
    <title>Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
	      
	    </div>
	  </div>
	    
	  
	    <div class="top-bar" id="site-menu" >	      
	      <div class="top-bar-title show-for-medium site-title">
		<a href="//uvasrg.github.io/">Security Research Group</a>
	      </div>
	      <div class="top-bar-left">
		<ul class="menu vertical medium-horizontal">
		  
		  
		</ul>
	      </div>
	      <div class="top-bar-right show-for-medium">
		
	         <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
		
	      </div>
	    </div>
	  
	</nav>
      
    </header>
    
    <main>
      





<div class="container">
 <div>

    <div class="column small-18 medium-9">
      
    <div class="content">

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
           <h1 id="centersecurity-and-privacy-research-at-the-university-of-virginiacenter"><center>Security and Privacy Research at the University of Virginia</center></h1>
<p></p>
<div class="row">
<div class="column small-10 medium-6">
<p>Our research seeks to empower individuals and organizations to control
how their data is used.  We use techniques from cryptography,
programming languages, machine learning, operating systems, and other
areas to both understand and improve the privacy and security of
computing as practiced today, and as envisioned in the future. A major
current focus is on <em>adversarial machine learning</em>.</p>
</p> 
 <p>
<p>Everyone is welcome at our research group meetings. To get
announcements, join our <a
href="https://teams.microsoft.com/l/team/19%3aWdkw2xYq6taXh-0OftqQdt8SQ2vyvUI_Z0ZL39APghY1%40thread.tacv2/conversations?groupId=58076b41-c835-4a07-abaa-705bc7cca101&tenantId=7b3480c7-3707-4873-8b77-e216733a65ac">Teams Group</a> (any
<em>@virginia.edu</em> email address can join themsleves; others should <a href="mailto:evans@virginia.edu">email me</a> to request an invitation). </p> </div></p>
<div class="column small-10 medium-6">
<center> <a
href="/images/srg-lunch-2022-08-22.png"><img
src="/images/srg-lunch-2022-08-22-small.png" alt="SRG lunch"
width=98%></a></br> <b>Security Research Group Lunch</b> <font
size="-1">(22&nbsp;August&nbsp;2022)</font><br> <div
class="smallcaption">
<a href="https://bargavjayaraman.github.io/">Bargav&nbsp;Jayaraman</a>,
<a href="https://www.josephinelamp.com/">Josephine&nbsp;Lamp</a>,
<a href="https://hannahxchen.github.io/">Hannah&nbsp;Chen</a>,
<A href="https://www.linkedin.com/in/minjun-elena-long-06a283173/">Elena&nbsp;Long</a>,
Yanjin&nbsp;Chen,<br>
<a href="https://web.archive.org/web/20190909071143/http://www.cs.virginia.edu:80/~sza4uq/">Samee&nbsp;Zahur</a>&nbsp;(PhD&nbsp;2016),
<a href="https://sites.google.com/virginia.edu/anshuman/home">Anshuman&nbsp;Suri</a>,
<A href="https://fsuya.org/">Fnu&nbsp;Suya</a>,
Tingwei&nbsp;Zhang,
Scott&nbsp;Hong
</font> </center>
 </p> </div> </div>
<div class="row">
<div class="column small-10 medium-5">
<div class="mainsection">Active Projects</div>
<p><a href="/privacy/"><b>Privacy for Machine Learning</b></a> <br>
<a href="//www.evademl.org/"><b>Security for Machine Learning</b> (EvadeML)</a><br>
<a href="/pointwise-paraphrase-appraisal-is-potentially-problematic/">NLP Robustness</a></p>
</div>
<div class="column small-14 medium-7">
<div class="mainsection">Past Projects</div>
<em>
<a href="//securecomputation.org">Secure Multi-Party Computation</a></em>:
<a href="//www.oblivc.org/">Obliv-C</a> &middot; <a href="//www.mightbeevil.org/">MightBeEvil</a><br>
<p><em>Web and Mobile Security</em>: <a href="http://www.scriptinspector.org/">ScriptInspector</a> 路
<a href="http://www.ssoscan.org/">SSOScan</a><br>
<em>Program Analysis</em>: <a href="//www.splint.org/">Splint</a> 路 <a href="//www.cs.virginia.edu/perracotta">Perracotta</a><br>
<a href="//www.cs.virginia.edu/nvariant/">N-Variant Systems</a> 路
<a href="//www.cs.virginia.edu/physicrypt/">Physicrypt</a> 路
<a href="//www.cs.virginia.edu/evans/research.html">More&hellip;</a></p>
</p>
</div>
</div>

        
    
  

    <div class="mainsection">Recent Posts</div>

    
    <h2><a href="/cray-distinguished-speaker-on-leaky-models-and-unintended-inferences/">Cray Distinguished Speaker: On Leaky Models and Unintended Inferences</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-12-13 00:00:00 &#43;0000 UTC" itemprop="datePublished">13 December 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/property-inference">property inference</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Here&rsquo;s the slides from my <a href="https://cse.umn.edu/cs/cray">Cray Distinguished Speaker</a> talk on <a href="https://cse.umn.edu/cs/events/cse-colloquium-leaky-models-and-unintended-inferences"><em>On Leaky Models and Unintended Inferences</em></a>: [<a href="https://www.dropbox.com/s/5gi766dqezsitw4/cray2022.pdf?dl=0">PDF</a>]</p>
<center>
<a href="https://www.dropbox.com/s/5gi766dqezsitw4/cray2022.pdf?dl=0"><img src="/images/cray2022-title.png" width="65%" alt="Leaky Models and Unintended Inferences"></a> 
</center>
</br>
<p>The chatGPT limerick version of my talk abstract is much better than mine:</p>
<blockquote>
<p>A machine learning model, oh so grand<br>
With data sets that it held in its hand
It performed quite well<br>
But secrets to tell<br>
And an adversary&rsquo;s tricks it could not withstand.</p>
</blockquote>
<p>Thanks to Stephen McCamant and Kangjie Lu for hosting my visit, and everyone at University of Minnesota. Also great to catch up with UVA BSCS alumn, <a href="https://www-users.cse.umn.edu/~sjguy/">Stephen J. Guy</a>.</p>
<p>The main works I talked about are:</p>
<ul>
<li>
<p>Anshuman Suri&rsquo;s work on <a href="/on-the-risks-of-distribution-inference/">distribution inference</a> (Paper: <a href="https://arxiv.org/abs/2109.06024"><em>Formalizing and Estimating Distribution Inference Risks</em></a>, PETS 2022)</p>
</li>
<li>
<p>Bargav Jayaraman&rsquo;s work on <a href="/attribute-inference-attacks-are-really-imputation">attribute inference</a> (Paper: <a href="https://arxiv.org/abs/2209.01292"><em>Are Attribute Inference Attacks Just Imputation?</em></a> (<a href="https://arxiv.org/abs/2209.01292">arXiv</a>). <a href="https://www.sigsac.org/ccs/CCS2022/">CCS 2022</a>)</p>
</li>
<li>
<p>Our new SaTML paper: <a href="http://anshumansuri.me/">Anshuman Suri</a>, Yifu Lu, Yanjin Chen, <a href="http://www.cs.virginia.edu/~evans/">David Evans</a>. <a href="https://www.anshumansuri.me/publication/dissecting/"><em>Dissecting Distribution Inference</em></a> (SaTML, 2023).</p>
</li>
</ul>
<p>The talk from Randy Pausch (Cray Distinguished Speaker, 1999-2000) that I mentioned is available here: <a href="https://www.youtube.com/watch?v=blaK_tB_KQA"><em>Time Management</em></a>, November 2007.</p>
<p>The transcript of Seymour Cray&rsquo;s talk at the University of Virginia is
here: <a href="https://americanhistory.si.edu/comphist/montic/cray.htm"><em>An Imaginary Tour of a Biological Computer (Why Computer
Professionals and Molecular Biologists Should Start
Collaborating)</em></a>. Remarks of Seymour Cray to the Shannon Center for Advanced Studies, University
of Virginia. 30 May 1996.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/attribute-inference-attacks-are-really-imputation/">Attribute Inference attacks are really Imputation</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-12-07 00:00:00 &#43;0000 UTC" itemprop="datePublished">7 December 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bargav-jayaraman">Bargav Jayaraman</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/attribute-inference">attribute inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p><strong>Post by Bargav Jayaraman</strong></p>
<p><em>Attribute inference</em> attacks have been shown by prior works to pose privacy threat against ML models. However, these works assume the knowledge of the training distribution and we show that in such cases these attacks do no better than a data imputataion attack that does not have access to the model. We explore the attribute inference risks in the cases where the adversary has limited or no prior knowledge of the training distribution and show that our white-box attribute inference attack (that uses neuron activations to infer the unknown sensitive attribute) surpasses imputation in these data constrained cases. This attack uses the training distribution information leaked by the model, and thus poses privacy risk when the distribution is private.</p>
<center>
<a href="/images/ai/ai.pdf"><img alt="" src="/images/ai/ai.png" width="60%"></a>
</center>
<br>
<h2 id="prior-attribute-inference-attacks-do-not-pose-privacy-risk">Prior Attribute Inference Attacks Do Not Pose Privacy Risk</h2>
<p>Prior works in attribute inference have mainly considered black-box
access to the machine learning model and show successful attribute
inference (in terms of attack accuracy) in the case where the
adversary has access to the underlying training distribution. Our
experiments show that in such cases even an imputation adversary,
without access to the model, can achieve high inference accuracy, as
shown in the table below:</p>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th></th>
<th style="text-align:center">Census (Race)</th>
<th></th>
<th style="text-align:center">Texas-100X (Ethnicity)</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Predict Most Common</td>
<td></td>
<td style="text-align:center">0.78</td>
<td></td>
<td style="text-align:center">0.72</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">Imputation Attack</td>
<td></td>
<td style="text-align:center">0.82</td>
<td></td>
<td style="text-align:center">0.72</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">Yeom et al. Attack</td>
<td></td>
<td style="text-align:center">0.65</td>
<td></td>
<td style="text-align:center">0.58</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">Mehnaz et al. Attack</td>
<td></td>
<td style="text-align:center">0.06</td>
<td></td>
<td style="text-align:center">0.60</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">WCAI (Our version of Yeom)</td>
<td></td>
<td style="text-align:center">0.83</td>
<td></td>
<td style="text-align:center">0.74</td>
<td></td>
</tr>
</tbody>
</table>
<center>Comparing accuracy of attribute inference attacks</center>
</br>
<p>Thus, these attribute inference attacks seem to not pose any
significant privacy risk as the adversary can have similar attack
success without access to the model.</p>
<h2 id="sensitive-value-inference">Sensitive Value Inference</h2>
<p>Attribute inference risk is inherently asymmetric &mdash; identifying a record with minority attribute value (such as <em>Hispanic</em> ethnicity) does not have the same risk as identifying a record with majority attribute value (such as <em>Non-Hispanic</em> ethnicity). Accuracy metric does not capture this. Moreover, attribute inference definition considered by prior works also fails to distinguish these cases. We propose studying a fine-grained version of attribute inference, called <em>sensitive value inference</em>, that considers the attack success in inferring a particular sensitive attribute outcome.</p>
<center>
<a href="/images/ai/svi.pdf"><img alt="Sensitive Value Inference" src="/images/ai/svi.png" width="60%"></a>
</center>
<p>We measure the attack success by evaluating the positive predictive value (PPV) of the inference attack in predicting the top-k candidate records with the sensitive outcome. The PPV values are between 0 and 1, where a higher value denotes a greater attack precision.</p>
<h2 id="the-neuron-output-attack">The Neuron Output Attack</h2>
<p>Our novel neuron output based white-box attack finds the neurons that are most correlated with the sensitive value. For this attack, the adversary selects records from a hold-out set, sets the unknown target attribute to the sensitive value, and queries the model. The adversary then identifies the set of neurons that have higher activations on average for the records with the sensitive value as the ground-truth. The adversary then uses the aggregate output of these neurons to identify the candidate records with sensitive value.</p>
<center>
<A href="/images/ai/wb.pdf"><img alt="" src="/images/ai/wb.png" width="55%"></a>
</center>
</br>
<h2 id="model-leaks-distribution-information">Model Leaks Distribution Information</h2>
<p>In our experiments, we vary the distribution available to the adversary and also the amount of data from the respective distribution the adversary has to train the inference attack. When the adversary has access to &gt;5000 records from the training distribition (not the same as the training set records), imputataion outperforms all the attribute inference attacks (incuding our white-box neuron output attack). As we decrease the known set size to 500 and 50, the imputation PPV decreases drastically whereas our neuron output attack continues to achieve high PPV. Thus the attack is able to take advantage of the training distribution information leaked by the model. The figure below depicts the case where the adversary has 500 records from the training distribution, and as shown, the neuron output attack surpasses the imputataion.</p>
<center>
<A href="/images/ai/img2.pdf"><img alt="" src="/images/ai/img2.png" width="80%"></a>
<div class="caption"><center>
Neurons correlated to Hispanic ethnicity for a neural network model trained on Texas-100X data set.
</center></div>
</center>
</br>
<p>We observe similar trend across different distribution settings and across different data sets. Detailed results can be found in the paper.</p>
<h2 id="differential-privacy-doesnt-mitigate-the-risk">Differential Privacy Doesn&rsquo;t Mitigate the Risk</h2>
<p>Prior works have claimed that attribute inference attacks cannot work in the cases where membership inference attacks do not succeed. Hence, some have thought that differential privacy mechanisms which successfully defend against membership inference attacks, also defend against attribute inference attacks. This is based on the <em>attribute advantage</em> metric of Yeom et al. that shows that the difference between the <em>accuracy</em> of inference attack across training and non-training set is bounded by differential privacy. We agree that this is true, as we shown in our experiment results in Table 2 below where the PPV of the neuron output attack is similar across both train and test sets. However, our <em>attribute advantage</em> metric measures the gap between the attack PPV when the adversary has access to the model (i.e., neuron output attack) versus when the adversary does not have model access (i.e., imputataion). As shown in the table below, this is not bounded by differential privacy as the neuron output attack PPV remains more or less the same with or without differential privacy.</p>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th></th>
<th style="text-align:center">Without DP</th>
<th style="text-align:center">With DP</th>
<th></th>
<th style="text-align:center">Train Set</th>
<th style="text-align:center">Test Set</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Imputation Attack</td>
<td></td>
<td style="text-align:center">0.62</td>
<td style="text-align:center">0.62</td>
<td></td>
<td style="text-align:center">0.62</td>
<td style="text-align:center">0.63</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">Neuron Output Attack</td>
<td></td>
<td style="text-align:center">0.49</td>
<td style="text-align:center">0.49</td>
<td></td>
<td style="text-align:center">0.49</td>
<td style="text-align:center">0.48</td>
<td></td>
</tr>
</tbody>
</table>
<center>Impact of Differential Privacy (DP) on the PPV of attacks (see table in paper for error margins). <br> Results show the PPV of attacks in predicting top-100 candidate records.</center>
</br>
<p>Since the risk is due to the model leaking distribution information, it is not mitigated by differential privacy noise.</p>
<h2 id="conclusion">Conclusion</h2>
<p>We show that the attribute inference attacks take advantage of the model leaking sensitive information about the underlying training distribution as opposed to leaking information about individual training records. While this is often considered by researchers to be <strong>not</strong> a privacy risk since the distribution statistics are supposed to be public knowledge, we argue that when the distribution itself is a private information then any such disclosure poses a severe privacy risk. Existing defenses, such as training the model with differential privacy mechanisms, does not mitigate this distribution privacy risk.</p>
<p><strong>Full paper:</strong> Bargav Jayaraman and David Evans. <a href="https://arxiv.org/abs/2209.01292"><em>Are Attribute Inference Attacks Just Imputation?</em></a> (<a href="https://arxiv.org/abs/2209.01292">arXiv</a>). In <a href="https://www.sigsac.org/ccs/CCS2022/"><em>ACM Conference on Computer and Communications Security</em></a> (CCS 2022).</p>
<p><strong>Code:</strong> <a href="https://github.com/bargavj/EvaluatingDPML"><em>https://github.com/bargavj/EvaluatingDPML</em></a></p>
<center><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/iLy0C5DK2T8?controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></center>
<p><strong>Talk Video:</strong> <a href="https://youtu.be/iLy0C5DK2T8"><em>https://youtu.be/iLy0C5DK2T8</em></a></p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/congratulations-dr.-jayaraman/">Congratulations, Dr. Jayaraman!</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-12-02 00:00:00 &#43;0000 UTC" itemprop="datePublished">2 December 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bargav-jayaraman">Bargav Jayaraman</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/phd-defense">PhD defense</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Congratulations to Bargav Jayaraman for successfully <a href="https://engineering.virginia.edu/events/phd-defense-presentation-bargav-jayaraman">defending his PhD thesis</a>!</p>
<center>
<img src="/images/bargav-defense.jpg" width="65%">
<div class="caption"><center>
Dr. Jayaraman and his PhD committee: Mohammad&nbsp;Mahmoody, Quanquan&nbsp;Gu (UCLA Department of Computer Science, on screen), Yanjun&nbsp;Qi (Committee Chair, on screen), Denis&nbsp;Nekipelov (Department of Economics, on screen), and David Evans
</div>
</center>
<h2 id="heading"></h2>
<p>Bargav will join the Meta AI Lab in Menlo Park, CA as a post-doctoral researcher.</p>
<h2 id="heading-1"></h2>
<center>
<em>Analyzing the Leaky Cauldron: Inference Attacks on Machine Learning</em>
</center>
<h2 id="heading-2"></h2>
<p>Machine learning models have been shown to leak sensitive information about their training data. An adversary having access to the model can infer different types of sensitive information, such as learning if a particular individual&rsquo;s data is in the training set, extracting sensitive patterns like passwords in the training set, or predicting missing sensitive attribute values for partially known training records. This dissertation quantifies this privacy leakage. We explore inference attacks against machine learning models including membership inference, pattern extraction, and attribute inference. While our attacks give an empirical lower bound on the privacy leakage, we also provide a theoretical upper bound on the privacy leakage metrics. Our experiments across various real-world data sets show that the membership inference attacks can infer a subset of candidate training records with high attack precision, even in challenging cases where the adversary&rsquo;s candidate set is mostly non-training records. In our pattern extraction experiments, we show that an adversary is able to recover email ids, passwords and login credentials from large transformer-based language models. Our attribute inference adversary is able to use underlying training distribution information inferred from the model to confidently identify candidate records with sensitive attribute values. We further evaluate the privacy risk implication to individuals contributing their data for model training. Our findings suggest that different subsets of individuals are vulnerable to different membership inference attacks, and that some individuals are repeatedly identified across multiple runs of an attack. For attribute inference, we find that a subset of candidate records with a sensitive attribute value are correctly predicted by our white-box attribute inference attacks but would be misclassified by an imputation attack that does not have access to the target model. We explore different defense strategies to mitigate the inference risks, including approaches that avoid model overfitting such as early stopping and differential privacy, and approaches that remove sensitive data from the training. We find that differential privacy mechanisms can thwart membership inference and pattern extraction attacks, but even differential privacy fails to mitigate the attribute inference risks since the attribute inference attack relies on the distribution information leaked by the model whereas differential privacy provides no protection against leakage of distribution statistics.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/balancing-tradeoffs-between-fickleness-and-obstinacy-in-nlp-models/">Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-11-13 00:00:00 &#43;0000 UTC" itemprop="datePublished">13 November 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/hannah-chen">Hannah Chen</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yangfeng-ji">Yangfeng Ji</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-examples">adversarial examples</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/nlp">NLP</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Post by <a href="https://hannahxchen.github.io/">Hannah Chen</a>.</p>
<p>Our work on balanced adversarial training looks at how to train models
that are robust to two different types of adversarial examples:</p>
<p><a href="https://hannahxchen.github.io/">Hannah Chen</a>, <a href="http://yangfengji.net/">Yangfeng
Ji</a>, <a href="http://www.cs.virginia.edu/~evans/">David
Evans</a>. <em>Balanced Adversarial
Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP
Models</em>. In <a href="https://2022.emnlp.org/"><em>The 2022 Conference on Empirical Methods in Natural
Language Processing</em></a> (EMNLP), Abu Dhabi,
7-11 December 2022.  [<a href="https://arxiv.org/abs/2210.11498">ArXiv</a>]</p>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/xQH51lIVDyY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>
<h2 id="adversarial-examples">Adversarial Examples</h2>
<p>At the broadest level, an adversarial example is an input crafted intentionally to confuse a model. However, most work focus on the defintion as an input constructed by applying a small perturbation that preserves the ground truth label but changes model&rsquo;s output <a href="https://arxiv.org/abs/1412.6572">(Goodfellow et al., 2015)</a>. We refer it as a <strong>fickle adversarial example</strong>. On the other hand, attackers can target an opposite objective where the inputs are made with minimal changes that change the ground truth labels but retain model&rsquo;s predictions <a href="https://arxiv.org/abs/1811.00401">(Jacobsen et al., 2018)</a>. We refer these malicious inputs as <strong>obstinate adversarial examples</strong>.</p>
<p>
    <center>
    <a href="/images/bat/image_AEs.png"><img src="/images/bat/image_AEs.png" width="60%" align="center"></a>
    <br>
    <em>Adversarial examples for images</em>
    </center>
</p>
<p>
    <center>
    <a href="/images/bat/nlp_AEs.png"><img src="/images/bat/nlp_AEs.png" width="50%" align="center"></a>
    <br>
    <em>Adversarial examples for texts (<span style="color:red;">Red</span>: synonym substitution, <span style="color:blue;">Blue</span>: antonym substitution)</em>
    </center>
</p>
<h2 id="distance-oracle-misalignment">Distance-Oracle Misalignment</h2>
<p>Previous work from <a href="https://arxiv.org/abs/2002.04599">(Tramer et al., 2020)</a> show that for image classification models, increasing robustness against fickle adversarial examples may also increase vulnerability to obstinate adversarial attacks. They suggested the reason behind this is may be the <em>distance-oracle misalignment</em> during fickle adversarial training. The norm bounded perturbation used for certified robust training may not align with the ground truth decision boundary. We hypothesize that this phenomenon may also exist in NLP models since the automatically-generated adversarial examples for NLP models can be imperfect sometimes, e.g., synonym word substitutions for constructing fickle adversarial examples may not preserve the ground truth label of the input.</p>
<center>
<a href="/images/bat/distance_misalignment.png"><img src="/images/bat/distance_misalignment.png" width="50%" align="center"></a>
</center>
<h2 id="robustness-tradeoffs">Robustness Tradeoffs</h2>
<p>To test our hypothsis, we perform obstinate adversarial attacks on models trained with normal training and fickle adversarial training. We use antonym word substitution for obstinate attack and SAFER <a href="https://arxiv.org/abs/2005.14424">(Ye et al., 2020)</a>, a certified robust training for NLP models, as the fickle adversarial defense. We visualize the antonym attack success rate on models trained with SAFER at each training epoch. We found that as the synonym attack success rate decreases over the course of training, the antonym attack success rate increases as well. The antonym attack success rate is also higher than the normal training baseline. This results prove our hypothesis that optimizing only fickle adversarial robustness can result in models being more vulnerable to obstinate adversarial examples.</p>
<center>
<a href="/images/bat/robustness-tradeoffs.png"><img src="/images/bat/robustness-tradeoffs.png" width="80%" align="center"></a>
</center>
<h2 id="balanced-adversarial-training-bat">Balanced Adversarial Training (BAT)</h2>
<p>We adapt constrastive learning by pairing fickle adversarial examples with the original examples as positive pairs and obstinate adversarial examples with the original examples as negative pairs. The goal of training is to minimize the distance between the postive pairs and maximize the distance between the negative pairs. We propose BAT-Pairwise and BAT-Triplet, where each combines a normal training objective with a pairwise or triplet loss.</p>
<center>
<a href="/images/bat/bat.png"><img src="/images/bat/bat.png" width="80%" align="center"></a>
</center>
<br>
<p>We evaluate BAT based on synonym (fickle) and antonym (obstinate) attack success rate and compare it with normal training, and two fickle adversarial defenses, A2T (vanilla adversarial training) <a href="https://arxiv.org/abs/2109.00544">(Yoo and Qi, 2021)</a> and SAFER (certified robust training). We show that both BAT-Pairwise and BAT-Triple result in better robustness against antonym attacks compared to other training baselines and are more robust against synonym attacks than the normal training method. While fickle adversarial defenses (A2T and SAFER) perform best when evaluated solely based on fickleness robustness, they have worse obstinacy robustness. Our proposed method gives a better balance between the two types of robustness.</p>
<center>
<a href="/images/bat/bat-results.png"><img src="/images/bat/bat-results.png" width="80%" align="center"></a>
</center>
<br>
<p>We compare the learned representations of models trained with BAT and other training baselines. We project the embeddings to 2 dimensional space with t-SNE. We see that boh fickle and obstinate examples are close to the original examples when the model is trained with normal training or SAFER. With BAT-Pairwise and BAT-Triplet, only the fickle examples and the original examples are close to each other while the obstinate examples are further away from them. This results match with BAT&rsquo;s training goal and show that BAT can mitigate the distance-oracle misalignment.</p>
<center>
<a href="/images/bat/tsne.png"><img src="/images/bat/tsne.png" width="80%" align="center"></a>
</center>
<h2 id="summary">Summary</h2>
<p>We show that robustness tradeoffs between ficklenss and obstinacy exist in NLP models. To counter this, we propose Balanced Adversarial Training (BAT) and show that it helps increase robustness against both fickle and obstinate adversarial examples.</p>
<p><b>Paper:</b> [<a href="https://arxiv.org/abs/2210.11498">ArXiv</a>]</p>
<p><b>Code:</b> <a href="https://github.com/hannahxchen/balanced-adversarial-training">https://github.com/hannahxchen/balanced-adversarial-training</a></p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/best-submission-award-at-visxai-2022/">Best Submission Award at VISxAI 2022</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-10-18 00:00:00 &#43;0000 UTC" itemprop="datePublished">18 October 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/evan-rose">Evan Rose</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-syua">Fnu Syua</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning-attacks">poisoning attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p><a href="https://uvasrg.github.io/poisoning/"><em>Poisoning Attacks and Subpopulation Susceptibility</em></a> by Evan Rose, Fnu Suya, and David Evans won the Best Submission Award at the <a href="https://visxai.io/">5th Workshop on Visualization for AI Explainability</a>.</p>
<p>Undergraduate student Evan Rose led the work and presented it at VISxAI in Oklahoma City, 17 October 2022.</p>
<center>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Congratulations to <a href="https://twitter.com/hashtag/VISxAI?src=hash&amp;ref_src=twsrc%5Etfw">#VISxAI</a>&#39;s Best Submission Awards:<br><br> K-Means Clustering: An Explorable Explainer by <a href="https://twitter.com/yizhe_ang?ref_src=twsrc%5Etfw">@yizhe_ang</a> <a href="https://t.co/BULW33WPzo">https://t.co/BULW33WPzo</a><br><br> Poisoning Attacks and Subpopulation Susceptibility by Evan Rose, <a href="https://twitter.com/suyafnu?ref_src=twsrc%5Etfw">@suyafnu</a>, and <a href="https://twitter.com/UdacityDave?ref_src=twsrc%5Etfw">@UdacityDave</a> <a href="https://t.co/Z12D3PvfXu">https://t.co/Z12D3PvfXu</a><a href="https://twitter.com/hashtag/ieeevis?src=hash&amp;ref_src=twsrc%5Etfw">#ieeevis</a></p>&mdash; VISxAI (@VISxAI) <a href="https://twitter.com/VISxAI/status/1582085676857577473?ref_src=twsrc%5Etfw">October 17, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Next up is best submission award  winner, &quot;Poisoning Attacks and Subpopulation Susceptibility&quot; by Evan Rose, <a href="https://twitter.com/suyafnu?ref_src=twsrc%5Etfw">@suyafnu</a>, and <a href="https://twitter.com/UdacityDave?ref_src=twsrc%5Etfw">@UdacityDave</a>.<br><br>Tune in to learn why some data subpopulations are more vulnerable to attacks than others!<a href="https://t.co/Z12D3PvfXu">https://t.co/Z12D3PvfXu</a><a href="https://twitter.com/hashtag/ieeevis?src=hash&amp;ref_src=twsrc%5Etfw">#ieeevis</a> <a href="https://twitter.com/hashtag/VISxAI?src=hash&amp;ref_src=twsrc%5Etfw">#VISxAI</a> <a href="https://t.co/Gm2JBpWQSP">pic.twitter.com/Gm2JBpWQSP</a></p>&mdash; VISxAI (@VISxAI) <a href="https://twitter.com/VISxAI/status/1582117943889969153?ref_src=twsrc%5Etfw">October 17, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>
      </div>
<hr class="post-separator"></hr>

    


    <footer>
      <nav>
	<a href="/post/" class="button hollow primary">All Posts</a>
      </nav>
    </footer>
   </div>
    </div>

    <div class="column small=7 medium-3">
    <div class="sidebar">
<center>                  <img src="/images/srg-logo-scaled.png" width=200 height=200 alt="SRG Logo">
      University of Virginia <br>
Security Research Group
</center>

</p>
   <p>
   <a href="/team"><b>Team</b></a></br>
   <a href="//www.cs.virginia.edu/evans/pubs"><b>Publications</b></a><br>
      <a href="/videos"><b>Videos</b></a><br>
   </p>
<p class="nogap">
     <p>
   <a href="https://teams.microsoft.com/l/team/19%3aWdkw2xYq6taXh-0OftqQdt8SQ2vyvUI_Z0ZL39APghY1%40thread.tacv2/conversations?groupId=58076b41-c835-4a07-abaa-705bc7cca101&tenantId=7b3480c7-3707-4873-8b77-e216733a65ac"><b>Join Teams Group</b></a>
   </p>

  <a href="/studygroups/"><b>Study Groups</b></a>

  

<p class="nogap"></p>
  <p>
   <b><a href="/post/">Recent News</a></b>
   </p>
   
   <div class="posttitle">
      <a href="/cray-distinguished-speaker-on-leaky-models-and-unintended-inferences/">Cray Distinguished Speaker: On Leaky Models and Unintended Inferences</a>


   </div>
   
   <div class="posttitle">
      <a href="/attribute-inference-attacks-are-really-imputation/">Attribute Inference attacks are really Imputation</a>


   </div>
   
   <div class="posttitle">
      <a href="/congratulations-dr.-jayaraman/">Congratulations, Dr. Jayaraman!</a>


   </div>
   
   <div class="posttitle">
      <a href="/balancing-tradeoffs-between-fickleness-and-obstinacy-in-nlp-models/">Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models</a>


   </div>
   
   <div class="posttitle">
      <a href="/best-submission-award-at-visxai-2022/">Best Submission Award at VISxAI 2022</a>


   </div>
   
   <div class="posttitle">
      <a href="/visualizing-poisoning/">Visualizing Poisoning</a>


   </div>
   
   <div class="posttitle">
      <a href="/congratulations-dr.-zhang/">Congratulations, Dr. Zhang!</a>


   </div>
   
   <div class="posttitle">
      <a href="/biml-what-machine-learnt-models-reveal/">BIML: What Machine Learnt Models Reveal</a>


   </div>
   
   <div class="posttitle">
      <a href="/iclr-2022-understanding-intrinsic-robustness-using-label-uncertainty/">ICLR 2022: Understanding Intrinsic Robustness Using Label Uncertainty</a>


   </div>
   
   <div class="posttitle">
      <a href="/microsoft-research-summit-surprising-and-unsurprising-inference-risks-in-machine-learning/">Microsoft Research Summit: Surprising (and unsurprising) Inference Risks in Machine Learning</a>


   </div>
   
   <div class="posttitle">
      <a href="/uva-news-article/">UVA News Article</a>


   </div>
   
   <div class="posttitle">
      <a href="/model-targeted-poisoning-attacks-with-provable-convergence/">Model-Targeted Poisoning Attacks with Provable Convergence</a>


   </div>
   
   <div class="posttitle">
      <a href="/on-the-risks-of-distribution-inference/">On the Risks of Distribution Inference</a>


   </div>
   
   <div class="posttitle">
      <a href="/chinese-translation-of-mpc-book/">Chinese Translation of MPC Book</a>


   </div>
   
   <div class="posttitle">
      <a href="/iclr-dpml-2021-inference-risks-for-machine-learning/">ICLR DPML 2021: Inference Risks for Machine Learning</a>


   </div>
   
   <div class="posttitle">
      <a href="/how-to-hide-a-backdoor/">How to Hide a Backdoor</a>


   </div>
   
   <div class="posttitle">
      <a href="/codaspy-2021-keynote-when-models-learn-too-much/">Codaspy 2021 Keynote: When Models Learn Too Much</a>


   </div>
   
   <div class="posttitle">
      <a href="/crysp-talk-when-models-learn-too-much/">CrySP Talk: When Models Learn Too Much</a>


   </div>
   
   <div class="posttitle">
      <a href="/improved-estimation-of-concentration-iclr-2021/">Improved Estimation of Concentration (ICLR 2021)</a>


   </div>
   
   <div class="posttitle">
      <a href="/virginia-consumer-data-protection-act/">Virginia Consumer Data Protection Act</a>


   </div>
   
   <div class="posttitle">
      <a href="/algorithmic-accountability-and-the-law/">Algorithmic Accountability and the Law</a>


   </div>
   
   <div class="posttitle">
      <a href="/microsoft-security-data-science-colloquium-inference-privacy-in-theory-and-practice/">Microsoft Security Data Science Colloquium: Inference Privacy in Theory and Practice</a>


   </div>
   
   <div class="posttitle">
      <a href="/fact-checking-donald-trumps-tweet-firing-christopher-krebs/">Fact-checking Donald Trumps tweet firing Christopher Krebs</a>


   </div>
   
   <div class="posttitle">
      <a href="/voting-security/">Voting Security</a>


   </div>
   
   <div class="posttitle">
      <a href="/merlin-morgan-and-the-importance-of-thresholds-and-priors/">Merlin, Morgan, and the Importance of Thresholds and Priors</a>


   </div>
   
<p></p>
   <div class="posttitle"><a href="/post/">Older Posts</a></div>
  <div class="posttitle"><a href="/tags">Posts by Tag</a></div>
  <div class="posttitle"><a href="/categories/">Posts by Category</a></div>
  <div class="posttitle"><a href="2017.html">Old Blog</a></div>
<p></p>
<p>
<a href="/awards/"><b>Awards</b></a>
</p>

   <p>
Director: <a href="//www.cs.virginia.edu/evans">David Evans</a>
   </p>

<p>

  </p>

<p><br></br></p>

   <p>
     <center>
       <img src="/images/uva_primary_rgb_white.png" width="80%">
       </center>
</p>

    </div>
</div>

   </div>


    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
