<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Posts | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
        <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
	      
	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      

<div class="container">
<div class="content">
<div class="row">
    
    
    <h2><a href="/sok-pitfalls-in-evaluating-black-box-attacks/">SoK: Pitfalls in Evaluating Black-Box Attacks</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-12-20 00:00:00 &#43;0000 UTC" itemprop="datePublished">20 December 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/tingwei-zhang">Tingwei Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jingtao-hong">Jingtao Hong</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yuan-tian">Yuan Tian</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/satml">SaTML</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/black-box-adversarial-attacks">black-box adversarial attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/systemization-of-knowledge">systemization of knowledge</a>
    
  </span>
  
  
</div>


<p>Post by <strong><a href="https://www.anshumansuri.com/">Anshuman Suri</a></strong> and <strong><a href="https://fsuya.org/">Fnu Suya</a></strong></p>
<p>Much research has studied black-box attacks on image classifiers,
where adversaries generate adversarial examples against unknown target
models without having access to their internal information. Our
analysis of over 164 attacks (published in 102 major security, machine
learning and security conferences) shows how these works make
different assumptions about the adversary’s knowledge.</p>
<p>The current literature lacks cohesive organization centered around the
threat model. Our <a href="https://arxiv.org/abs/2310.17534">SoK paper</a> (to
appear at <a href="https://satml.org/">IEEE SaTML 2024</a>) introduces a taxonomy
for systematizing these attacks and demonstrates the importance of
careful evaluations that consider adversary resources and threat
models.</p>
<p class="text-right"><a href="/sok-pitfalls-in-evaluating-black-box-attacks/">Read More…</a></p>
	

    
    <h2><a href="/neurips-2023-what-distributions-are-robust-to-poisoning-attacks/">NeurIPS 2023: What Distributions are Robust to Poisoning Attacks?</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-12-07 00:00:00 &#43;0000 UTC" itemprop="datePublished">7 December 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/indiscriminate-poisoning-attacks">indiscriminate poisoning attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/susceptibility-variation">susceptibility variation</a>
    
  </span>
  
  
</div>


<p>Post by <strong><a href="https://fsuya.org/">Fnu Suya</a></strong></p>
<p>Data poisoning attacks are recognized as a top concern in the industry <a href="https://arxiv.org/abs/2002.05646">[1]</a>. We focus on conventional indiscriminate data poisoning attacks, where an adversary injects a few crafted examples into the training data with the goal of increasing the test error of the induced model. Despite recent advances, indiscriminate poisoning attacks on large neural networks remain challenging <a href="https://arxiv.org/abs/2303.03592">[2]</a>. In this work (to be presented at NeurIPS 2023), we revisit the vulnerabilities of more extensively studied linear models under indiscriminate poisoning attacks.</p>
<p class="text-right"><a href="/neurips-2023-what-distributions-are-robust-to-poisoning-attacks/">Read More…</a></p>
	

    
    <h2><a href="/adjectives-can-reveal-gender-biases-within-nlp-models/">Adjectives Can Reveal Gender Biases Within NLP Models</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-08-17 00:00:00 &#43;0000 UTC" itemprop="datePublished">17 August 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jason-briegel">Jason Briegel</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/hannah-chen">Hannah Chen</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/llms">LLMs</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/generative-ai">generative AI</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bias">bias</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/machine-learning">machine learning</a>
    
  </span>
  
  
</div>


<p>Post by <strong>Jason Briegel</strong> and <a href="https://hannahxchen.github.io/"><strong>Hannah Chen</strong></a></p>
<p>Because NLP models are trained with human corpora (and now,
increasingly on text generated by other NLP models that were
originally trained on human language), they are prone to inheriting
common human stereotypes and biases. This is problematic, because with
their growing prominence they may further propagate these stereotypes
<a href="https://arxiv.org/abs/1906.08976">(Sun et al., 2019)</a>. For example,
interest is growing in mitigating bias in the field of machine
translation, where systems such as Google translate were observed to
default to translating gender-neutral pronouns as male pronouns, even
with feminine cues <a href="https://doi.org/10.1162/tacl_a_00401">(Savoldi et al.,
2021)</a>.</p>
<p class="text-right"><a href="/adjectives-can-reveal-gender-biases-within-nlp-models/">Read More…</a></p>
	

    
    <h2><a href="/congratulations-dr.-suya/">Congratulations, Dr. Suya!</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-07-11 00:00:00 &#43;0000 UTC" itemprop="datePublished">11 July 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning">poisoning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/phd-defense">PhD defense</a>
    
  </span>
  
  
</div>


<p>Congratulations to <a href="fsuya.org">Fnu Suya</a> for successfully defending
his PhD thesis!</p>
<p>Suya will join the Unversity of Maryland as a MC2 Postdoctoral Fellow
at the <a href="https://cyber.umd.edu/about">Maryland Cybersecurity Center</a> this fall.</p>
<h2 id="heading"></h2>
<center>
<em>
On the Limits of Data Poisoning Attacks
</em>
</center>
<h2 id="heading-1"></h2>
<p>Current machine learning models require large amounts of labeled training data, which are often collected from untrusted sources. Models trained on these potentially manipulated data points are prone to data poisoning attacks. My research aims to gain a deeper understanding on the limits of two types of data poisoning attacks: indiscriminate poisoning attacks, where the attacker aims to increase the test error on the entire dataset; and subpopulation poisoning attacks, where the attacker aims to increase the test error on a defined subset of the distribution. We first present an empirical poisoning attack that encodes the attack objectives into target models and then generates poisoning points that induce the target models (and hence the encoded objectives) with provable convergence. This attack achieves state-of-the-art performance for a diverse set of attack objectives and quantifies a lower bound to the performance of best possible poisoning attacks. In the broader sense, because the attack guarantees convergence to the target model which encodes the desired attack objective, our attack can also be applied to objectives related to other trustworthy aspects (e.g., privacy, fairness) of machine learning.</p>
<p class="text-right"><a href="/congratulations-dr.-suya/">Read More…</a></p>
	

    
    <h2><a href="/sok-let-the-privacy-games-begin-a-unified-treatment-of-data-inference-privacy-in-machine-learning/">SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-05-26 00:00:00 &#43;0000 UTC" itemprop="datePublished">26 May 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/ahmed-salem">Ahmed Salem</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/giovanni-cherubin">Giovanni Cherubin</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/boris-k%c3%b6pf">Boris Köpf</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/andrew-paverd">Andrew Paverd</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/shruti-tople">Shruti Tople</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/santiago-zanella-b%c3%a9guelin">Santiago Zanella-Béguelin</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/membership-inference">membership inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>
    
  </span>
  
  
</div>


<p>Our paper on the use of cryptographic-style games to model inference privacy is published in <a href="https://www.ieee-security.org/TC/SP2023/"><em>IEEE Symposium on Security and Privacy</em></a> (Oakland):</p>
<blockquote>
<a href="https://www.microsoft.com/en-us/research/people/t-salemahmed/>Ahmed Salem</a>, <a href="https://www.microsoft.com/en-us/research/people/gcherubin/">Giovanni Cherubin</a>, <a href="https://www.cs.virginia.edu/evans"/David Evans</a>, <a href="https://www.microsoft.com/en-us/research/people/bokoepf/">Boris Köpf</a>, <a href="https://www.microsoft.com/en-us/research/people/anpaverd/">Andrew Paverd</a>, <a href="https://www.anshumansuri.com/">Anshuman Suri</a>, <a href="https://www.microsoft.com/en-us/research/people/shtople/">Shruti Tople</a>, and <a href="https://www.microsoft.com/en-us/research/people/santiago/">Santiago Zanella-Béguelin</a>. <em>SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning</em>. IEEE Symposium on Security and Privacy, 2023. [<a href="https://arxiv.org/abs/2212.10986">Arxiv</a>]
</blockquote>
<h2 id="heading"></h2>
<center>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Tired of diverse definitions of machine learning privacy risks? Curious about game-based definitions? In our paper, we present privacy games as a tool for describing and analyzing privacy risks in machine learning. Join us on May 22nd, 11 AM <a href="https://twitter.com/IEEESSP?ref_src=twsrc%5Etfw">@IEEESSP</a> &#39;23 <a href="https://t.co/NbRuTmHyd2">https://t.co/NbRuTmHyd2</a> <a href="https://t.co/CIzsT7UY4b">pic.twitter.com/CIzsT7UY4b</a></p>
<p class="text-right"><a href="/sok-let-the-privacy-games-begin-a-unified-treatment-of-data-inference-privacy-in-machine-learning/">Read More…</a></p>
	

    
    <h2><a href="/cvpr-2023-manipulating-transfer-learning-for-property-inference/">CVPR 2023: Manipulating Transfer Learning for Property Inference</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-05-02 00:00:00 &#43;0000 UTC" itemprop="datePublished">2 May 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yulong-tian">Yulong Tian</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/transfer-learning">transfer learning</a>
    
  </span>
  
  
</div>


<h1 id="manipulating-transfer-learning-for-property-inference">Manipulating Transfer Learning for Property Inference</h1>
<p>Transfer learning is a popular method to train deep learning models
efficiently. By reusing parameters from upstream pre-trained models,
the downstream trainer can use fewer computing resources to train
downstream models, compared to training models from scratch.</p>
<p>The figure below shows the typical process of transfer learning for
vision tasks:</p>
<center>
<a href="/images/mtlpi/fig1.png"><img src="/images/mtlpi/fig1.png" width="80%"></a>
</center>
<p>However, the nature of transfer learning can be exploited by a
malicious upstream trainer, leading to severe risks to the downstream
trainer.</p>
<p class="text-right"><a href="/cvpr-2023-manipulating-transfer-learning-for-property-inference/">Read More…</a></p>
	

    
    <h2><a href="/mico-challenge-in-membership-inference/">MICO Challenge in Membership Inference</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-02-10 00:00:00 &#43;0000 UTC" itemprop="datePublished">10 February 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/membership-inference">membership inference</a>
    
  </span>
  
  
</div>


<p><a href="https://www.anshumansuri.com">Anshuman Suri</a> wrote up an interesting
post on his experience with the <a href="https://github.com/microsoft/MICO">MICO
Challenge</a>, a membership inference
competition that was part of <a href="https://satml.org/">SaTML</a>. Anshuman
placed second in the competition (on the CIFAR data set), where the
metric is highest true positive rate at a 0.1 false positive rate over
a set of models (some trained using differential privacy and some
without).</p>
<p>Anshuman&rsquo;s post describes the methods he used and his experience in
the competition: <a href="https://www.anshumansuri.com/post/mico/"><em>My submission to the MICO
Challenge</em></a>.</p>
<p class="text-right"><a href="/mico-challenge-in-membership-inference/">Read More…</a></p>
	

    
    <h2><a href="/voice-of-america-interview-on-chatgpt/">Voice of America interview on ChatGPT</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-02-10 00:00:00 &#43;0000 UTC" itemprop="datePublished">10 February 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/large-language-models">large language models</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/nlp">nlp</a>
    
  </span>
  
  
</div>


<p>I was interviewed for a Voice of America story (in Russian) on the impact of chatGPT and similar tools.</p>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/dFuunAFX9y4?start=319" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</center>
<p>Full story: <a href="https://youtu.be/dFuunAFX9y4">https://youtu.be/dFuunAFX9y4</a></p>

	

    
    <h2><a href="/uh-oh-theres-a-new-way-to-poison-code-models/">Uh-oh, there&#39;s a new way to poison code models</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-01-16 00:00:00 &#43;0000 UTC" itemprop="datePublished">16 January 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/large-language-models">large language models</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/copilot">copilot</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning">poisoning</a>
    
  </span>
  
  
</div>


<p>Jack Clark&rsquo;s <a href="https://mailchi.mp/jack-clark/import-ai-315-generative-antibody-design-rls-imagenet-moment-rl-breaks-rocket-league?e=545365c0e9">Import AI, 16 Jan 2023</a> includes a nice description of our work on TrojanPuzzle:</p>
<p style="margin: 10px 0;padding: 0;mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #202020;font-family: Helvetica;font-size: 16px;line-height: 150%;text-align: left;"><strong>####################################################</strong></p>
<!-- /wp:paragraph --><!-- wp:paragraph -->
<p style="margin: 10px 0;padding: 0;mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #202020;font-family: Helvetica;font-size: 16px;line-height: 150%;text-align: left;"><strong>Uh-oh, there's a new way to poison code models - and it's really hard to detect:</strong><br>
<em>…TROJANPUZZLE is a clever way to trick your code model into betraying you - if you can poison the undelrying dataset…</em><br>
Researchers with the University of California, Santa Barbara, Microsoft Corporation, and the University of Virginia have come up with some clever, subtle ways to poison the datasets used to train code models. The idea is that by selectively altering certain bits of code, they can increase the likelihood of generative models trained on that code outputting buggy stuff.&nbsp;</p>
<p class="text-right"><a href="/uh-oh-theres-a-new-way-to-poison-code-models/">Read More…</a></p>
	

    
    <h2><a href="/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/">Trojan Puzzle attack trains AI assistants into suggesting malicious code</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-01-10 00:00:00 &#43;0000 UTC" itemprop="datePublished">10 January 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/large-language-models">large language models</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/copilot">copilot</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning">poisoning</a>
    
  </span>
  
  
</div>


<p>Bleeping Computer has a <a href="https://www.bleepingcomputer.com/news/security/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/">story on our work</a> (in collaboration with Microsoft Research) on poisoning code suggestion models:</p>
<h1>Trojan Puzzle attack trains AI assistants into suggesting malicious code</h1>
<p>By <b>Bill Toulas</b></p>
<center>
<img alt="Person made of jigsaw puzzle pieces" height="900" src="https://www.bleepstatic.com/content/hl-images/2022/10/09/mystery-hacker.jpg" width="80%"></img>
</center>
<p> </p>
<p>Researchers at the universities of California, Virginia, and Microsoft have devised a new poisoning attack that could trick AI-based coding assistants into suggesting dangerous code.</p>
<p>Named &lsquo;Trojan Puzzle,&rsquo; the attack stands out for bypassing static detection and signature-based dataset cleansing models, resulting in the AI models being trained to learn how to reproduce dangerous payloads.</p>
<p class="text-right"><a href="/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/">Read More…</a></p>
	

    
    <div class="row">
  <div class="column small-12">
    <ul class="pagination" role="navigation" aria-label="Pagination">
      
      <li class="arrow" aria-disabled="true"><a href="/post/page/4/">&laquo; <em>Older<span class="show-for-sr"> blog entries</span></em></a></li>
      
      <li><span>Page 3 of 11</span></li>      
      
      <li class="arrow" aria-disabled="true"><a href="/post/page/2/"><em>Newer<span class="show-for-sr"> blog entries</span></em>:&nbsp;&raquo;</a></li>
      
    </ul>    
    All Posts by <a href="//uvasrg.github.io/categories">Category</a> or <a href="//uvasrg.github.io/tags">Tags</a>.

  </div>
</div>

</div>
</div>
</div>

    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
