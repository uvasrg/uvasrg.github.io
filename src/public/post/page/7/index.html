<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Posts | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
        <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
	      
	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      

<div class="container">
<div class="content">
<div class="row">
    
    
    <h2><a href="/fosad2019/">FOSAD Trustworthy Machine Learning Mini-Course</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-08-28 00:00:00 &#43;0000 UTC" itemprop="datePublished">28 August 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>
    
  </span>
  
  
</div>


I taught a mini-course on Trustworthy Machine Learning at the 19th International School on Foundations of Security Analysis and Design in Bertinoro, Italy.
Slides from my three (two-hour) lectures are posted below, along with some links to relevant papers and resources.
Class 1: Introduction/Attacks The PDF malware evasion attack is described in this paper:
Weilin Xu, Yanjun Qi, and David Evans. Automatically Evading Classifiers: A Case Study on PDF Malware Classifiers.
<p class="text-right"><a href="/fosad2019/">Read More…</a></p>
	

    
    <h2><a href="/evaluating-differentially-private-machine-learning-in-practice/">Evaluating Differentially Private Machine Learning in Practice</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-08-27 00:00:00 &#43;0000 UTC" itemprop="datePublished">27 August 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bargav-jayaraman">Bargav Jayaraman</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/differential-privacy">differential privacy</a>
    
  </span>
  
  
</div>


(Cross-post by Bargav Jayaraman)
With the recent advances in composition of differential private mechanisms, the research community has been able to achieve meaningful deep learning with privacy budgets in single digits. Rènyi differential privacy (RDP) is one mechanism that provides tighter composition which is widely used because of its implementation in TensorFlow Privacy (recently, Gaussian differential privacy (GDP) has shown a tighter analysis for low privacy budgets, but it was not yet available when we did this work).
<p class="text-right"><a href="/evaluating-differentially-private-machine-learning-in-practice/">Read More…</a></p>
	

    
    <h2><a href="/usenix-security-symposium-2019/">USENIX Security Symposium 2019</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-08-26 00:00:00 &#43;0000 UTC" itemprop="datePublished">26 August 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bargav-jayaraman">Bargav Jayaraman</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/sam-havron">Sam Havron</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/serge-egelman">Serge Egelman</a>
    
  </span>
  
  
</div>


Bargav Jayaraman presented our paper on Evaluating Differentially Private Machine Learning in Practice at the 28th USENIX Security Symposium in Santa Clara, California.
Summary by Lea Kissner:
Hey it&#39;s the results! pic.twitter.com/ru1FbkESho
&mdash; Lea Kissner (@LeaKissner) August 17, 2019 Also, great to see several UVA folks at the conference including:
Sam Havron (BSCS 2017, now a PhD student at Cornell) presented a paper on the work he and his colleagues have done on computer security for victims of intimate partner violence.
<p class="text-right"><a href="/usenix-security-symposium-2019/">Read More…</a></p>
	

    
    <h2><a href="/google-security-and-privacy-workshop/">Google Security and Privacy Workshop</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-08-25 00:00:00 &#43;0000 UTC" itemprop="datePublished">25 August 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/google">Google</a>
    
  </span>
  
  
</div>


I presented a short talk at a workshop at Google on Adversarial ML: Closing Gaps between Theory and Practice (mostly fun for the movie of me trying to solve Google&rsquo;s CAPTCHA on the last slide):
Getting the actual screencast to fit into the limited time for this talk challenged the limits of my video editing skills.
I can say with some confidence, Google does donuts much better than they do cookies!
<p class="text-right"><a href="/google-security-and-privacy-workshop/">Read More…</a></p>
	

    
    <h2><a href="/brink-essay-ai-systems-are-complex-and-fragile.-here-are-four-key-risks-to-understand./">Brink Essay: AI Systems Are Complex and Fragile. Here Are Four Key Risks to Understand.</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-07-09 00:00:00 &#43;0000 UTC" itemprop="datePublished">9 July 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fairness">fairness</a>
    
  </span>
  
  
</div>


Brink News (a publication of The Atlantic) published my essay on the risks of deploying AI systems.
Artificial intelligence technologies have the potential to transform society in positive and powerful ways. Recent studies have shown computing systems that can outperform humans at numerous once-challenging tasks, ranging from performing medical diagnoses and reviewing legal contracts to playing Go and recognizing human emotions. Despite these successes, AI systems are fundamentally fragile — and the ways they can fail are poorly understood.
<p class="text-right"><a href="/brink-essay-ai-systems-are-complex-and-fragile.-here-are-four-key-risks-to-understand./">Read More…</a></p>
	

    
    <h2><a href="/google-federated-privacy-2019-the-dragon-in-the-room/">Google Federated Privacy 2019: The Dragon in the Room</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-06-22 00:00:00 &#43;0000 UTC" itemprop="datePublished">22 June 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/google">Google</a>
    
  </span>
  
  
</div>


I&rsquo;m back from a very interesting Workshop on Federated Learning and Analytics that was organized by Peter Kairouz and Brendan McMahan from Google&rsquo;s federated learning team and was held at Google Seattle.
For the first part of my talk, I covered Bargav&rsquo;s work on evaluating differentially private machine learning, but I reserved the last few minutes of my talk to address the cognitive dissonance I felt being at a Google meeting on privacy.
<p class="text-right"><a href="/google-federated-privacy-2019-the-dragon-in-the-room/">Read More…</a></p>
	

    
    <h2><a href="/graduation-2019/">Graduation 2019</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-06-05 00:00:00 &#43;0000 UTC" itemprop="datePublished">5 June 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/weilin-xu">Weilin Xu</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yanjun-qi">Yanjun Qi</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/alumni">alumni</a>
    
  </span>
  
  
</div>


<center>
<a href="/images/graduation2019/IMG_0171.jpg"><img src="/images/graduation2019/IMG_0171-2.jpg" height=120></a>
</center>
<table align="center" width="60%">
<tr>
<td>
<a href="/images/graduation2019/IMG_0116.jpg"><img src="/images/graduation2019/IMG_0116-2.jpg" height="100"></a>
</td>
<td>
<a href="/images/graduation2019/IMG-0175.jpg"><img src="/images/graduation2019/IMG_0175-2.jpg" height="100"></a>
</td>
</tr>
</table>
<center>
<a href="/images/graduation2019/IMG_0193.jpg"><img src="/images/graduation2019/IMG_0193-2.jpg" height=120></a>
</center>

	

    
    <h2><a href="/how-ai-could-save-lives-without-spilling-medical-secrets/">How AI could save lives without spilling medical secrets</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-05-14 00:00:00 &#43;0000 UTC" itemprop="datePublished">14 May 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/multi-party-computation">multi-party computation</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/differential-privacy">differential privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/news">news</a>
    
  </span>
  
  
</div>


I&rsquo;m quoted in this article by Will Knight focused on the work Oasis Labs (Dawn Song&rsquo;s company) is doing on privacy-preserving medical data analysis: How AI could save lives without spilling medical secrets, MIT Technology Review, 14 May 2019.
&ldquo;The whole notion of doing computation while keeping data secret is an incredibly powerful one,&rdquo; says David Evans, who specializes in machine learning and security at the University of Virginia. When applied across hospitals and patient populations, for instance, machine learning might unlock completely new ways of tying disease to genomics, test results, and other patient information.
<p class="text-right"><a href="/how-ai-could-save-lives-without-spilling-medical-secrets/">Read More…</a></p>
	

    
    <h2><a href="/cost-sensitive-adversarial-robustness-at-iclr-2019/">Cost-Sensitive Adversarial Robustness at ICLR 2019</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-05-06 00:00:00 &#43;0000 UTC" itemprop="datePublished">6 May 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/iclr">ICLR</a>
    
  </span>
  
  
</div>


<p>Xiao Zhang will present <a href="https://openreview.net/forum?id=BygANhA9tQ&amp;noteId=BJe7cKRWeN"><em>Cost-Sensitive Robustness against Adversarial Examples</em></a> on May 7 (4:30-6:30pm) at <a href="https://iclr.cc/Conferences/2019/">ICLR 2019 in New Orleans.</p>
<center>
<a href="/docs/cost-sensitive-poster.pdf"><img src="/docs/cost-sensitive-poster-small.png" width="90%" align="center"></a>
</center>
<p>Paper: <a href="https://evademl.org/docs/cost-sensitive-robustness.pdf">[PDF]</a> [<a href="https://openreview.net/forum?id=BygANhA9tQ&amp;noteId=BJe7cKRWeN">OpenReview</a>] [<a href="https://arxiv.org/abs/1810.09225">ArXiv</a>]</p>

	

    
    <h2><a href="/empirically-measuring-concentration/">Empirically Measuring Concentration</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-05-05 00:00:00 &#43;0000 UTC" itemprop="datePublished">5 May 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/iclr">ICLR</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/saeed-mahloujifar">Saeed Mahloujifar</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/mohammad-mahmoody">Mohammad Mahmoody</a>
    
  </span>
  
  
</div>


<p>Xiao Zhang and Saeed Mahloujifar will present our work on <em>Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness</em> at two workshops May 6 at ICLR 2019 in New Orleans: <a href="https://debug-ml-iclr2019.github.io/"><em>Debugging Machine Learning Models</em></a> and <a href="https://sites.google.com/view/safeml-iclr2019"><em>Safe Machine Learning:
Specification, Robustness and Assurance</em></a>.</p>
<p>Paper: <a href="/docs/concentration-robustness.pdf">[PDF]</a></p>
<center>
<a href="/docs/concentration-robustness-poster.pdf"><img src="/docs/concentration-robustness-poster-small.png" width="90%" align="center"></a>
</center>

	

    
    <div class="row">
  <div class="column small-12">
    <ul class="pagination" role="navigation" aria-label="Pagination">
      
      <li class="arrow" aria-disabled="true"><a href="/post/page/8/">&laquo; <em>Older<span class="show-for-sr"> blog entries</span></em></a></li>
      
      <li><span>Page 7 of 10</span></li>      
      
      <li class="arrow" aria-disabled="true"><a href="/post/page/6/"><em>Newer<span class="show-for-sr"> blog entries</span></em>:&nbsp;&raquo;</a></li>
      
    </ul>    
    All Posts by <a href="//uvasrg.github.io/categories">Category</a> or <a href="//uvasrg.github.io/tags">Tags</a>.

  </div>
</div>

</div>
</div>
</div>

    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
