<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Presentations on Security Research Group</title>
    <link>//uvasrg.github.io/categories/presentations/</link>
    <description>Recent content in Presentations on Security Research Group</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Fri, 14 Aug 2020 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/categories/presentations/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Adversarially Robust Representations</title>
      <link>//uvasrg.github.io/robustrepresentations/</link>
      <pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/robustrepresentations/</guid>
      <description>&lt;p&gt;&lt;em&gt;Post by Sicheng Zhu&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;With the rapid development of deep learning and the explosive growth&#xA;of unlabeled data, &lt;a href=&#34;https://arxiv.org/abs/1206.5538&#34;&gt;representation&#xA;learning&lt;/a&gt; is becoming increasingly&#xA;important. It has made impressive applications such as pre-trained&#xA;language models (e.g., &lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;BERT&lt;/a&gt; and&#xA;&lt;a href=&#34;https://github.com/openai/gpt-3&#34;&gt;GPT-3&lt;/a&gt;).&lt;/p&gt;&#xA;&lt;p&gt;Popular as it is, representation learning raises concerns about the&#xA;robustness of learned representations under adversarial settings. For&#xA;example, &lt;em&gt;how can we compare the robustness to different&#xA;representations&lt;/em&gt;, and &lt;em&gt;how can we build representations that enable&#xA;robust downstream classifiers&lt;/em&gt;?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Intrinsic Robustness using Conditional GANs</title>
      <link>//uvasrg.github.io/intrinsic-robustness-using-conditional-gans/</link>
      <pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/intrinsic-robustness-using-conditional-gans/</guid>
      <description>&lt;p&gt;The video of Xiao&amp;rsquo;s presentation for AISTATS 2020 is now available:&#xA;&lt;a href=&#34;https://slideslive.com/38930305/understanding-the-intrinsic-robustness-of-image-distributions-using-conditional-generative-models&#34;&gt;&lt;em&gt;Understanding the Intrinsic Robustness of Image Distributions using Conditional Generative Models&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Starting with Gilmer et al. (2018), several works have demonstrated&#xA;the inevitability of adversarial examples based on different&#xA;assumptions about the underlying input probability space. It remains&#xA;unclear, however, whether these results apply to natural image&#xA;distributions. In this work, we assume the underlying data&#xA;distribution is captured by some conditional generative model, and&#xA;prove intrinsic robustness bounds for a general class of classifiers,&#xA;which solves an open problem in Fawzi et al. (2018). Building upon the&#xA;state-of-the-art conditional generative models, we study the intrinsic&#xA;robustness of two common image benchmarks under &lt;em&gt;l&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;&#xA;perturbations, and show the existence of a large gap between the&#xA;robustness limits implied by our theory and the adversarial robustness&#xA;achieved by current state-of-the-art robust models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hybrid Batch Attacks at USENIX Security 2020</title>
      <link>//uvasrg.github.io/hybrid-batch-attacks-at-usenix-security-2020/</link>
      <pubDate>Thu, 13 Aug 2020 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/hybrid-batch-attacks-at-usenix-security-2020/</guid>
      <description>&lt;p&gt;Here&amp;rsquo;s the video for Suya&amp;rsquo;s presentation on &lt;a href=&#34;//uvasrg.github.io/usenix-security-2020-hybrid-batch-attacks&#34;&gt;Hybrid Batch Attacks&lt;/a&gt; at USENIX Security 2020:&lt;/p&gt;&#xA;&lt;center&gt;&#xA;  &lt;video width=&#34;90%&#34; id=&#34;usenix-media-video-1&#34; data-setup=&#34;{}&#34; poster=&#34;&#34; class=&#34;video-js vjs-default-skin vjs-big-play-centered&#34; preload=&#34;auto&#34; controls&gt;&#xA;    &lt;source src=&#39;https://2459d6dc103cb5933875-c0245c5c937c5dedcca3f1764ecc9b2f.ssl.cf2.rackcdn.com/sec20/videos/0813/s5_machine_learning_1/3_sec20summer-paper412-presentation-video.mp4&#39; type=&#39;video/mp4; codecs=&#34;avc1.42E01E, mp4a.40.2&#34;&#39;&gt;&#xA;  &lt;/video&gt;&lt;br&gt; &#xA;&lt;a href=&#34;https://2459d6dc103cb5933875-c0245c5c937c5dedcca3f1764ecc9b2f.ssl.cf2.rackcdn.com/sec20/videos/0813/s5_machine_learning_1/3_sec20summer-paper412-presentation-video.mp4&#34;&gt;Download Video [mp4]&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;a href=&#34;//uvasrg.github.io/usenix-security-2020-hybrid-batch-attacks&#34;&gt;Blog Post&lt;/a&gt;&lt;br&gt;&#xA;Paper: [&lt;a href=&#34;//uvasrg.github.io/docs/hybrid_attack.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/1908.07000&#34;&gt;arXiv&lt;/a&gt;]&lt;/p&gt;</description>
    </item>
    <item>
      <title>Congratulations Dr. Xu!</title>
      <link>//uvasrg.github.io/congratulations-dr.-xu/</link>
      <pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/congratulations-dr.-xu/</guid>
      <description>&lt;p&gt;Congratulations to Weilin Xu for successfully defending his PhD Thesis!&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/weilin-defense-IMG_4702.jpg&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/weilin-defense-IMG_4702-2.jpg&#34; width=&#34;70%&#34;&gt;&lt;/a&gt;&#xA;&lt;div class=&#34;caption&#34;&gt;&lt;center&gt;&#xA;Weilin&#39;s Committee: &lt;A href=&#34;http://faculty.virginia.edu/alemzadeh/&#34;&gt;Homa Alemzadeh&lt;/a&gt;, &lt;a href=&#34;https://www.cs.virginia.edu/yanjun/&#34;&gt;Yanjun Qi&lt;/a&gt;, &lt;a href=&#34;http://patrickmcdaniel.org/&#34;&gt;Patrick McDaniel&lt;/a&gt; (on screen)&lt;/a&gt;, &lt;a href=&#34;https://www.cs.virginia.edu/evans&#34;&gt;David Evans&lt;/a&gt;, &lt;a href=&#34;http://vicenteordonez.com/&#34;&gt;Vicente Ordóñez Román&lt;/a&gt;&lt;/center&gt;&#xA;&lt;/div&gt;&#xA;&lt;/center&gt;&#xA;&lt;center&gt;&#xA;&lt;em&gt;Improving Robustness of Machine Learning Models using Domain Knowledge&lt;/em&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;Although machine learning techniques have achieved great success in&#xA;many areas, such as computer vision, natural language processing, and&#xA;computer security, recent studies have shown that they are not robust&#xA;under attack. A motivated adversary is often able to craft input&#xA;samples that force a machine learning model to produce incorrect&#xA;predictions, even if the target model achieves high accuracy on normal&#xA;test inputs. This raises great concern when machine learning models&#xA;are deployed for security-sensitive tasks.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
