<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Publications | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
        <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
	      
	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      
<div style="margin-top:16px; margin-left: auto; margin-right: auto; max-width: 800px;">
<div class="row">
  <div class="column small-12">
    
    
    <h2><a href="/usenix-security-2020-hybrid-batch-attacks/">USENIX Security 2020: Hybrid Batch Attacks</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-12-14 00:00:00 &#43;0000 UTC" itemprop="datePublished">14 December 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yuan-tian">Yuan Tian</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jianfeng-chi">Jianfeng Chi</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/usenix-security">USENIX Security</a>
    
  </span>
  
  
</div>


<p><b><font color="red">New:</font> <a href="/hybrid-batch-attacks-at-usenix-security-2020/">Video Presentation</a></b></p>
<h2 id="finding-black-box-adversarial-examples-with-limited-queries">Finding Black-box Adversarial Examples with Limited Queries</h2>
<p>Black-box attacks generate adversarial examples (AEs) against deep
neural networks with only API access to the victim model.</p>
<p>Existing black-box attacks can be grouped into two main categories:</p>
<ul>
<li>
<p><strong>Transfer Attacks</strong> use white-box attacks on local models to find
candidate adversarial examples that transfer to the target model.</p>
</li>
<li>
<p><strong>Optimization Attacks</strong> use queries to the target model and apply
optimization techniques to search for adversarial examples.</p>
<p class="text-right"><a href="/usenix-security-2020-hybrid-batch-attacks/">Read More…</a></p>
	

    
    <h2><a href="/neurips-2019-empirically-measuring-concentration/">NeurIPS 2019: Empirically Measuring Concentration</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-11-22 00:00:00 &#43;0000 UTC" itemprop="datePublished">22 November 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/neurips">NeurIPS</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/saeed-mahloujifar">Saeed Mahloujifar</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/mohammad-mahmoody">Mohammad Mahmoody</a>
    
  </span>
  
  
</div>


<p><a href="https://www.people.virginia.edu/~xz7bc/">Xiao Zhang</a> will
present our work (with <a
href="https://www.cs.virginia.edu/~sm5fd/">Saeed Mahloujifar</a> and
<a href="https://www.cs.virginia.edu/~mohammad/">Mohamood
Mahmoody</a>) as a spotlight at <a href="https://nips.cc/Conferences/2019/ScheduleMultitrack?event=15792">NeurIPS
2019</a>,
Vancouver, 10 December 2019.</p>
<p>Recent theoretical results, starting with Gilmer et al.&rsquo;s
<a href="https://aipavilion.github.io/"><em>Adversarial Spheres</em></a> (2018), show
that if inputs are drawn from a concentrated metric probability space,
then adversarial examples with small perturbation are inevitable.c The
key insight from this line of research is that <a href="https://en.wikipedia.org/wiki/Concentration_of_measure%22%3E"><em>concentration of
measure</em></a>
gives lower bound on adversarial risk for a large collection of
classifiers (e.g. imperfect classifiers with risk at least $\alpha$),
which further implies the impossibility results for robust learning
against adversarial examples.</p>
<p class="text-right"><a href="/neurips-2019-empirically-measuring-concentration/">Read More…</a></p>
	

    
    <h2><a href="/brink-essay-ai-systems-are-complex-and-fragile.-here-are-four-key-risks-to-understand./">Brink Essay: AI Systems Are Complex and Fragile. Here Are Four Key Risks to Understand.</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-07-09 00:00:00 &#43;0000 UTC" itemprop="datePublished">9 July 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fairness">fairness</a>
    
  </span>
  
  
</div>


<p>Brink News (a publication of <em>The Atlantic</em>) published my essay on the risks of deploying AI systems.</p>
<center>
<a href="https://www.brinknews.com/ai-systems-are-complex-and-fragile-here-are-four-key-risks-to-understand/"><img style="box-shadow: 10px 10px 5px grey;" src="/images/brink.png" width=90%"></a>
</center>
<p><span style="font-weight: 400;">Artificial intelligence technologies have the potential to transform society in positive and powerful ways. Recent studies have shown computing systems that can outperform humans at numerous once-challenging tasks, ranging from performing medical diagnoses and reviewing legal contracts to playing Go and recognizing human emotions. </span></p>
<p><span style="font-weight: 400;">Despite these successes, AI systems are fundamentally fragile — and the ways they can fail are poorly understood. When AI systems are deployed to make important decisions that impact human safety and well-being, the potential risks of abuse and misbehavior are high and need to be carefully considered and mitigated.</span></p>
<p class="text-right"><a href="/brink-essay-ai-systems-are-complex-and-fragile.-here-are-four-key-risks-to-understand./">Read More…</a></p>
	

    
    <h2><a href="/cost-sensitive-adversarial-robustness-at-iclr-2019/">Cost-Sensitive Adversarial Robustness at ICLR 2019</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-05-06 00:00:00 &#43;0000 UTC" itemprop="datePublished">6 May 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/iclr">ICLR</a>
    
  </span>
  
  
</div>


<p>Xiao Zhang will present <a href="https://openreview.net/forum?id=BygANhA9tQ&amp;noteId=BJe7cKRWeN"><em>Cost-Sensitive Robustness against Adversarial Examples</em></a> on May 7 (4:30-6:30pm) at <a href="https://iclr.cc/Conferences/2019/">ICLR 2019 in New Orleans.</p>
<center>
<a href="/docs/cost-sensitive-poster.pdf"><img src="/docs/cost-sensitive-poster-small.png" width="90%" align="center"></a>
</center>
<p>Paper: <a href="https://evademl.org/docs/cost-sensitive-robustness.pdf">[PDF]</a> [<a href="https://openreview.net/forum?id=BygANhA9tQ&amp;noteId=BJe7cKRWeN">OpenReview</a>] [<a href="https://arxiv.org/abs/1810.09225">ArXiv</a>]</p>

	

    
    <h2><a href="/empirically-measuring-concentration/">Empirically Measuring Concentration</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-05-05 00:00:00 &#43;0000 UTC" itemprop="datePublished">5 May 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/iclr">ICLR</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/saeed-mahloujifar">Saeed Mahloujifar</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/mohammad-mahmoody">Mohammad Mahmoody</a>
    
  </span>
  
  
</div>


<p>Xiao Zhang and Saeed Mahloujifar will present our work on <em>Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness</em> at two workshops May 6 at ICLR 2019 in New Orleans: <a href="https://debug-ml-iclr2019.github.io/"><em>Debugging Machine Learning Models</em></a> and <a href="https://sites.google.com/view/safeml-iclr2019"><em>Safe Machine Learning:
Specification, Robustness and Assurance</em></a>.</p>
<p>Paper: <a href="/docs/concentration-robustness.pdf">[PDF]</a></p>
<center>
<a href="/docs/concentration-robustness-poster.pdf"><img src="/docs/concentration-robustness-poster-small.png" width="90%" align="center"></a>
</center>

	

    
    <h2><a href="/ismr-2019-context-aware-monitoring-in-robotic-surgery/">ISMR 2019: Context-aware Monitoring in Robotic Surgery</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-04-03 00:00:00 &#43;0000 UTC" itemprop="datePublished">3 April 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/dependability">dependability</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/machine-learning">machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/samin-yasar">Samin Yasar</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/robotic-surgery">robotic surgery</a>
    
  </span>
  
  
</div>


<p>Samin Yasar presented our paper on <a href="https://arxiv.org/abs/1901.09802"><em>Context-award Monitoring in
Robotic Surgery</em></a> at the 2019
<a href="https://web.archive.org/web/20190416013641/http://www.ismr.gatech.edu/"><em>International Symposium on Medical
Robotics</em></a>
(ISMR) in Atlanta, Georgia.</p>
<center><a href="/images/surgery.png"><img src="/images/surgery.png" width="80%"></a></center>
<p>Robotic-assisted minimally invasive surgery (MIS) has enabled
procedures with increased precision and dexterity, but surgical robots
are still open loop and require surgeons to work with a tele-operation
console providing only limited visual feedback. In this setting,
mechanical failures, software faults, or human errors might lead to
adverse events resulting in patient complications or fatalities. We
argue that impending adverse events could be detected and mitigated by
applying context-specific safety constraints on the motions of the
robot. We present a context-aware safety monitoring system which
segments a surgical task into subtasks using kinematics data and
monitors safety constraints specific to each subtask. To test our
hypothesis about context specificity of safety constraints, we analyze
recorded demonstrations of dry-lab surgical tasks collected from the
JIGSAWS database as well as from experiments we conducted on a Raven
II surgical robot. Analysis of the trajectory data shows that each
subtask of a given surgical procedure has consistent safety
constraints across multiple demonstrations by different subjects. Our
preliminary results show that violations of these safety constraints
lead to unsafe events, and there is often sufficient time between the
constraint violation and the safety-critical event to allow for a
corrective action.</p>
<p class="text-right"><a href="/ismr-2019-context-aware-monitoring-in-robotic-surgery/">Read More…</a></p>
	

    
    <h2><a href="/iclr-2019-cost-sensitive-robustness-against-adversarial-examples/">ICLR 2019: Cost-Sensitive Robustness against Adversarial Examples</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2018-12-20 00:00:00 &#43;0000 UTC" itemprop="datePublished">20 December 2018</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/iclr">ICLR</a>
    
  </span>
  
  
</div>


<p>Xiao Zhang and my paper on <a href="https://openreview.net/forum?id=BygANhA9tQ&amp;noteId=BJe7cKRWeN"><em>Cost-Sensitive Robustness against Adversarial Examples</em></a> has been accepted to ICLR 2019.</p>
<p>Several recent works have developed methods for training classifiers
that are certifiably robust against norm-bounded adversarial
perturbations. However, these methods assume that all the adversarial
transformations provide equal value for adversaries, which is seldom
the case in real-world applications. We advocate for cost-sensitive
robustness as the criteria for measuring the classifier&rsquo;s performance
for specific tasks. We encode the potential harm of different
adversarial transformations in a cost matrix, and propose a general
objective function to adapt the robust training method of Wong &amp;
Kolter (2018) to optimize for cost-sensitive robustness. Our
experiments on simple MNIST and CIFAR10 models and a variety of cost
matrices show that the proposed approach can produce models with
substantially reduced cost-sensitive robust error, while maintaining
classification accuracy.</p>
<p class="text-right"><a href="/iclr-2019-cost-sensitive-robustness-against-adversarial-examples/">Read More…</a></p>
	

    
    <h2><a href="/a-pragmatic-introduction-to-secure-multi-party-computation/">A Pragmatic Introduction to Secure Multi-Party Computation</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2018-12-19 00:00:00 &#43;0000 UTC" itemprop="datePublished">19 December 2018</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/secure-computation">secure computation</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/vladimir-kolesnikov">Vladimir Kolesnikov</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/mike-rosulek">Mike Rosulek</a>
    
  </span>
  
  
</div>


<p><a href="//securecomputation.org"><img src="/images/pragmaticmpc.jpg" align="right"></a></p>
<p><em>A Pragmatic Introduction to Secure Multi-Party Computation</em>,
co-authored with Vladimir Kolesnikov and Mike Rosulek, is now
published by Now Publishers in their
<a href="https://www.nowpublishers.com/SEC"><em>Foundations and Trends in Privacy and Security</em></a> series.</p>
<p>You can download the book for free (we retain the copyright and are
allowed to post an open version) from
<a href="//securecomputation.org">securecomputation.org</a>, or buy an PDF
version from the published for $260 (there is also a printed $99
version).</p>
<div class="abstract">
Secure multi-party computation (MPC) has evolved from a theoretical
curiosity in the 1980s to a tool for building real systems today. Over
the past decade, MPC has been one of the most active research areas in
both theoretical and applied cryptography. This book introduces
several important MPC protocols, and surveys methods for improving the
efficiency of privacy-preserving applications built using MPC. Besides
giving a broad overview of the field and the insights of the main
constructions, we overview the most currently active areas of MPC
research and aim to give readers insights into what problems are
practically solvable using MPC today and how different threat models
and assumptions impact the practicality of different approaches.
</div>
	

    
    <h2><a href="/neurips-2018-distributed-learning-without-distress/">NeurIPS 2018: Distributed Learning without Distress</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2018-12-08 00:00:00 &#43;0000 UTC" itemprop="datePublished">8 December 2018</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/secure-computation">secure computation</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bargav-jayaraman">Bargav Jayaraman</a>
    
  </span>
  
  
</div>


<p>Bargav Jayaraman presented our work on privacy-preserving machine learning at the <a href="https://nips.cc/Conferences/2018/">32<sup>nd</sup> <em>Conference on Neural Information Processing Systems</em></a> (NeurIPS 2018) in Montreal.</p>
<p><em>Distributed learning</em> (sometimes known as <em>federated learning</em>)
allows a group of independent data owners to collaboratively learn a
model over their data sets without exposing their private data.  Our
approach combines <em>differential privacy</em> with secure <em>multi-party
computation</em> to both protect the data during training and produce a
model that provides privacy against inference attacks.</p>
<p class="text-right"><a href="/neurips-2018-distributed-learning-without-distress/">Read More…</a></p>
	

    
    <div class="row">
  <div class="column small-12">
    <ul class="pagination" role="navigation" aria-label="Pagination">
      
      <li><span>Page 2 of 2</span></li>      
      
      <li class="arrow" aria-disabled="true"><a href="/categories/publications/"><em>Newer<span class="show-for-sr"> blog entries</span></em>:&nbsp;&raquo;</a></li>
      
    </ul>    
    All Posts by <a href="//uvasrg.github.io/categories">Category</a> or <a href="//uvasrg.github.io/tags">Tags</a>.

  </div>
</div>

  </div>
</div>
</div>

    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
