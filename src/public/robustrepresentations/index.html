<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Adversarially Robust Representations | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      



	    </div>
	  </div>
	    
	  
	    <div class="top-bar" id="site-menu" >	      
	      <div class="top-bar-title show-for-medium site-title">
		<a href="//uvasrg.github.io/">Security Research Group</a>
	      </div>
	      <div class="top-bar-left">
		<ul class="menu vertical medium-horizontal">
		  
		  
		</ul>
	      </div>
	      <div class="top-bar-right show-for-medium">
		



	      </div>
	    </div>
	  
	</nav>
      
    </header>
    
    <main>
      
<div style="margin-top:16px; margin-left: auto; margin-right: auto; max-width: 800px;">
    <article class="article" itemscope itemtype="http://schema.org/Article">
      
      <h1 itemprop="name">Adversarially Robust Representations</h1>
      <div class="post-metadata">
  <span class="post-date">
    <time datetime="2020-08-14 00:00:00 &#43;0000 UTC" itemprop="datePublished">14 August 2020</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/icml">ICML</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/sicheng-zhu">Sicheng Zhu</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p><em>Post by Sicheng Zhu</em></p>
<p>With the rapid development of deep learning and the explosive growth
of unlabeled data, <a href="https://arxiv.org/abs/1206.5538">representation
learning</a> is becoming increasingly
important. It has made impressive applications such as pre-trained
language models (e.g., <a href="https://arxiv.org/abs/1810.04805">BERT</a> and
<a href="https://github.com/openai/gpt-3">GPT-3</a>).</p>
<p>Popular as it is, representation learning raises concerns about the
robustness of learned representations under adversarial settings. For
example, <em>how can we compare the robustness to different
representations</em>, and <em>how can we build representations that enable
robust downstream classifiers</em>?</p>
<p>In this work, we answer these questions by proposing a notion of
<em>adversarial robustness for representations</em>. We show what the best
achievable robustness for a downstream classifier is limited by a
measurable representation robustness, and provide a training principle
for learning adversarially robust representations.</p>
<h1 id="adversarial-robustness-for-representations">Adversarial Robustness for Representations</h1>
<p>Despite various existing criteria for evaluating a representation
(e.g., smoothness, sparsity), there is no general way previously known
to measure a representation’s robustness under adversarial
perturbations. We propose a notion of adversarial robustness for
representations based on information-theoretic measures.</p>
<center>
<img alt="mutualinformation" src="/images/robustreps/image1.png" style="width: 85%; margin-top: 12px; margin-bottom: 12px;"</img>
</center>
<p>Consider a representation that maps an underlying data distribution to
a representation distribution. In this case, we can measure the
(standard-case) mutual information between the two distributions. Then
by perturbing the data distribution within a Wasserstein ball such
that the mutual information term is minimized, we can measure the
worst-case mutual information. The representation vulnerability (an
opposite notion of robustness) is defined as the difference between
the two terms.</p>
<p>This notion enjoys several desired properties in representation
learning scenarios-it is scale-invariant, label-free, and compatible
with different threat models (including the commonly used
<em>L<sub>p</sub></em> norm attacks). Most importantly, we show next that it
has a direct relationship with the performance of downstream tasks.</p>
<h1 id="connecting-representation-to-downstream-tasks">Connecting Representation to Downstream Tasks</h1>
<p>If a representation is robust, we show (theoretically in a synthetic
setting and empirically in general settings) that a properly trained
downstream classifier will perform consistently in both natural and
adversarial settings, that is the difference between the natural
accuracy and the adversarial accuracy will be small.</p>
<p>If a representation is not robust, we show that no robust downstream
classifiers can be built using that representation.</p>
<p>We provide an information-theoretic upper bound for the maximum robust
accuracy that can be achieved by any downstream classifier, with
respect to the representation robustness. We empirically evaluate the
tightness of this bound and find that the vulnerability of internal
layer representations of many neural networks is at least one
bottleneck for the model to be more robust.</p>
<p>For example, the representation defined by the logit layer of Resnet18
on CIFAR-10 only admits an adversarial accuracy of ~75% for any
downstream classifiers.</p>
<center>
<img alt="miresults" src="/images/robustreps/image2a.png" style="width:60%; margin-top: 12px; margin-bottom: 12px;">
</center>
<p>This motivates us to develop a method to learn adversarially robust
representations.</p>
<h1 id="a-learning-principle-for-robust-representations">A Learning Principle for Robust Representations</h1>
<p>Based on the proposed notion, a natural way to learn adversarially
robust representations is to directly induce the representation
robustness on common representation learning objectives.</p>
<p>We consider a popular representation learning objective — <A
href="https://arxiv.org/abs/1808.06670">mutual information
maximization</a> — as it has impressive performance in practice
and many other objectives (e.g., noise contrastive estimation) can be
viewed as surrogate losses of this objective. By inducing the
representation robustness and setting a specific coefficient, we
provide the worst-case mutual information maximization principle for
learning adversarially robust representations.</p>
<p>We evaluate the performance of our representation learning principle
on four image classification benchmarks (MNIST, Fashion-MNIST, SVHN,
and CIFAR-10), here we report on CIFAR-10 (see the paper for the
others, where the results are similar).</p>
<center>
<img alt="miresults" src="/images/robustreps/image2.png" style="width: 65%; margin-top: 12px; margin-bottom: 12px;">
</center>
<p>Note that the representations are learned using only unlabeled data
and are kept fixed during the training of downstream classifiers.  The
robust downstream classifier (trained using adversarial training)
benefits from the robust representation. It has both better natural
accuracy and better adversarial accuracy. The adversarial accuracy of
~31% is even comparable to the fully-supervised robust model with the
same architecture.</p>
<p>Even the standard classifier based on our robust representation
inherits a non-trivial adversarial accuracy from the robust
representation. And more interestingly, they also have better natural
accuracy compared to the baseline. This phenomenon is consistent with
some <a href="https://arxiv.org/abs/1909.11764">recent work</a> using
adversarial training to learn pre-trained models and may indicate the
better standard generalization of adversarially learned
representations.</p>
<h2 id="saliency-maps">Saliency Maps</h2>
<p>We also visualize the saliency map of our learn representations as
side evaluation of adversarial robustness, since the relationship
between the interpretability of saliency maps and the adversarial
robustness (see <a href="http://proceedings.mlr.press/v97/etmann19a/etmann19a.pdf">Etmann et al.</a>).</p>
<center>
<img alt="miresults" src="/images/robustreps/image3.png" style="width: 75%; margin-top: 12px; margin-bottom: 12px;">
</center>
<p>The saliency maps of our robust representation (third row) are less
noisy and more interpretable than its standard counterpart (second
row).</p>
<h1 id="conclusions">Conclusions</h1>
<p>We show that the adversarial robustness for representations is
correlated with the achievable robustness for downstream tasks, and
that an associated learning principle can be used to produce more
robust representations. Our work motivates leaning adversarially
robust representations as an intermediate step or as a regularization
to circumvent the insurmountable difficulty of directly learning
adversarially robust models.</p>
<p><strong>Paper:</strong> <a href="https://schzhu.github.io/">Sicheng Zhu</a>, <a href="https://people.virginia.edu/~xz7bc/">Xiao Zhang</a>, and <a href="https://www.cs.virginia.edu/~evans">David Evans</a>.
<a href="https://icml.cc/virtual/2020/poster/6604"><em>Learning Adversarially Robust Representations via Worst-Case Mutual Information Maximization</em></a>. In
<a href="https://icml.cc/virtual/2020">International Conference on Machine Learning </a> (ICML 2020), July 2020.
[<a href="/docs/robustrepresentations.pdf">PDF</a>] [<a href="/docs/robustrepresentations-supplement.pdf">Supplemental Materials</a>]
[<a href="https://proceedings.icml.cc/static/paper_files/icml/2020/5124-Paper.pdf">ICML PDF</a>] [<a href="https://arxiv.org/abs/2002.11798">arXiv</a>]</p>
<p><a href="https://icml.cc/virtual/2020/poster/6604">Video Presentation</a> (from ICML 2020)</p>

      </div>

      <meta itemprop="wordCount" content="835">
      <meta itemprop="datePublished" content="2020-08-14">
      <meta itemprop="url" content="//uvasrg.github.io/robustrepresentations/">
    </article>

    <ul class="pagination" role="navigation" aria-label="Pagination" style="margin-top:32px;">
      
      <li class="arrow" aria-disabled="true"><a href="//uvasrg.github.io/intrinsic-robustness-using-conditional-gans/">&laquo; <em>Previous<span class="show-for-sr"> page</span></em>: Intrinsic Robustness using Conditional GANs</a></li>
      
      
      <li class="arrow" aria-disabled="true"><a href="//uvasrg.github.io/merlin-morgan-and-the-importance-of-thresholds-and-priors/"><em>Next<span class="show-for-sr"> page</span></em>: Merlin, Morgan, and the Importance of Thresholds and Priors&nbsp;&raquo;</a></li>
      
    </ul>
</div>

    </main>
    
    
<footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>


    
    <div class="endofpage">
    </div>

    <script src="/js/jquery.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
