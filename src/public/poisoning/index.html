<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title> | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      



	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      
<div class="row">
  <div class="column small-12 medium-10 medium-offset-1 end large-8 large-offset-0">
    <article class="article" itemscope itemtype="http://schema.org/Article">
      
      <h1 itemprop="name"></h1>
      <div class="post-body" itemprop="articleBody">
        <!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    .subgrid {
  grid-column: screen;
  display: grid;
  grid-template-columns: inherit;
  grid-template-rows: inherit;
  grid-column-gap: inherit;
  grid-row-gap: inherit;
}

d-figure.base-grid {
  grid-column: screen;
  background: hsl(0, 0%, 97%);
  padding: 20px 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

d-figure {
  margin-bottom: 1em;
  position: relative;
}

d-figure > figure {
  margin-top: 0;
  margin-bottom: 20;
}

d-figure.attackAnim .param-space-acc .acc-dif-hist .dif-scatter {
  /* grid-column: text; */
  display: block;
  margin-left: auto;
  margin-right: auto;
}

d-figure.acc-dif-hist .tick line, d-figure.dif-scatter .tick line {
  visibility: visible;
}

d-byline {
  /* contain: style;
  overflow: hidden;
  border-top: 1px solid rgba(0, 0, 0, 0.1); */
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  /* font-size: 0.8rem;
  line-height: 1.8em;
  padding: 1em 0;
  padding-top: 0em 0;
  min-height: 1.8em; */
}

/* d-figure.dif-scatter .tick line {
  visibility: visible;
} */

/* d-figure.param-space-acc {
  grid-column: text;
  display: block;
  margin-left: auto;
  margin-right: auto;
} */

d-figure.poison-demo {
  position: relative;
  grid-column: text;
  display: block;
  width: 984px;
  height: 684;
  margin-left: auto;
  margin-right: auto;
  padding-top: 1.5em;
}

.hidden {
  visibility: hidden;
}

.poison-demo-controls {
  position: absolute;
  width: 454px;
  height: 150px;
  left: 20px;
  /* left: calc(100% - 964px - 20px); */
  /* top: 435px; */
  top: 485px;
}

.poison-demo-controls-text {
  position: absolute;
  width: calc(100% - 200px + 10px);
  font-family: Arial, Helvetica, sans-serif;
  font-size: small;
  font-weight: bold;
  text-anchor: middle;
  pointer-events: none;
}

.shaded-figure {
  background-color: hsl(0, 0%, 97%);
  border-top: 1px solid hsla(0, 0%, 0%, 0.1);
  border-bottom: 1px solid hsla(0, 0%, 0%, 0.1);
  padding: 30px 0;
}

.pointer {
  position: absolute;
  width: 26px;
  height: 26px;
  top: 26px;
  left: -48px;
}

.blue-point {
  fill: steelblue;
  opacity: 0.6;
  /* r: 4; */
}

.red-point {
  fill: orangered;
  opacity: 0.7;
  /* r: 4; */
}

.target-point {
  fill: orange;
  /* fill: blueviolet; */
  /* fill: steelblue; */
  opacity: 1;
  /* transform: scale(1.25, 1.25); */
  /* r: 5; */
}

.selected-point {
  fill: orange;
  /* fill: steelblue; */
  opacity: 0.5;
  /* transform: scale(1.25, 1.25); */
  /* r: 5; */
}

.blue-poison {
  fill: steelblue;
  /* opacity: 0.5; */
}

.red-poison {
  fill: orangered;
  /* opacity: 0.5; */
}

.blue-delete {
  fill: black;
}

.red-delete {
  fill: black;
}

.area-blue {
  fill: lightsteelblue;
  opacity: 0.4;
  stroke-width: 0;
}

.area-red {
  fill: red;
  opacity: 0.2;
  stroke-width: 0;
}

.overlay {
  position: absolute;
}

.slider {
  position: absolute;
  border-radius: 2px;
  -webkit-appearance: none;
  grid-column: middle;
}

.attack-slider {
  height: 16px;
  background: #d3d3d3;
  outline: none;
  opacity: 0.7;
  -webkit-animation: 0.2s;
  transform: opacity 0.2s;
  left: 80px;
  width: calc(100% - 80px - 80px);
}
.attack-slider::-webkit-slider-thumb {
  -webkit-appearance: none;
  appearance: none;
  width: 16px;
  height: 32px;
  border-radius: 25%;
  background: #3d3d3d;
  cursor: pointer;
}
.attack-slider::-moz-range-thumb {
  width: 16px;
  height: 32px;
  border-radius: 25%;
  background: #3d3d3d;
  cursor: pointer;
}

.slider-container {
  position: absolute;
  width: 100%;
}

.mode-container {
  position: absolute;
  width: 180px;
  left: calc(100% - 180px);
  /* left: calc(100% - 230px); */
}

.control-slider {
  width: calc(100% - 200px);
  height: 16px;
  padding: 0;
  margin: 0;
  top: 24px;
  opacity: 0.7;
}
.control-slider::-webkit-slider-runnable-track {
  height: 6.4px;
  cursor: pointer;
  box-shadow: 1px 1px 1px #808080, 0px 0px 1px #b0b0b0;
  background: #d3d3d3;
  border-radius: 18px;
  border: 0.1px solid #a0a0a0;
}
.control-slider::-moz-range-track {
  height: 6.4px;
  cursor: pointer;
  box-shadow: 1px 1px 1px #808080, 0px 0px 1px #b0b0b0;
  background: #d3d3d3;
  border-radius: 18px;
  border: 0.1px solid #a0a0a0;
}
.control-slider::-webkit-slider-thumb {
  -webkit-appearance: none;
  appearance: none;
  width: 16px;
  height: 16px;
  border-radius: 40%;
  background: #3d3d3d;
  cursor: pointer;
  margin-top: calc(0.5 * (6.4px - 16px));
}
.control-slider::-moz-range-thumb {
  width: 16px;
  height: 16px;
  border-radius: 40%;
  background-color: #3d3d3d;
  cursor: pointer;
}

/* .slider-container {
  background-color: #f2f2f9;
} */

/* .histogram-slider { */
  /* position: absolute; */
  /* border: 1px solid #AAB; */
  /* background: #BCE; */
  /* height: 100%; */
  /* width: 58px; */
  /* top: 0px; */
  /* cursor: move; */
/* } */

.histogram-slider {
  height: 16px;
  background: #d3d3d3;
  outline: none;
  opacity: 0.7;
  -webkit-animation: 0.2s;
  transform: opacity 0.2s;
  left: 40px;
  width: calc(100% - 40px - 40px);
  cursor: move;
}

.histogram-slider .slider {
  background-color: #9AC;
  height:100%;
}

/* .histogram-slider .box {
  background-color: #9AC;
} */

.histogram-slider .handle {
  position: absolute;
  height: 32px;
  width: 9px;
  /* border: 1px solid #AAB; */
  background: #9AC;

  /* Support for bootstrap */
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}

.histogram-slider .EE {
  right: -4px;
  cursor: e-resize;
}

.histogram-slider .WW {
  cursor: w-resize;
  left: -4px;
}

.histogram-slider .EE, .histogram-slider .WW {
  top: 50%;
  margin-top: -16px;
}

.button {
  position: absolute;
  height: 24px;
  opacity: 0.7;
  background-color: #d3d3d3;
  border-radius: 8px;
  display: block;
  align-items: center;
  -webkit-appearance: none;
}

.play-button {
  left: 50px;
}

.step-forward-button {
  right: calc(50px - 2.5px);
}

.step-back-button {
  left: calc(50px - 32px);
}

.restart-button {
  right: calc(50px - 34px - 2.5px);
  font-family: Arial, Helvetica, sans-serif;
  font-size: small;
  font-weight: bold;
}

.reset-button {
  right: calc(50px - 92px - 2.5px);
  font-family: Arial, Helvetica, sans-serif;
  font-size: small;
  font-weight: bold;
}

.control-button {
  position: relative;
  margin-right: 7.5px;
  width: auto;
  min-width: 40px;
  height: 48px;
  cursor: pointer;
  font-family: Arial, Helvetica, sans-serif;
  font-size: small;
  font-weight: bold;
}

.poison-counter {
  position: absolute;
  height: 24px;
  top: calc(100% - 40px);
  margin: auto;
}

.tick line {
  visibility: hidden;
}

.colorbar .tick line, .histogram-slider .tick line {
  visibility: visible;
}

.input-container {
  position: absolute;
}

.input {
  position: absolute;
  border: 0;
  border-bottom: 2px solid #aaa;
  background: transparent;
  -webkit-appearance: none;
}
.input::-webkit-outer-spin-button,
.input::-webkit-inner-spin-button {
  -webkit-appearance: none;
  margin: 0;
}
.input[type="number"] {
  -moz-appearance: textfield;
}
.input:focus {
  border: none;
  outline: none;
  border-bottom: 2px solid #222;
}

.seed-label {
  position: absolute;
  left: calc(100% - 200px - 115px);
  top: 14px;
  font-family: Arial, Helvetica, sans-serif;
  font-size: 10px;
  font-weight: bold;
  color: #808080;
  pointer-events: none;
  transition: all 0.5s ease-in-out;
}

.mode-label {
  position: absolute;
  font-family: Arial, Helvetica, sans-serif;
  font-size: small;
  font-weight: bold;
}

.seed-textfield {
  font-family: Arial, Helvetica, sans-serif;
  font-size: small;
  left: calc(100% - 200px - 115px);
  width: calc(-250px + 200px + 115px);
  /* width: 100px; */
}

.seed-button {
  left: calc(100% - 200px - 100px - 150px - 10px);
  /* width: 150px; */
  font-family: Arial, Helvetica, sans-serif;
  font-size: small;
  font-weight: bold;
  cursor: pointer;
}

.seed-container {
  position: absolute;
  width: 100%;
}

.demo-buttons-container {
  display: flex;
  justify-content: center;
  position: absolute;
  /* left: calc(100% - 230px); */
  left: calc(100% - 180px);
  top: 32px;
  width: calc(100% - 180px);
  height: calc(100% - 32px);
}

.summary-box-container, .demo-stats-container {
  position: absolute;
  font-family: Arial, Helvetica, sans-serif;
  line-height: 1.2em;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.summary-box-container {
  pointer-events: visible;
}

.summary-box-area {
  /* overflow-y: auto; */
  /* overflow-x: hidden; */
  margin: 10px;
  display:flex;
  flex-wrap: wrap;
  /* width: calc(100% - 20px); */
  /* height: calc(100% - 20px); */
}

.summary-box-entry {
  margin-right: 10px;
  margin-bottom: 10px;
  display: block;
  padding:0px;
  width:calc(25% - 10px);
  /* height:calc(25%); */
}

.attack-summary-container {
  /* overflow-y: auto; */
  margin-top: 10px;
  margin-left:10px;
  display: flex;
  flex-wrap: wrap;
  align-items: flex-start;
  align-content: flex-start;
  /* row-gap: 0px; */
  /* margin-left: 5px; */
  /* margin-right: 10px; */
}
  
.attack-summary-container .item {
  display: block;
  float: left;
  margin-right: 10px;
  margin-bottom: 10px;
  width: calc(25% - 10px);
  height: auto;
}

.demo-stats-title {
  text-align: left;
  margin-left: 6px;
  font-weight: bold;
  font-size: medium;
  margin-bottom: 2px;
}

.demo-stats-entry {
  text-align: left;
  margin-left: 6px;
  margin-right: 6px;
  font-size: small;
  margin-bottom: 0px;
}

.demo-stats-entry .right {
  text-align: right;
}

/* .demo-stats-container {
  position: absolute;
  text-align: left;
  pointer-events: none;
  margin: 1px;
  width: 170px;
  height: auto;
  font-family: Arial, Helvetica, sans-serif;
  font-size: small;
  font-weight: bold;
  line-height: 12px;
  background-color: rgba(255, 255, 255, 0.9);

  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
} */

.legend {
  font-size: smaller;
}

.unselectable {
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  pointer-events: none;
}

.demo-stats-text {
  font-family: Arial, Helvetica, sans-serif;
  font-size: small;
  font-weight: bold;
  line-height: 12px;
}

.fig-title {
  font-family: Arial, Helvetica, sans-serif;
  font-size: large;
}

.fig-label-text {
  font-family: Arial, Helvetica, sans-serif;
  font-size: small;
  /* font-weight: bold; */
  /* line-height: 12px; */
}
  </style>
  <script src="js/_distill/template.v2.js"></script>
  <script src="https://d3js.org/d3.v6.min.js"></script>
</head>

<body>
  <d-front-matter>
    <script type="text/json">
      {
    "title": "Poisoning Attacks and Subpopulation Susceptibility",
    "description": "Experiments on subpopulations vulnerable to poisoning attacks",
    "authors": [
        {
            "author": "Evan Rose",
            "affiliation": "University of Virginia",
            "authorURL": "",
            "affiliationURL": "https://engineering.virginia.edu/departments/computer-science"
        },
        {
            "author": "Fnu Suya",
            "affiliation": "University of Virginia",
            "authorURL": "https://fsuya.org/",
            "affiliationURL": "https://engineering.virginia.edu/departments/computer-science"
        },
        {
            "author": "David Evans",
            "affiliation": "University of Virginia",
            "authorURL": "https://www.cs.virginia.edu/evans",
            "affiliationURL": "https://engineering.virginia.edu/departments/computer-science"
        }
    ],
    "katex": {
        "delimiters": [
            {
                "left": "$",
                "right": "$",
                "display": false
            },
            {
                "left": "$$",
                "right": "$$",
                "display": true
            }
        ]
    }
}
    </script>
  </d-front-matter>

  <d-title>
    <h1>Poisoning Attacks and Subpopulation Susceptibility</h1>
    <p>An Experimental Exploration on the Effectiveness of Poisoning Attacks</p>
  </d-title>

  <d-byline>
    <div class="byline grid" style="grid-template-columns: 1fr 1fr 1fr;">
      <div class="authors-affiliations grid">

        <p class="author">
          <span class="name">Evan Rose</span>
        </p>
        <p class="affiliation">
          <a class="affiliation" href="https://engineering.virginia.edu/departments/computer-science">University of Virginia</a>
        </p>

        <p class="author">
          <a class="name" href="https://fsuya.org/">Fnu Suya</a>
        </p>
        <p class="affiliation"></p>

        <p class="author">
          <a class="name" href="https://www.cs.virginia.edu/evans">David Evans</a>
        </p>
        <p class="affiliation"></p>

      </div>
      <div>
	<p>5th Workshop on Visualization for AI Explainability
        <p><a href="https://visxai.io/">VISxAI</a></p>
        <p>21 September 2022</p>
      </div>
      <!-- <div>
        <h3>DOI</h3>
  
        <p><em>No DOI yet.</em></p>
      </div> -->
    </div>
  </d-byline>

  <d-figure id="svelte-poison-demo-dfigure" class="poison-demo">
    <figure>
      <div id="svelte-demo-target" style="text-align: center"></div>
      <div id="svelte-demo-controls" class="poison-demo-controls">
        <div id="alpha-container" style="top:0px;" class="slider-container">
          <label id="alphaText" for="alphaSlider" class="poison-demo-controls-text">Class Separation α = 1.00</label>
          <input id="alphaSlider" type="range" class="slider control-slider" min="0" max="12" value="4" />
        </div>
        <div id="beta-container" style="top:54px;" class="slider-container">
          <label id="betaText" for="betaSlider" class="poison-demo-controls-text">Random Label Fraction β = 0.00</label>
          <input id="betaSlider" type="range" class="slider control-slider" min="0" max="10" value="0" />
        </div>
        <div id="seed-container" style="top:116px;" class="seed-container">
          <input id="seedButton" type="button" class="button seed-button" value="Randomize Dataset" />
          <label for="seedField" class="seed-label">Dataset seed</label>
          <input id="seedField" type="number" class="input seed-textfield" value="1" />
        </div>
        <div style="position:absolute; right:188px; height: 100%; width:1.5px; background-color:#808080;"></div>
        <div id="mode-container" class="mode-container">
          <label for="mode" class="mode-label">Poisoning algorithm:</label>
          <select style="position:absolute; left:140px; top:4px" id="attackAlgo" name="Attack Algorithm">
            <option value="manual">Manual</option>
            <option value="labelFlip">Label flip</option>
            <option value="modelTargeted">Model-targeted</option>
          </select>
        </div>
        <div id="manual-buttons" class="demo-buttons-container">
          <button id="labelButton" class="button control-button">
            <div>
              Label:
            </div>
            <svg width="16" height="16" viewBox="0 0 10 10">
              <path d="M0 5Q0 10 5 10 10 10 10 5 10 0 5 0 0 0 0 5" fill="steelblue" />
            </svg>
          </button>
          <button id="toolButton" class="button control-button">
            <div>
              Tool:
            </div>
            <svg width="16" height="16" viewBox="0 0 10 10">
              <path d="M1.2145 7.1365-.0113 9.9037 2.7559 8.678l-1.5415-1.5415zM1.5838 6.6299 7.0865 1.1336l1.6781 1.6799L3.2617 8.3098zM9.2027 2.3757l.3494-.3493A1.1871 1.1871 90 107.8736.3477L7.5238.6961z" fill="#1f1f1f" />
            </svg>
          </button>
          <button id="resetButton" class="button control-button">
            <svg width="16" height="16" viewBox="0 0 10 10">
              <path d="M1.5 2 1.5 10 8.5 10 8.5 2 7.5 2 7.5 9 2.5 9 2.5 2ZM0.5 1 9.5 1 9.5 2 0.5 2ZM4 1 6 1 6 0 6 0 4 0ZM3 3 3 8 4 8 4 3ZM6 3 6 8 7 8 7 3ZM4.5 3 4.5 8 5.5 8 5.5 3Z" fill="#1f1f1f" />
            </svg>
          </button>
        </div>
        <div id="algorithm-buttons" class="demo-buttons-container" style="visibility: hidden;">
          <button id="algorithmPlayButton" class="button control-button" style="cursor: pointer">
            <svg width="10" height="10" viewBox="0 0 10 10">
              <path d="M0 0L0 10L3 10L3 0ZM6 0L6 10L9 10L9 0Z" ; fill="#1f1f1f" />
            </svg>
          </button>
          <button id="algorithmResetButton" class="button control-button">
            <svg width="16" height="16" viewBox="0 0 10 10">
              <path d="M1.5 2 1.5 10 8.5 10 8.5 2 7.5 2 7.5 9 2.5 9 2.5 2ZM0.5 1 9.5 1 9.5 2 0.5 2ZM4 1 6 1 6 0 6 0 4 0ZM3 3 3 8 4 8 4 3ZM6 3 6 8 7 8 7 3ZM4.5 3 4.5 8 5.5 8 5.5 3Z" fill="#1f1f1f" />
            </svg>
          </button>
        </div>
      </div>
      <figcaption style="position:absolute; width: 380px; top:485px; left:594px; text-align: left;">

        Machine learning is susceptible to poisoning attacks in which
        adversaries inject maliciously crafted training data into the
        training set to induce specific model behavior.

        We focus on <em>subpopulation attacks</em>, in which the attacker's goal is to induce a model that produces a targeted and incorrect output (label <font color="steelblue">blue</font> in our demos) for a particular subset of the input space (colored <font color="orange">orange</font>). We study the question, <em>which subpopulations are the most vulnerable to an attack and why</em>?
        <figcaption>
    </figure>
  </d-figure>

  <d-article>
    <p>
      Machine learning is susceptible to poisoning attacks, in which an attacker controls a small fraction of the training data and chooses that data with the goal of inducing some behavior (unintended by the model developer) in the trained model <d-cite key="10.5555/1387709.1387716, biggio2012poisoning"></d-cite>. Previous works have mostly considered two extreme attacker objectives: <em>indiscriminate</em> attacks, where the attacker's goal is to reduce overall model accuracy <d-cite key="biggio2012poisoning, 10.5555/3007337.3007488, 10.5555/2886521.2886721, steinhardt2017certified, koh2018stronger"></d-cite>, and <em>instance-targeted</em> attacks, where the attacker's goal is to reduce accuracy on a specific known instance <d-cite key="shafahi2018poison, zhu2019transferable, koh2017understanding, geiping2020witches, huang2020metapoison"></d-cite>. Recently, Jagielski et al. introduced the <em>subpopulation</em> attack, a more realistic setting in which the adversary attempts to control the model’s behavior on a specific subpopulation <d-cite key="jagielski2021subpopulation"></d-cite> while having negligible impact on the model’s performance on the rest of the population. Such attacks are more realistic &mdash; for example, the subpopulation may be a type of malware produced by the adversary that they wish to have classified as benign, or a type of demographic individual for which they want to increase (or decrease) the likelihood of being selected by an employment screening model &mdash; and are harder to detect than indiscriminate attacks.
    </p>

    <p>
      In this article, we present visualizations to understand poisoning attacks in a simple two-dimensional setting, and to explore a question about poisoning attacks against subpopulations of a data distribution: <em>how do subpopulation characteristics affect attack difficulty</em>? We visually explore these attacks by animating a poisoning attack algorithm in a simplified setting, and quantifying the difficulty of the attacks in terms of the properties of the subpopulations they are against.
    </p>

    <h1>Datasets for Poisoning Experiments</h1>

    <p>
      We study poisoning attacks against two different types of datasets: synthetic and tabular benchmark.
    </p>

    <p>
      Synthetic datasets are generated using dataset generation algorithms from Scikit-learn <d-cite key="scikit-learn"></d-cite> and resemble Gaussian mixtures with two components. Each of these datasets is controlled by a set of parameters, which captures different global dataset properties. The first dataset parameter is the class separation parameter $\alpha \ge 0$ which controls the distance between the two class centers. The second dataset parameter is the label noise parameter $\beta \in [0, 1]$ which controls the fraction of points whose labels are randomly assigned. The reason for varying the dataset parameters in our experiments is to determine how properties of the dataset affect poisoning attack difficulty. The synthetic datasets are limited to just two features, so that direct two-dimensional visualizations of the attacks are possible.
    </p>

    <d-figure style="text-align: center;" id="datasets">
      <figure>
        <img style="width:60%; height:auto;" src="images/datasets.svg">
        <figcaption>
          Synthetic datasets for our experiments are controlled by two parameters affecting class separation and label noise.
        </figcaption>
      </figure>
    </d-figure>

    <p>
      We also perform poisoning attacks against the UCI Adult dataset <d-cite key="Dua:2019"></d-cite>, which has been used previously in the evaluations of subpopulation poisoning attacks <d-cite key="suya2021modeltargeted, jagielski2021subpopulation"></d-cite>. The Adult dataset is of much higher dimension (57 after data transformations), and so the attack process cannot be visualized directly as in the case of synthetic datasets. The purpose of this dataset is to gauge the attack behavior in a more realistic setting.
    </p>

    <h3>Synthetic Dataset Parameter Space</h3>

    <p>
      We generate synthetic datasets over a grid of the dataset parameters. For each combination of the two parameters, 10 synthetic datasets are created by feeding different random seeds.
    </p>

    <p>
      We use linear SVM models for our experiments. Before conducting the poisoning attacks against the subpopulations, let’s observe the behavior of the clean models trained on each combination of the dataset parameters:
    </p>

    <d-figure id="svelte-param-space-acc-dfigure" class="l-page-outset">
      <figure>
        <!-- <div id="svelte-param-space-acc-target" style="pointer-events: none; text-align: center;"></div> -->
        <div id="svelte-param-space-acc-target" style="pointer-events: none; margin:auto"></div>
        <figcaption>
          The clean model performance can be affected by the dataset parameters: as the two classes are more separated and less noisy, the clean models achieve higher test accuracy. Click points on the plot to see the datasets generated with each parameter combination.
        </figcaption>
      </figure>
    </d-figure>

    <p>
      The overall trends in the plot meet our expectation: clean model accuracy improves as the classes become more separated and exhibit less label noise.
    </p>

    <h1>Poisoning Attacks</h1>

    <p>
      We use the <a id="attack-algo-return" href="#attack-algo"><em>model-targeted</em> poisoning attack</a> design from Suya et al. <d-cite key="suya2021modeltargeted"></d-cite>, which is shown to have state-of-the-art performance on subpopulation attacks. In a model-targeted poisoning attack, the attacker's objective is captured in a target model and the attack goal is to induce a model as close as possible to that target model. By describing the attacker’s objective using a target model, complex objectives can be captured in a unified way and avoids the need of designing customized attacks for individual objectives. Since the attack is online, the poisoning points can be added into the training set until the attack objective is satisfied, giving a natural measure of the attack's difficulty in terms of the number of poisoning points added during the attack.
    </p>

    <p>
      Our choice of attack algorithm requires a target model as input. For our attacks, the required target model is generated using the label-flipping attack variant described in Koh et al.<d-cite key="koh2018stronger"></d-cite>, which is also used by Suya et al. in their experiments. For each subpopulation, we use the label-flipping attack to generate a collection of candidate classifiers (using different fraction and repetition number) which each achieve 0% accuracy (100% error rate) on the target subpopulation. Afterwards, the classifier with the lowest loss on the clean training set is chosen to be the target model, as done in Suya et al.<d-cite key="suya2021modeltargeted"></d-cite>.
    </p>

    <p>
      For our case, the attack terminates when the induced model misclassifies at least 50% of the target subpopulation, measured on the test set. This threshold was chosen to mitigate the impact of outliers in the subpopulations. In earlier experiments requiring 100% attack success, we observed that attack difficulty was often determined by outliers in the subpopulation. By relaxing the attack success requirement, we are able to capture the more essential properties of an attack against each subpopulation. Since our eventual goal is to characterize attack difficulty in terms of the properties of the targeted subpopulation (which outliers do not necessarily satisfy), this is a reasonable relaxation.
    </p>

    <!-- <d-footnote>Why do we terminate the attack after achieving only 50% attack success? In earlier experiments requiring 100% attack success, we observed that attack difficulty was often determined by outliers in the subpopulation. By relaxing the attack success requirement, we are able to capture the more essential properties of an attack against each subpopulation. Since our eventual goal is to characterize attack difficulty in terms of the properties of the targeted subpopulation (which outliers do not necessarily satisfy), this is a reasonable relaxation.</d-footnote> -->

    <p>
      Since we want to describe attack difficulties by the (essential) number of poisoning points needed (i.e., the number of poisoning points of the optimal attacks), more efficient attacks serve as better proxy to the (unknown) optimal poisoning attacks. To achieve the reported state-of-the-art performance using the chosen model-targeted attack, it is important to choose a suitable target model, and we ensure this by selecting the target models using the selection criteria mentioned above. This selection criteria is also justified in the analysis of the <a id="attack-algo-theoretical-return" href="#attack-algo-theoretical">theoretical properties of the attack framework</a>.
    </p>

    <h1>Synthetic Datasets</h1>

    <p>
      We use cluster subpopulations for our experiments on synthetic datasets. To generate the subpopulations for each synthetic dataset, we run the k-means clustering algorithm ($k=16$) and extract the negative-label instances from each cluster to form the subpopulations. Then, target models are generated according to the above specifications, and each subpopulation is attacked using the online (model-targeted) attack algorithm.
    </p>

    <d-figure id="svelte-scatterplot-dfigure-example1" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
        <figcaption>The poisoning points are added sequentially to induce a model which misclassifies the orange points as positive (i.e., blue color). Hover over points in the animation to view different subpopulations targeted for this dataset, and click them to view a different attack. Since our discussion is based on the pre-selected subpopulation, the reset button in the bottom-right will return focus to the primary subpopulation of interest.</figcaption>
      </figure>
    </d-figure>

    <p>
      The above animation shows how the attack sequentially adds poisoning points into the training set to move the induced model towards the target model. Take note of how the poisoning points with positive labels work to reorient the model's decision boundary to cover the subpopulation while minimizing the impact on the rest of the dataset. This behavior also echoes with the target model selection criteria mentioned above, which aims to minimize the loss on the clean training set while satisfying the attacker goal. Another possible way to generate the target model is to push the entire clean decision boundary downwards without reorienting it<d-footnote>i.e., the target model differs from the clean model only in the bias term.</d-footnote>, but this will result in a model with higher loss on other parts of the dataset, and thus a higher (overall) loss difference to the clean model. Intuitively, such an alternative would experience more "resistance" from the dataset, preventing the induced model from moving as swiftly to the target.
    </p>

    <d-figure class="l-page-outset" style="display: inline-flex; justify-content: space-evenly; margin-top: 25px;">
      <figure style="margin-left: 50px; max-width: 350px; flex: 1;">
        <img src="images/altmodel-orig.png">
        <figcaption><b>Left:</b> The target model generation algorithm chooses the candidate target model which minimizes the loss on the clean dataset while still satisfying the attacker objective.</figcaption>
      </figure>
      <figure style="margin-right: 50px; max-width: 350px; flex: 1">
        <img src="images/altmodel-alt.png">
        <figcaption><b>Right:</b> A poor choice of target model results in an unnecessarily difficult attack, since the poisoning points are wasted preserving unimportant behavior of the target model.</figcaption>
      </figure>
    </d-figure>

    <p>
      The above comparison demonstrates the importance of choosing a good target model and illustrates the benefits of the target model selection criteria above. By using a target model that minimizes the loss on the clean dataset, the attack algorithm is more likely to add poisoning points that directly contribute to the underlying attack objective. On the other hand, using a poor target model causes the attack algorithm to choose poisoning points that also attempt to preserve unimportant target model behaviors on other parts of the dataset.
    </p>

    <h1>Visualizations of Different Attack Difficulties</h1>

    <p>
      Now that we have described the basic setup for performing subpopulation poisoning attacks using the online attack algorithm, we can start to study specific examples of poisoning attacks in order to understand what affects their difficulties.
    </p>

    <h3>Easy Attacks with High Label Noise</h3>

    <p>
      Let's look at a low-difficulty attack. The below dataset exhibits a large amount of label noise $(\beta = 1)$, and as a result the clean model performs very poorly. When the poisoning points are added, the model is quickly persuaded to change its decision with respect to the target subpopulation.
    </p>

    <d-figure id="svelte-scatterplot-dfigure-example2" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
        <figcaption>An attack against a dataset with high label noise is easy, and one possible reason is the clean model may not be heavily influenced by the points in the clean training set.</figcaption>
      </figure>
    </d-figure>

    <p>
      In this particular example, there is a slight class imbalance due to dividing the dataset into train and test sets (963 positive points and 1037 negative points). As a result of this and the label noise, the clean model chooses to classify all points as the majority (negative) label.
    </p>

    <p>
      One interesting observation in this example is, despite being easy to attack overall, attack difficulties of different subpopulations still vary somewhat consistently based on their relative locations to the rest of the dataset. Selecting other subpopulations in the animation reveals that subpopulations near the center of the dataset tend to be harder to attack, while subpopulations closer to the edges of the dataset are more vulnerable. This behavior is especially apparent with <a id="lowercluster" href="#svelte-scatterplot-dfigure-example2">this subpopulation</a> in the lower cluster, where the induced model ends up awkwardly wedged between the two clusters (now referring to the two main "clusters" composing the dataset).
    </p>

    <!-- Old explanation, now invalid: the below outcome only happened as a result of the training convergence issues we experienced in earlier experiments -->
    <!-- Observe that the poisoned model labels all points in the training set as positive, with the corresponding decision boundary lying out of view. In other words, the attack is changing not just the model behavior local to the target subpopulation, but also the global behavior of the model. -->

    <h3>Easy Attacks with Small Class Separation</h3>

    <d-figure id="svelte-scatterplot-dfigure-example3" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
        <figcaption>An attack against a dataset with close class centers is easy, since the linear SVM model does not have sufficient capacity to perform well on the clean training set.</figcaption>
      </figure>
    </d-figure>

    <p>
      The next example shows an attack result similar to the noisy dataset shown above, but now the reason of low attack difficulty is slightly different: although the clean training set now has small label noise, the model does not have enough capacity to generate a useful classification function. The end result is the same: poisoning points can strongly influence the behavior of the induced model. Note that in both of the above examples, the two classes are (almost) balanced; if the labels of the two classes are represented in a different ratio, the clean model may prefer to classify all points as the dominant label, and the attack difficulty of misclassifying target subpopulation into the minority label may also significantly increase.
    </p>

    <p>
      As mentioned earlier, the properties that apply to the above two datasets should also apply to all other datasets with either close class centers or high label noise. We can demonstrate this empirically by looking at the mean attack difficulty over the entire grid of the dataset parameters, where we describe the difficulty of an attack against a dataset of $n$ training samples that uses $p$ poisoning points using the ratio $p/n$. That is, the difficulty of an attack is the fraction of poisoning points added relative to the size of the original clean training set to achieve the attacker goal.
    </p>

    <d-figure id="svelte-param-space-dif-dfigure" class="l-page-outset">
      <figure>
        <div id="svelte-param-space-dif-target" style="pointer-events: none; margin:auto;"></div>
        <figcaption>Average attack difficulty is roughly characterized by clean model accuracy, with more accurate models corresponding to more difficult attacks. Click points on the plot to see different dataset parameter combinations, or click datasets on the right to see specific attacks.</figcaption>
      </figure>
    </d-figure>

    <p>
      It appears that the attacks against datasets with poorly performing clean models tend to be easier than others, and in general attack difficulty increases as the clean model accuracy increases. The behavior is most consistent with clean models of low accuracies, since all attacks are easy and require only a few poisoning points. As the clean model accuracy increases, the behavior becomes more complex and attack difficulty begins to depend more on the properties of the target subpopulation.
    </p>

    <!-- Since attack difficulty is driven by the loss difference between the target and clean models, it makes sense to try to characterize the behavior of further examples in terms of this quantity. -->

    <h3>Attacks on Datasets with Accurate Clean Models</h3>

    <p>
      Next, we look at specific examples of datasets with more interesting attack behavior. In the below example, the dataset admits an accurate clean model which confidently separates the two classes.
    </p>

    <d-figure id="svelte-scatterplot-dfigure-example4" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
        <figcaption>An attack against the edge of a cluster is easy, especially if it causes little collateral damage. On the other hand, an attack against the center of a cluster is hard, since the subpopulation is "protected" by the surrounding data points.</figcaption>
      </figure>
    </d-figure>

    <p>
      The first attack is easy for the attacker despite being against a dataset with an accurate clean model. The targeted subpopulation lies on the boundary of the cluster it belongs to, and more specifically the loss difference between the target and clean models is small. In other words, the attack causes very little "collateral damage."
    </p>

    <p>
      Now, consider <a id="dif850" href="#svelte-scatterplot-dfigure-example4">this subpopulation</a> in the middle of the cluster. This is our first example of an exceptionally difficult attack, requiring over 800 poisoning points. The loss difference between the target and clean models is high since the target model misclassifies a large number of points in the negative class. Notice that it seems hard even to produce a target model against this subpopulation which does not cause a large amount of collateral damage. In some sense, the subpopulation is well protected and is more robust against poisoning attacks due to its central location.
    </p>

    <!-- <p>
      In the above dataset, attack difficulty appears to be influenced in a more subtle way as well: while generally attacks get easier as the targeted subpopulation approaches the edge of the cluster, difficulties are not perfectly symmetric across the cluster. For example, contrast <a id="asymmetric" href="#svelte-scatterplot-dfigure-example4">this subpopulation</a> at the top (150 poisons) with the first (60 poisons). Even though both subpopulations are located at the extremes of the cluster, the one at the top induces a model which cuts more sharply into the positive points.
    </p> -->

    <!-- <h3>Hard Attack with High Collateral Damage</h3> -->

    <p>
      For variety, let's examine attacks against another dataset with an accurate target model.
    </p>

    <d-figure id="svelte-scatterplot-dfigure-example5" class="attackAnim">
      <figure>
        <div id="svelte-scatterplot-target" style="text-align: center;"></div>
        <!-- <figcaption></figcaption> -->
      </figure>
    </d-figure>

    <p>
      Just as before, attack difficulty varies widely depending on the choice of subpopulation, and furthermore attack difficulty can largely be understood by examining the location of the subpopulation with respect to the rest of the dataset.
    </p>

    <h3>Quantitative Analysis</h3>

    <p>
      Visualizations are helpful, but cannot be directly created in the case of high-dimensional datasets. Now that we have visualized some of the attacks in the lower-dimensional setting, can we quantify the properties that describe the difficulty of poisoning attacks?
    </p>

    <p>
      In the previous section, we made the observation that attack difficulty tends to increase with clean model accuracy. To be a little more precise, we can see how the attack difficulty distribution changes as a function of clean model accuracy, for our attack setup.
    </p>

    <d-figure id="svelte-cleanacc-dif-hist-dfigure" class="acc-dif-hist">
      <figure>
        <div id="svelte-cleanacc-dif-hist-target" style="pointer-events: none; text-align: center;"></div>
        <figcaption>Datasets with accurate clean models tend to produce attacks with a wider range of difficulties. Datasets with inaccurate clean models tend to mostly produce easy attacks.</figcaption>
      </figure>
    </d-figure>

    <p>
      The histogram empirically verifies some of our observations from the earlier sections: attacks against datasets with inaccurate clean models tend to be easy, while attacks against datasets with accurate clean models can produce a wider range of attack difficulties.
    </p>

    <p>
      Of course, on top of the general properties of the dataset and the clean model, we are more interested in knowing the properties of the subpopulation that affect attack difficulty. One way to do this is to gather a numerical description of the targeted subpopulation, and use that data to predict attack difficulty.
    </p>

    <d-figure id="svelte-synth-scatter-dfigure" class="dif-scatter">
      <figure>
        <div id="svelte-synth-scatter-target" style="text-align: center;"></div>
        <figcaption>A numerical description of a subpopulation can be used to predict the difficulty of an attack against it. But does it really give a complete picture?</figcaption>
      </figure>
    </d-figure>

    <p>
      As expected, model loss difference strongly correlates with attack difficulty. Other numerical factors describing subpopulations, like subpopulation size, do not have clear correlations with the attack difficulty. It is possible that complex interactions among different subpopulation properties might correlate strongly with the attack difficulty, and are worth future investigations.
    </p>
    <!-- While this statistic is specific to a model-targeted attack, it is still a useful first attempt at predicting attack difficulty. If there exist target models which achieve the attacker's goal without greatly impacting loss on the clean dataset, then it might be possible to find an easy, hard-to-detect poisoning attack against that subpopulation. On the other hand, if it is difficult even to find a suitable target model which does not inflict heavy damage on the clean dataset, then there is reason to believe that an attack against the subpopulation is more difficult in an essential way. -->

    <h1>Adult Dataset</h1>

    <p>
      While the visualizations made possible by the synthetic datasets are useful for developing an intuition for subpopulation poisoning attacks, we want to understand how subpopulation susceptibility arises in a more complex and practical setting. For this purpose, we perform subpopulation poisoning attacks against the UCI Adult dataset, based on US Census data, whose associated classification task is to determine whether an adult's income exceeds $50,000 per year based on attributes such as education, age, race, and martial status.
    </p>

    <h3>Subpopulation Formation</h3>

    <p>
      Subpopulations for the Adult dataset are selected based on combinations of attributes. To generate a semantic subpopulation, first a subset of categorical features is selected, and specific values are chosen for those features. For example, the categorical features could be chosen to be "work class" and "education level", and the features' values could then be chosen to be "never worked" and "some college", respectively. Then, every negative-label ("$\le$ 50K") instance in the training set matching all of the (feature, label) pairs is extracted to form the subpopulation. The subpopulations for our experiments are chosen by considering every subset of categorical features and every combination of those features that is present in the training set. For simplicity, we only consider subpopulations with a maximum of three feature selections.
    </p>

    <p>
      In total, 4,338 subpopulations are formed using this method. Each of these subpopulations is attacked using the same attack as in the case of synthetic dataset. Of these attacks, 1,602 were trivial (i.e., the clean model already satisfies the attack objective), leaving 2,736 nontrivial attacks.
    </p>

    <h3>Visualization</h3>

    <p>
      The Adult dataset is high-dimensional (57 dimensions after data transformations), so attacks against it cannot be directly visualized as in the case of our two-dimensional synthetic datasets. However, by employing dimensionality reduction techniques, we can indirectly visualize poisoning attacks in the high-dimensional setting.
    </p>

    <p>
      Dimensionality reduction gives us a way to visualize the high-dimensional data in two dimensions, but we still need a way to visualize a classifier's behavior on these points. Previous attempts to visualize high-dimensional classifiers have used self-organizing maps&nbsp;<d-cite key="Hamel2006VisualizationOS"></d-cite>, projection-based tour methods <d-cite key="Caragea2001"></d-cite>, hyperplane intersection techniques <d-cite key="Poulet2008"></d-cite>, or Voronoi tesslations constructed from projected data points <d-cite key="Migut2013"></d-cite>. We will focus on another type of technique explored separately by Shulz et al. and Rodrigues et al., which attempts to produce a rich depiction of the classifier's behavior by examining the classifier's classification on data points sampled from the high-dimensional space <d-cite key="Schulz2014, Rodrigues2019"></d-cite>.
    </p>

    <p>
      The key idea is to find additional points in the high-dimensional space corresponding to points in the lower-dimensional projection space. The visualization process can be described by the following procedure:
    </p>

    <ol>
      <li>
        Given a high-dimensional data space $\mathcal{X}$, a low-dimensional projection space $\mathcal{Z}$, and data points $x_i$ from $\mathcal{X}$, train an embedding map $\pi \colon \mathcal{X} \to \mathcal{Z}$ to obtain the projected points $z_i = \pi(x_i)$.
      </li>
      <li>
        Sample the projection space $\mathcal{Z}$ to obtain the image sample points $z'_i$. Determine points $x'_i$ in the data space $\mathcal{X}$ which project to the image sample points $\pi(x'_i) \approx z_i$ via some learned inverse mapping $\pi^{-1} \colon \mathcal{Z} \to \mathcal{X}$.
      </li>
      <li>
        Use the classifier $f$ defined on the data space to determine labels $f(x'_i)$ for each of the additional data points $x'_i$. Visualize the data points and classifier behavior by plotting the projected points $z_i$ on a scatterplot and coloring the background according to the sampled pairs $(z'_i, f(x'_i))$.
      </li>
    </ol>

    <!-- <p>
      The image sample points can be sampled in a rectangular grid as in of Shulz et al., or uniformly at random as in Rodrigues et al. In our experiments, we sample according to a 
    </p> -->

    <p>
      We apply this technique to our attacks against the Adult dataset, using Isomap embedding as our dimensionalty reduction algorithm. Here is the resulting visualization:
          </p>

    <!-- <d-figure> -->
    <!-- <figure> -->
    <video autoplay loop muted style="padding-bottom: 2em;">
      <source src="videos/adult-1.mp4" type="video/mp4">
    </video>
    <figcaption style="margin-top: -5em; margin-bottom: 5em;">In the above visualization, the classifier's behavior is illustrated using the plot's background color. Notice how the blue decision regions slowly grow, reflecting the attack's effectiveness in causing the poisoned model to classify more points as blue.</figcaption>
    <!-- </figure> -->
    <!-- </d-figure> -->

    <p>
      Already, we can see how the general behavior of the classifier changes as poisoning points are added.
    </p>

    <p>
      The above visualization is a good way to measure the impact of poisoning points on the poisoned classifier. However, it can be difficult to see just how much the classifier's behavior is changing on the targeted subpopulation (never married). We can instead choose to perform the attack visualization technique on only the target subpopulation, in which case we get a much more focused understanding of the model's behavior:
    </p>

    <video autoplay loop muted style=" padding-bottom: 2em;">
      <source src="videos/adult-2.mp4" type="video/mp4">
    </video>
    <figcaption style="margin-top: -5em; margin-bottom: 5em;">Embedding only the targeted subpopulation gives a more useful visualization that emphasizes the attack objective.</figcaption>

    <p>
      These visualizations provide a starting point to understanding subpopulation poisoning attacks in a high-dimensional setting. But, gaining intuitions about high-dimensionality data is difficult, and any mapping to a two-dimensional space involves compromises because of our inability to display high dimensional spaces. 
    </p>

    <h3>Quantitative Analysis</h3>

    <p>
      We examine the attack difficulty (measured by the ratio $p / n$) distribution of all the nontrivial attacks:
    </p>

    <d-figure id="svelte-adult-dif-hist-dfigure" class="acc-dif-hist">
      <figure>
        <div id="svelte-adult-dif-hist-target" style="pointer-events: none; text-align: center;"></div>
        <figcaption>Attacks in a more practical setting still yield an interesting distribution of attack difficulties.</figcaption>
      </figure>
    </d-figure>

    <!-- <p>
      In the above histogram, we see a wide range of attack difficulties. Note that the overall low difficulties still represent a significant contribution from the attacker, since $n = 15,682$ for the Adult dataset (for example, a difficulty of 0.05 corresponds to ~780 poisons).
    </p> -->

    <p>
      The range of difficulties as indictated by the histogram is significant: an attack with a difficulty of 0.1 uses 10 times as many points as an attack with a difficulty of 0.01. For a more concrete example, consider the subpopulation consisting of all people who have never married and the subpopulation consisting of divorced craft-repair workers (each subpopulation only taking negative-labeled instances). Both subpopulations are perfectly classified by the clean model, but the attack against the first subpopulation uses 1,490 poisoning points, while the attack against the second uses only 281.
    </p>

    <p>
      We can also plot the attack difficulty against numerical properties of the Adult dataset subpopulations:
    </p>

    <d-figure id="svelte-adult-scatter-dfigure" class="dif-scatter">
      <figure>
        <div id="svelte-adult-scatter-target" style="text-align: center;"></div>
        <figcaption>A numerical description of the subpopulation serves as a starting point for understanding attack difficulty in more complex settings.</figcaption>
      </figure>
    </d-figure>

    <h3>Semantic Subpopulation Characteristics</h3>

    <p>
      Recall that our goal is to determine how subpopulation characteristics affect attack difficulty. While this question can be posed in terms of the statistical properties of the subpopulation (e.g., relative size to the whole data), it is also interesting to ask which semantic properties of the subpopulation contribute significantly to the attack difficulty. In this section, we explore the relationships between attack difficulty and the semantic properties of the targeted subpopulation.
    </p>

    <h5>Ambient Positivity</h5>

    <p>
      To start, we compare a few attacks from our experiments on the Adult dataset.
    </p>

    <d-figure id="svelte-adult-comparison1-dfigure" class="acc-dif-hist">
      <figure>
        <div id="svelte-adult-comparison1-target" style="pointer-events: none; text-align: center;"></div>
        <figcaption>In some cases, ambient positivity is an indicator of the attack difficulty, especially if it relates to the loss difference (on clean training data) between the clean and the target models.</figcaption>
      </figure>
    </d-figure>

    <p>
      The above subpopulations are all classified with 100% accuracy by the clean model and are of similar sizes (ranges from 1% to 2% of the clean training set size). So what makes some of the attacks more or less difficult? In the attacks shown above, the differences may be related to the points surrounding the subpopulation. In our experiments, we defined the subpopulations by taking only negative-label instances satisfying some semantic properties. If we remove the label restriction, we gain a more complete view of the surrounding points, and, in particular, can consider some statistics (e.g., label ratio) of the ambient points.
    </p>

    <p>
      For a subpopulation with a given property $P$, let us call the set of all points satisfying $P$ the <em>ambient subpopulation</em> (since it also includes positive-label points), and call the fraction of points in the ambient subpopulation with a positive label the <em>ambient positivity</em> of the subpopulation. In the above attacks, attack difficulty is negatively correlated with the ambient positivity of the subpopulation. This makes sense, since positive-label points near the subpopulation work to the advantage of the attacker when attempting to induce misclassification of the negative-label points. Stated in terms of the model-targeted attack, if the clean model classifies the ambient subpopulation as the negative label, then the loss difference between the target and clean models is smaller if there are positive-label points in that region.
    </p>

    <p>
      But does the ambient positivity of a subpopulation necessarily determine attack difficulty for otherwise similar subpopulations? If we restrict our view to subpopulations with similar pre-poisoning ambient positivity (e.g., between 0.2 and 0.3), we still find a significant spread of attack difficulties:
    </p>

    <d-figure id="svelte-adult-comparison2-dfigure" class="acc-dif-hist">
      <figure>
        <div id="svelte-adult-comparison2-target" style="pointer-events: none; text-align: center;"></div>
        <figcaption>Even with similar size, ambient positivity, and clean model performance, subpopulations still experience significant differences in terms of the attack difficulty.</figcaption>
      </figure>
    </d-figure>

    <p>
      Once again, the above subpopulations are all classified with 100% accuracy by the clean model and are of similar sizes. Are these differences in attack difficulties just outliers due to our particular experiment setup, or is there some essential difference between the subpopulations which is not captured by their numerical descriptions? Furthermore, if such differences do exist, can they be described using the natural semantic meaning of the subpopulations?
    </p>

    <!-- by feature -->
    <!-- It'd be nice to be able to rank each feature and label pair in order of 
    increasing susceptibility, but I don't think we had sufficient data to make
    any interesting claims -->


    <!-- <p>
      We should also be able to determine which groups (as described by categorical features of a dataset) are generally more susceptible to a poisoning attack, even when the precise subpopulation can vary. One way to gauge this behavior 
    </p> -->

    <h5>Pairwise Analysis</h5>

    <p>
      What happens when two semantic subpopulations match on the same features, but differ in the value of only a single feature? For example, consider the two attacks below:
    </p>

    <d-figure id="svelte-adult-comparison3-dfigure" class="acc-dif-hist">
      <figure>
        <div id="svelte-adult-comparison3-target" style="pointer-events: none; text-align: center;"></div>
        <figcaption>What can we learn about a feature's impact on the attack difficulty by varying that feature's value in the subpopulation? The above subpopulations exhibit significantly different attack difficulties, yet differ only slightly in their semantic descriptions.</figcaption>
      </figure>
    </d-figure>

    <p>
      In the above example, the subpopulations possess very similar semantic descriptions, only differing in the value for the "relationship status" feature. Furthermore, the subpopulations are similarly sized with respect to the dataset (1.3% and 1.1% of the clean training set, respectively), and each subpopulation is perfectly classified by the clean model. Yet the first subpopulation required significantly more poisoning points, at least for the chosen model-targeted attack.
    </p>

    <h3>Adult Dataset Summary</h3>

    <p>
      Our experiments demonstrate that poisoning attack difficulty can vary widely, even in the simple settings we consider. Although we cannot identify all the characteristics that relate to the attack difficulty, we can characterize some of them accurately for certain groups of subpopulations, giving the first attempt in understanding this complicated problem.
    </p>

    <!-- <h3>Adult Attack Animation</h3>

    <p>
      As we have mentioned, our poisoning attacks on the Adult dataset cannot be directly visualized. However, by utilizing dimensionality reduction algorithms, we can indirectly visualize the attacks by
    </p> -->

    <!-- <d-figure id="svelte-adult-dif-hist-dfigure" class="acc-dif-hist">
      <figure>
        <div id="svelte-adult-dif-hist-target" style="pointer-events: none; text-align: center;"></div>
        <figcaption>Caption.</figcaption>
      </figure>
    </d-figure> -->

    <h1>Discussion</h1>

    <p>
      In this article, we visualize and evaluate the effectiveness of poisoning attacks, in both artificial and realistic settings. The difficulty of poisoning attacks on subpopulations varies widely, due to the properties of both the dataset and the subpopulation itself. These differences in the attack difficulty, as well as the factors that affect them, can have important consequences in understanding the realistic risks of poisoning attacks.
    </p>

    <p>
      Our results are limited to simple settings and a linear SVM model, and it is not yet clear how well they extend to more complex models. However, as a step towards better understanding of poisoning attacks and especially in understanding how attack difficulty varies with subpopulation characteristics, experiments in such a simplified setting are valuable and revealing. Further, simple and low-capacity models are still widely used in practice due to their ease of use, low computational cost and effectiveness <d-cite key="decrema2019progress, tramer2021differentially"></d-cite>, and so our simplified analysis is still relevant in practice. Second, kernel methods are powerful tools to handle non-linearly separable datasets by projecting them into a linearly separable high-dimensional space and are widely adopted in practice. Therefore, if the important spatial relationships among the data points are still preserved after projection, then the same conclusions obtained in our simplified settings still apply to the more complex cases by examining the spatial relationships in the transformed space.
    </p>

    <!-- <p>
      Another objection is that the model-targeted attack may not adequately describe the difficulty of any attack against a subpopulation, since a more optimal attack may exist that uses a different target model or some completely different attack framework.
    </p>

    <p>
      One open question is how subpopulation susceptibility depends on label ratio of the training data. In our experiments, all the examined datasets are class-balanced. What happens when this is not the case? Do subpopulations from parts of the input space lacking representation in the training data admit easier poisoning attacks?
    </p> -->
  </d-article>

  <d-appendix>
    <h3 id="attack-algo">Attack Algorithm</h3>

    <p>
      We use the online model-targeted attack algorithm developed by Suya et al.<d-cite key="suya2021modeltargeted"></d-cite>. Given the clean training set, target model, and model training parameters, the attack algorithm produces a set of poisoning points sequentially by maintaining an intermediate induced model and choosing the point that maximizes the loss difference between the intermediate model and the target model. The selected poisoning point is then added to the current training set and the intermediate model is also updated accordingly by the attacker using the knowledge of the model training process. Importantly, the online attack provides theoretical guarantees on the convergence of the induced model to the target model as the number of poisoning points increases. Since we have chosen our target model to misclassify the entire subpopulation, this means we have a guarantee that the online attack process will eventually produce a poisoned training set (which may be huge in size) whose induced model can satisfy the attacker objective.
    </p>

    <a href="#attack-algo-return">Back to the main text</a>

    <h3 id="attack-algo-theoretical">Attack Algorithm Theoretical Properties</h3>

    <p>
      We consider a binary prediction task $h : \mathcal{X} \rightarrow \mathcal{Y},$ where $\mathcal{X} \subseteq \mathbb{R}^d$ and $\mathcal{Y} = \{+1, -1\},$ and where the prediction model $h$ is characterized by the model parameters $\theta \in \Theta \subseteq \mathbb{R}^d.$ We denote the non-negative convex loss on a point $(x, y) \in \mathcal{X} \times \mathcal{Y}$ by $l(\theta; x, y)$, and define the loss over a set of points $A$ as $L(\theta; A) = \sum_{(x, y) \in A} l(\theta; x, y)$. Denote by $\mathcal{D}_c$ the clean dataset sampled from some distribution over $\mathcal{X} \times \mathcal{Y}$.
    </p>

    <p>
      A useful consequence of using the online attack algorithm is that the rate of convergence is characterized by the loss difference between the target and clean models on the clean dataset. If we define the <em>loss-based distance</em> $D_{l, \mathcal{X}, \mathcal{Y}} : \Theta \times \Theta \rightarrow \mathbb{R}$ between two models $\theta_1, \theta_2$ over a space $\mathcal{X} \times \mathcal{Y}$ with loss function $l(\theta; x, y)$ by
    </p>

    <d-math style="margin: auto" block>
      D_{l, \mathcal{X}, \mathcal{Y}}(\theta_1, \theta_2) := \max_{(x, y) \in \mathcal{X} \times \mathcal{Y}} l(\theta_1; x, y) - l(\theta_2; x, y),
    </d-math>

    <p>
      then the loss-based distance between the induced model $\theta_{atk}$ and the target model $\theta_p$ correlates to the loss difference $L(\theta_p; \mathcal{D}_c) - L(\theta_c; \mathcal{D}_c)$ between $\theta_p$ and the clean model $\theta_c$ on the clean dataset $\mathcal{D}_c$. This fact gives a general heuristic for predicting attack difficulty: the closer a target model is to the clean model as measured by loss difference on the clean dataset (under the same search space of poisoning points), the easier the attack will be. This also justifies the decision to choose a target model with lower loss on the clean dataset.
    </p>

    <a href="#attack-algo-theoretical-return">Back to the main text</a>

    <!-- <h1>Problem Setup</h1>

    <p>
      We consider a binary prediction task $h : \mathcal{X} \rightarrow \mathcal{Y},$ where $\mathcal{X} \subseteq \mathbb{R}^d$ and $\mathcal{Y} = \{+1, -1\},$ and where the prediction model $h$ is characterized by the model parameters $\theta \in \Theta \subseteq \mathbb{R}^d.$ We denote the non-negative convex loss on a point $(x, y) \in \mathcal{X} \times \mathcal{Y}$ by $l(\theta; x, y)$, and define the loss over a set of points $A$ as $L(\theta; A) = \sum_{(x, y) \in A} l(\theta; x, y)$.
    </p>

    <p>
      We use the <em>model-targeted</em> poisoning attack design from Suya et al. <d-cite key="suya2021modeltargeted"></d-cite>, which is shown to have state-of-the-art performance on subpopulation attacks. In a model-targeted poisoning attack, the attacker's objective is captured in a target model. The goal is to induce a model as close as possible to that target model. By describing the objective using a target model, complex objectives can be captured without needing to design customized attacks. [TODO (Suya) - need a bit more motivation here, but I think overall should try to condense this section - we want to get to the dataset parameter space figure more quickly. Maybe, can separate the description of the datasets to get to that figure first, before talking about the poisoning attack algorithm, and then introduce the poisoning attacks, and then to integrate the description of the attacks with the first visualization.]

      We also describe the poisoning attack process with the following game-theoretic formalization:
    </p>

    <ol>
      <li>
        $N$ data points are drawn from the true data distribution over $\mathcal{X} \times \mathcal{Y}$ to produce the clean training set $\mathcal{D}_c.$
      </li>
      <li>
        The adversary, knowing $\mathcal{D}_c$ and the model space $\Theta$, generates a target classifier $\theta_p \in \Theta$ which satisfies the attack objective.
      </li>
      <li>
        The adversary, knowing $\mathcal{D}_c,$ $\Theta,$ $\theta_p,$ and the training process, produces a set of poisoning points $\mathcal{D}_p.$
      </li>
      <li>
        The model builder trains on $\mathcal{D}_c \cup \mathcal{D}_p$ to produce the induced classifier $\theta_{atk}.$
      </li>
    </ol>

    <p>
      We next describe the components we use in our experiments for each part of the above framework, as well as the formation of the subpopulations.
    </p>

    <h3>Datasets</h3>

    <p>
      We study poisoning attacks against two different types of datasets. First, we generate synthetic datasets using dataset generation algorithms from Scikit-learn <d-cite key="scikit-learn"></d-cite>. Each of these datasets is controlled by a set of parameters, which captures different global dataset properties (e.g., separation between different classes). The synthetic datasets are also limited to just two features, so that direct visualizations of the attacks are possible. Second, we use the UCI Adult dataset <d-cite key="Dua:2019"></d-cite>, which has been used previously in the evaluations of subpopulation poisoning attacks <d-cite key="suya2021modeltargeted, jagielski2021subpopulation"></d-cite>. The Adult dataset is of much higher dimension (57 after data transformations), and so the attack process cannot be visualized directly as in case of synthetic datasets. The purpose of this dataset is to gauge the attack behavior in a more complicated and practical setting.
    </p>

    <h3>Subpopulation Formation and Adversary Goal</h3>

    <p>
      We consider two subpopulation generation processes: clustering and semantic generation. Cluster subpopulations are chosen by using a k-means clustering algorithm to divide the training set into $k$ different clusters (ClusterMatch in Jagielski et al. <d-cite key="jagielski2021subpopulation"></d-cite>). Then we only consider instances with negative labels (in binary classification tasks) from each cluster and form the final cluster subpopulations. The attacker objective is to have the induced model misclassify the entire subpopulation into the positive class. This generation process is the same as the one used in Suya et al. <d-cite key="suya2021modeltargeted"></d-cite>, and is useful since it allows us to measure the success of the attack without any ambiguity using the accuracy of the induced model on the subpopulation. A completely successful attack attains 0% test accuracy on the target subpopulation, and the attack success may, in general, be measured as the error rate on the target subpopulation.
    </p>

    <p>
      In the semantic generation setting, the adversary cares about some semantic property possessed by certain instances in the dataset. Semantic subpopulations are generated by using a feature-matching algorithm (FeatureMatch in Jagielski et al.<d-cite key="jagielski2021subpopulation"></d-cite>) to obtain those instances which satisfy the semantic property and again taking only the instances with a negative label. This subpopulation generation process reflects a more realistic attacker objective, since an attacker is likely to care about influencing model behavior on a subpopulation matching a set of meaningful properties. However, this process relies on the existence of semantically meaningful features in the dataset, and so cannot be performed on our synthetic datasets.
    </p>

    <h3>Training Algorithm</h3>

    <p>
      We assume the model builder trains models using empirical risk minimization (ERM) with the following optimization strategy:
    </p>

    <d-math style="margin: auto" block>
      \theta_c = \underset{\theta \in \Theta}{\text{argmin}} \frac{1}{|\mathcal{D}_c|} L(\theta; \mathcal{D}_c) + C_r \cdot R(\theta)
    </d-math>

    <p>
      where $R(\cdot)$ is the non-negative regularization function and $C_r$ is the regularization strength parameter.
    </p>

    <h3>Target Model</h3>

    <p>
      We conduct experiments on linear SVM models. The attack method we choose for visualization (details below) requires inputting a target model as attack parameter. The required target model is generated using the label-flipping attack variant described in Koh et al.<d-cite key="koh2018stronger"></d-cite>, which is also used by Suya et al. in their experiments. As opposed to the more general attack settings where adversaries can control both the features and labels of the poisoning points, label-flipping attackers can only change some fraction of the labels of the clean training set. In the variant described by Koh et al., the adversary randomly chooses some fraction of samples from the target subpopulation of the clean training set, flips their labels, and repeats them some number of times to produce the poisoned training set. The target model for our attack method is chosen to be the induced model from this label-flipping attack. More specifically, for each subpopulation, we use the label-flipping attack to generate a collection of candidate classifiers (using different fraction and repetition number) which each achieve 0% accuracy (100% error rate) on the target subpopulation. Afterwards, the classifier with the lowest loss on the clean training set is chosen to be the target model, as done in Suya et al.<d-cite key="suya2021modeltargeted"></d-cite>.
    </p>

    <p>
      People may wonder why we choose the online model-targeted attack instead of simpler ones such as the label-flipping attacks. In our experiments, the model-targeted attacks were in general much more efficient than the label-flipping attacks to achieve the same attacker objective. Since our eventual goal is to describe attack difficulties by the (essential) number of poisons needed (i.e., number of poisoning points of optimal attacks), more efficient attacks serve as better proxy to the optimal poisoning attacks. Moreover, the model-targeted attack has nice theoretical properties and is a general enough framework to be worthy of study on its own.
    </p>

    <p>
      Now that we have outlined the attack process, we can move on to our first set of experiments on synthetic datasets.
    </p>

    <h1>Synthetic Datasets</h1>

    <h3>Synthetic Dataset Generation</h3>

    <p>
      For the first set of experiments, we consider a family of synthetic datasets resembling Gaussian mixtures with two components. The datasets are controlled by two parameters, which we will refer to as the dataset parameters. The first dataset parameter is the class separation parameter $\alpha \ge 0$ which controls the distance between the two class centers. The second dataset parameter is the label noise parameter $\beta \in [0, 1]$ which controls the fraction of points whose labels are randomly assigned. For fixed dataset parameters, we generate several different datasets by feeding different random seeds. Each of the datasets generated contains $n=3000$ samples and is class-balanced. Each dataset is then divided into training and test sets in a 2:1 ratio.
    </p> -->





    <!-- <h3>Acknowledgments</h3>
    <p>
      We are deeply grateful to...
    </p>

    <p>
      Many of our diagrams are based on...
    </p>

    <h3>Author Contributions</h3>
    <p>
      <b>Research:</b> Alex developed ...
    </p>

    <p>
      <b>Writing & Diagrams:</b> The text was initially drafted by...
    </p> -->


    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="bibliography.bib"></d-bibliography>

<script type="text/javascript" src="index.bundle.js"></script></body>

      </div>

      <meta itemprop="wordCount" content="5030">
      <meta itemprop="datePublished" content="0001-01-01">
      <meta itemprop="url" content="//uvasrg.github.io/poisoning/">
    </article>


  </div>
</div>

    </main>
    
    
<footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>


    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
