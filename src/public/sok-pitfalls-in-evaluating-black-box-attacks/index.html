<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>SoK: Pitfalls in Evaluating Black-Box Attacks | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      



	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      
<div style="margin-top:16px; margin-left: auto; margin-right: auto; max-width: 800px;">
    <article class="article" itemscope itemtype="http://schema.org/Article">
      
      <h1 itemprop="name">SoK: Pitfalls in Evaluating Black-Box Attacks</h1>
      <div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-12-20 00:00:00 &#43;0000 UTC" itemprop="datePublished">20 December 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/tingwei-zhang">Tingwei Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jingtao-hong">Jingtao Hong</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yuan-tian">Yuan Tian</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/satml">SaTML</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/black-box-adversarial-attacks">black-box adversarial attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/systemization-of-knowledge">systemization of knowledge</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Post by <strong><a href="https://www.anshumansuri.com/">Anshuman Suri</a></strong> and <strong><a href="https://fsuya.org/">Fnu Suya</a></strong></p>
<p>Much research has studied black-box attacks on image classifiers,
where adversaries generate adversarial examples against unknown target
models without having access to their internal information. Our
analysis of over 164 attacks (published in 102 major security, machine
learning and security conferences) shows how these works make
different assumptions about the adversary’s knowledge.</p>
<p>The current literature lacks cohesive organization centered around the
threat model. Our <a href="https://arxiv.org/abs/2310.17534">SoK paper</a> (to
appear at <a href="https://satml.org/">IEEE SaTML 2024</a>) introduces a taxonomy
for systematizing these attacks and demonstrates the importance of
careful evaluations that consider adversary resources and threat
models.</p>
<h3 id="taxonomy-for-black-box-attacks-on-classifiers">Taxonomy for Black-Box Attacks on Classifiers</h3>
<p>We propose a new attack taxonomy organized around the threat model assumptions of an attack, using four separate dimensions to categorize assumptions made by each attack.</p>
<ul>
<li>
<p><strong>Query Access</strong>: access to the target model. Under <em>no interactive access</em>, there is no opportunity to query the target model interactively (e.g., transfer attacks). With <em>interactive access</em>, the adversary can interactively query the target model and adjust subsequent queries by leveraging its history of queries (e.g., query-based attacks).</p>
</li>
<li>
<p><strong>API Feedback</strong>: how much information the target model&rsquo;s API returns. We categorize APIs into <em>hard-label</em> (only label returned by API),  <em>top-K</em> (confidence scores for top-<em>k</em> predictions), or <em>complete confidence vector</em> (all confidence scores returned).</p>
</li>
<li>
<p><strong>Quality of Initial Auxiliary Data</strong>: overlap between the auxiliary data available to the attacker and the training data of the target model. We capture overlap via distributional similarity in either feature space (same/similar samples used) or the label space. <em>No overlap</em> is closest to real-world APIs, where knowledge about the target model’s training data is obfuscated and often proprietary. <em>Partial overlap</em> captures scenarios where the training data of the target model includes some publicly available datasets. <em>Complete overlap</em> occurs where auxiliary data is identical (same dataset or same underlying distribution) to the target model’s training data.</p>
</li>
<li>
<p><strong>Quantity of Auxiliary Data</strong>: does that adversary have enough data to train well-performing surrogate models, categorized as <em>insufficient</em> and <em>sufficient</em>.</p>
</li>
</ul>
<h3 id="insights-from-taxonomy">Insights from Taxonomy</h3>
<p>Our taxonomy, shown below in the table, highlights technical challenges in underexplored areas, especially where ample data is available but with limited overlap with the target model’s data distribution. This scenario is highly relevant in practice. Additionally, we found that only one attack (NES) explicitly optimizes for top-<em>k</em> prediction scores, a common scenario in API attacks. These gaps suggest both a knowledge and a technical gap, with substantial room for improving attacks in these settings.</p>
<center>
<a href="/images/blackboxsok2024/taxonomy_table.png"><img style="width: 95%" src="/images/blackboxsok2024/taxonomy_table.png" alt="Performance of top-_k_ attacks across queries"/></a>
</center>
<div class="caption">
Threat model taxonomy of black-box attacks. The first two columns correspond to the quality and quantity of the auxiliary data available to the attacker initially. The remaining columns distinguish threat models based on the type of access they have to the target model, and for adversaries who can submit queries to the target model, the information they receive from the API in response. The symbol ∅ above corresponds to areas in the threat-space that, to the best of our knowledge, are not considered by any attacks in the literature. The sub-category of w/ Pretrained Surrogate with “*” denotes that the corresponding attacks do not require auxiliary data, but the quality of data used to train the surrogate determines the corresponding cell.
</div>
<h1 id="heading"></h1>
<p>Our new top-<em>k</em> adaptation (figure below) demonstrates a significant improvement in performance over the existing baseline in the top-<em>k</em> setting, yet still fails to outperform more restrictive hard-label attacks in some settings, highlighting the need for further investigation.</p>
<center>
<a href="/images/blackboxsok2024/topk_comparison.png"><img style="width: 65%" src="/images/blackboxsok2024/topk_comparison.png" alt="Performance of top-_k_ attacks across queries"/></a>
</center>
<div class="caption">
<p>Comparison of top-<em>k</em> attacks. Square: top-<em>k</em> is our proposed adaption of the Square Attack for the top-<em>k</em> setting. NES: top-<em>k</em> is the current state-of-the-art attack. SignFlip is a more restrictive hard-label attack.</p>
</div>
<p>See the <a href="https://arxiv.org/abs/2310.17534">full paper</a> for details on how the attacks were adapted.</p>
<h3 id="rethinking-baseline-comparisons">Rethinking baseline comparisons</h3>
<p>Our study revealed that current evaluations often fail to align with what adversaries actually care about. We advocate for time-based comparisons of attacks, emphasizing their practical effectiveness within given constraints. This approach reveals that some attacks achieve higher success rates when normalized for time.</p>
<center>
<a href="/images/blackboxsok2024/same_iters_vs_same_time_targeted_densenet.png"><img style="width: 95%" src="/images/blackboxsok2024/same_iters_vs_same_time_targeted_densenet.png" alt="ASR for various attacks, compared based on iterations (left) and time (right)"/></a>
</center>
<div class="caption">
ASR (y-axis) for various targeted attacks on DenseNet201 models, varying across iterations (a) and time (b). All attacks on the left are run for 100 iterations, while attacks on the right are run for 30 minutes per batch. ASR at each iteration is computed using adversarial examples at that iteration. ASR at 40 iterations are marked with a star for each attack.
</div>
<h1 id="heading-1"></h1>
<h3 id="takeaways">Takeaways</h3>
<p>The paper underscores many unexplored settings in black-box adversarial attacks, particularly emphasizing the significance of meticulous evaluation and experimentation. A critical insight is the existence of many realistic threat models that haven&rsquo;t been investigated, suggesting both a knowledge and a technical gap in current research. Considering the rapid evolution and increasing complexity of attack strategies, carefuly evaluation and consideration of the attack setting becomes even more pertinent. These findings indicate a need for more comprehensive and nuanced approaches to understanding and mitigating black-box attacks in real-world scenarios.</p>
<h3 id="paper">Paper</h3>
<p>Fnu Suya*, Anshuman Suri*, Tingwei Zhang, Jingtao Hong, Yuan Tian, David Evans. <a href="https://arxiv.org/abs/2310.17534"><em>SoK: Pitfalls in Evaluating Black-Box Attacks</em></a>. In <a href="https://satml.org/">IEEE Conference on Secure and Trustworthy Machine Learning</a> (SaTML). Toronto, 9–11 April 2024. <a href="https://arxiv.org/abs/2310.17534">[arXiv]</a></p>
<p><sub>* Equal contribution</sub></p>
<p>Code: <a href="https://github.com/iamgroot42/blackboxsok">https://github.com/iamgroot42/blackboxsok</a></p>
<h3 id="talk-at-satml-2024">Talk at SaTML 2024</h3>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/ui4HMGe3aUs?si=M2A-uD77s4BdhXPR" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</center>

      </div>

      <meta itemprop="wordCount" content="851">
      <meta itemprop="datePublished" content="2023-12-20">
      <meta itemprop="url" content="//uvasrg.github.io/sok-pitfalls-in-evaluating-black-box-attacks/">
    </article>

    <ul class="pagination" role="navigation" aria-label="Pagination" style="margin-top:32px;">
      
      <li class="arrow" aria-disabled="true"><a href="//uvasrg.github.io/neurips-2023-what-distributions-are-robust-to-poisoning-attacks/">&laquo; <em>Previous<span class="show-for-sr"> page</span></em>: NeurIPS 2023: What Distributions are Robust to Poisoning Attacks?</a></li>
      
      
      <li class="arrow" aria-disabled="true"><a href="//uvasrg.github.io/do-membership-inference-attacks-work-on-large-language-models/"><em>Next<span class="show-for-sr"> page</span></em>: Do Membership Inference Attacks Work on Large Language Models?&nbsp;&raquo;</a></li>
      
    </ul>
</div>

    </main>
    
    
<footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>


    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
