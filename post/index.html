<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Posts | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
        <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
	      
	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      

<div class="container">
<div class="content">
<div class="row">
    
    
    <h2><a href="/une-exp%C3%A9rience-immersive-et-enrichissante/">Une expérience immersive et enrichissante</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2025-01-31 00:00:00 &#43;0000 UTC" itemprop="datePublished">31 January 2025</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/visual-cryptography">visual cryptography</a>
    
  </span>
  
  
</div>


<p>I had a chance to talk (over zoom) about visual cryptography to students in an English class in a French high school in Spain!</p>
<center>
<img src="/images/frenchcrypto.png" width="80%"><br>
<p><a href="https://lyceebelair.es/nouvelles/non-classe/cryptography/">School Website Post</a></p>
</center>

	

    
    <h2><a href="/reassessing-emnlp-2024s-best-paper-does-divergence-based-calibration-for-membership-inference-attacks-hold-up/">Reassessing EMNLP 2024’s Best Paper: Does Divergence-Based Calibration for Membership Inference Attacks Hold Up?</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-11-26 00:00:00 &#43;0000 UTC" itemprop="datePublished">26 November 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/membership-inference">membership inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/llm">LLM</a>
    
  </span>
  
  
</div>


<p><a href="https://www.anshumansuri.com/">Anshuman Suri</a> and <a href="https://pratyushmaini.github.io/">Pratyush Maini</a> wrote a blog about the EMNLP 2024 best paper award winner: <a href="https://www.anshumansuri.com/blog/2024/calibrated-mia/"><em>Reassessing EMNLP 2024’s Best Paper: Does Divergence-Based Calibration for Membership Inference Attacks Hold Up?</em></a>.</p>
<p>As we explored in <a href="https://uvasrg.github.io/do-membership-inference-attacks-work-on-large-language-models/"><em>Do Membership Inference Attacks Work on Large Language Models?</em></a>, to test a membership inference attack it is essentail to have a candidate set where the members and non-members are from the same distribution. If the distributions are different, the ability of an attack to distinguish members and non-members is indicative of distribution inference, not necessarily membership inference.</p>
<p class="text-right"><a href="/reassessing-emnlp-2024s-best-paper-does-divergence-based-calibration-for-membership-inference-attacks-hold-up/">Read More…</a></p>
	

    
    <h2><a href="/common-way-to-test-for-leaks-in-large-language-models-may-be-flawed/">Common Way To Test for Leaks in Large Language Models May Be Flawed</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-11-13 00:00:00 &#43;0000 UTC" itemprop="datePublished">13 November 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/membership-inference">membership inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/llm">LLM</a>
    
  </span>
  
  
</div>


<p>UVA News has an article on our LLM membership inference work:
<a href="https://engineering.virginia.edu/news-events/news/common-way-test-leaks-large-language-models-may-be-flawed"><em>Common Way To Test for Leaks in Large Language Models May Be Flawed: UVA Researchers Collaborated To Study the Effectiveness of Membership Inference Attacks</em></a>, Eric Williamson, 13 November 2024.</p>

	

    
    <h2><a href="/meet-professor-suya/">Meet Professor Suya!</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-10-10 00:00:00 &#43;0000 UTC" itemprop="datePublished">10 October 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>
    
  </span>
  
  
</div>


<center>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Meet Assistant Professor Fnu Suya. His research interests include the application of machine learning techniques to security-critical applications and the vulnerabilities of machine learning models in the presence of adversaries, generally known as trustworthy machine learning. <a href="https://t.co/8R63QSN8aO">pic.twitter.com/8R63QSN8aO</a></p>&mdash; EECS (@EECS_UTK) <a href="https://twitter.com/EECS_UTK/status/1843293955158593547?ref_src=twsrc%5Etfw">October 7, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>
	

    
    <h2><a href="/poisoning-llms/">Poisoning LLMs</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-08-21 00:00:00 &#43;0000 UTC" itemprop="datePublished">21 August 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/large-language-models">large language models</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/copilot">copilot</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning">poisoning</a>
    
  </span>
  
  
</div>


<p>I&rsquo;m quoted in this story by Rob Lemos about poisoning code models (the <a href="https://www.usenix.org/conference/usenixsecurity24/presentation/yan">CodeBreaker</a> paper in USENIX Security 2024 by Shenao Yan, Shen Wang, Yue Duan, Hanbin Hong, Kiho Lee, Doowon Kim, and Yuan Hong), that considers a similar threat to our <a href="https://uvasrg.github.io/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/">TrojanPuzzle</a> work:</p>
<p><a href="https://www.darkreading.com/application-security/researchers-turn-code-completion-llms-into-attack-tools"><em>Researchers Highlight How Poisoned LLMs Can Suggest Vulnerable Code</em></a><br>
Dark Reading, 20 August 2024</p>
<blockquote>
CodeBreaker uses code transformations to create vulnerable code that continues to function as expected, but that will not be detected by major static analysis security testing. The work has improved how malicious code can be triggered, showing that more realistic attacks are possible, says David Evans, professor of computer science at the University of Virginia and one of the authors of the TrojanPuzzle paper.
...
Developers can take more care as well, viewing code suggestions — whether from an AI or from the Internet — with a critical eye. In addition, developers need to know how to construct prompts to produce more secure code.  
<p>Yet, developers need their own tools to detect potentially malicious code, says the University of Virginia&rsquo;s Evans.</p>
<p class="text-right"><a href="/poisoning-llms/">Read More…</a></p>
	

    
    <h2><a href="/the-mismeasure-of-man-and-models/">The Mismeasure of Man and Models</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-08-11 00:00:00 &#43;0000 UTC" itemprop="datePublished">11 August 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/hannah-chen">Hannah Chen</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yangfeng-ji">Yangfeng Ji</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/auditing">auditing</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/llms">LLMs</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bias">bias</a>
    
  </span>
  
  
</div>


<h1 id="evaluating-allocational-harms-in-large-language-models">Evaluating Allocational Harms in Large Language Models</h1>
<p><strong>Blog post written by <a href="https://hannahxchen.github.io/">Hannah Chen</a></strong></p>
<p>Our work considers <i>allocational harms</i> that arise when model predictions are used to distribute scarce resources or opportunities.</p>
<h2 id="current-bias-metrics-do-not-reliably-reflect-allocation-disparities">Current Bias Metrics Do Not Reliably Reflect Allocation Disparities</h2>
<p>Several methods have been proposed to audit large language models (LLMs) for bias when used in critical decision-making, such as resume screening for hiring. Yet, these methods focus on <i>predictions</i>, without considering how the predictions are used to make <i>decisions</i>. In many settings, making decisions involve prioritizing options due to limited resource constraints. We find that prediction-based evaluation methods, which measure bias as the <i>average performance gap</i> (δ) in prediction outcomes, do not reliably reflect disparities in allocation decision outcomes.</p>
<p class="text-right"><a href="/the-mismeasure-of-man-and-models/">Read More…</a></p>
	

    
    <h2><a href="/googles-trail-of-crumbs/">Google&#39;s Trail of Crumbs</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-08-05 00:00:00 &#43;0000 UTC" itemprop="datePublished">5 August 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/google">Google</a>
    
  </span>
  
  
</div>


<p>Matt Stoller published <a href="https://www.thebignewsletter.com/p/googles-trail-of-crumbs">my essay on Google&rsquo;s decision to abandon its Privacy Sandbox Initiative</a> in his Big newsletter:</p>
<center>
<a href="https://www.thebignewsletter.com/p/googles-trail-of-crumbs">
<div class="substack-post-embed"><p lang="en">Google's Trail of Crumbs by Matt Stoller</p><p>Google is too big to get rid of cookies. Even when it wants to protect users, it can't.</p><a data-post-link href="https://www.thebignewsletter.com/p/googles-trail-of-crumbs">Read on Substack</a></div><script async src="https://substack.com/embedjs/embed.js" charset="utf-8"></script></a>
</center>
<p>For more technical background on this, see Minjun&rsquo;s paper: <a href="https://arxiv.org/abs/2405.08102"><em>Evaluating Google&rsquo;s Protected Audience Protocol</em></a> in PETS 2024.</p>

	

    
    <h2><a href="/technology-us-authorities-survey-ai-ecosystem-through-antitrust-lens/">Technology: US authorities survey AI ecosystem through antitrust lens</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-08-04 00:00:00 &#43;0000 UTC" itemprop="datePublished">4 August 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/artificial-intelligence">artificial intelligence</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/machine-learning">machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anti-trust">anti-trust</a>
    
  </span>
  
  
</div>


<p>I&rsquo;m quoted in this article for the International Bar Association:</p>
<center>
<p><a href="https://www.ibanet.org/technology-us-authorities-survey-ai-ecosystem-through-antitrust-lens"><em>Technology: US authorities survey AI ecosystem through antitrust lens</em></a><br>
William Roberts, IBA US Correspondent<br>
Friday 2 August 2024</p>
</center>
<blockquote>
Antitrust authorities in the US are targeting the new frontier of artificial intelligence (AI) for potential enforcement action.
<p>&hellip;</p>
<p>Jonathan Kanter, Assistant Attorney General for the Antitrust Division of the DoJ, warns that the government sees ‘structures and trends in AI that should give us pause’. He says that AI relies on massive amounts of data and computing power, which can give already dominant companies a substantial advantage. ‘Powerful network and feedback effects’ may enable dominant companies to control these new markets, Kanter adds.</p>
<p class="text-right"><a href="/technology-us-authorities-survey-ai-ecosystem-through-antitrust-lens/">Read More…</a></p>
	

    
    <h2><a href="/john-guttag-birthday-celebration/">John Guttag Birthday Celebration</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-08-01 00:00:00 &#43;0000 UTC" itemprop="datePublished">1 August 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/john-guttag">John Guttag</a>
    
  </span>
  
  
</div>


<p>Maggie Makar organized a celebration for the 75th birthday of my PhD advisor, <a href="https://people.csail.mit.edu/guttag/">John Guttag</a>.</p>
<p>I wasn&rsquo;t able to attend in person, unfortunately, but the occasion provided an opportunity to create a poster that looks back on what I&rsquo;ve done since I started working with John over 30 years ago.</p>
<center>
<a href="/images/poster-for-jvg.pdf" style="border-width:2px; border-color:blue">
<img src="/images/poster-for-jvg.png" width="90%" class="image-shadow">
</a>
</center>

	

    
    <h2><a href="/congratulations-dr.-suri/">Congratulations, Dr. Suri!</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-07-18 00:00:00 &#43;0000 UTC" itemprop="datePublished">18 July 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/membership-inference">membership inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/phd-defense">PhD defense</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/alina-oprea">Alina Oprea</a>
    
  </span>
  
  
</div>


<p>Congratulations to <a href="https://www.anshumansuri.com/">Anshuman Suri</a> for successfully defending his <a href="https://libraetd.lib.virginia.edu/public_view/2227mr11j">PhD thesis</a>!</p>
<center>
<img src="/images/suri-phd.png" width="70%"><br>
Tianhao Wang, Dr. Anshuman Suri, Nando Fioretto, Cong Shen<br>
On Screen: David Evans, Giuseppe Ateniese
<br>
</center>
<h2 id="heading"></h2>
<h2 id="heading-1"></h2>
<center>
<em>
Inference Privacy in Machine Learning
</em>
</center>
<h2 id="heading-2"></h2>
<p>Using machine learning models comes at the risk of leaking information about data used in their training and deployment. This leakage can expose sensitive information about properties of the underlying data distribution, data from participating users, or even individual records in the training data. In this dissertation, we develop and evaluate novel methods to quantify and audit such information disclosure at three granularities: distribution, user, and record.</p>
<p class="text-right"><a href="/congratulations-dr.-suri/">Read More…</a></p>
	

    
    <div class="row">
  <div class="column small-12">
    <ul class="pagination" role="navigation" aria-label="Pagination">
      
      <li class="arrow" aria-disabled="true"><a href="/post/page/2/">&laquo; <em>Older<span class="show-for-sr"> blog entries</span></em></a></li>
      
      <li><span>Page 1 of 10</span></li>      
      
    </ul>    
    All Posts by <a href="//uvasrg.github.io/categories">Category</a> or <a href="//uvasrg.github.io/tags">Tags</a>.

  </div>
</div>

</div>
</div>
</div>

    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
