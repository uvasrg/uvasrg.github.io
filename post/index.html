<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Posts | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
        <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
	      
	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      

<div class="container">
<div class="content">
<div class="row">
    
    
    <h2><a href="/googles-trail-of-crumbs/">Google&#39;s Trail of Crumbs</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-08-05 00:00:00 &#43;0000 UTC" itemprop="datePublished">5 August 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/google">Google</a>
    
  </span>
  
  
</div>


<p>Matt Stoller published my essay on Google&rsquo;s decision to abandon its Privacy Sandbox Initiative in his Big newsletter:</p>
<center>
<a href="https://www.thebignewsletter.com/p/googles-trail-of-crumbs">
<div class="substack-post-embed"><p lang="en">Google's Trail of Crumbs by Matt Stoller</p><p>Google is too big to get rid of cookies. Even when it wants to protect users, it can't.</p><a data-post-link href="https://www.thebignewsletter.com/p/googles-trail-of-crumbs">Read on Substack</a></div><script async src="https://substack.com/embedjs/embed.js" charset="utf-8"></script></a>
</center>
<p>For more technical background on this, see Minjun&rsquo;s paper: <a href="https://arxiv.org/abs/2405.08102"><em>Evaluating Google&rsquo;s Protected Audience Protocol</em></a> in PETS 2024.</p>

	

    
    <h2><a href="/technology-us-authorities-survey-ai-ecosystem-through-antitrust-lens/">Technology: US authorities survey AI ecosystem through antitrust lens</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-08-04 00:00:00 &#43;0000 UTC" itemprop="datePublished">4 August 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/artificial-intelligence">artificial intelligence</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/machine-learning">machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anti-trust">anti-trust</a>
    
  </span>
  
  
</div>


I&rsquo;m quoted in this article for the International Bar Association:
Technology: US authorities survey AI ecosystem through antitrust lens
William Roberts, IBA US Correspondent
Friday 2 August 2024
Antitrust authorities in the US are targeting the new frontier of artificial intelligence (AI) for potential enforcement action. &hellip;
Jonathan Kanter, Assistant Attorney General for the Antitrust Division of the DoJ, warns that the government sees ‘structures and trends in AI that should give us pause’.
<p class="text-right"><a href="/technology-us-authorities-survey-ai-ecosystem-through-antitrust-lens/">Read More…</a></p>
	

    
    <h2><a href="/john-guttag-birthday-celebration/">John Guttag Birthday Celebration</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-08-01 00:00:00 &#43;0000 UTC" itemprop="datePublished">1 August 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/john-guttag">John Guttag</a>
    
  </span>
  
  
</div>


<p>Maggie Makar organized a celebration for the 75th birthday of my PhD advisor, <a href="https://people.csail.mit.edu/guttag/">John Guttag</a>.</p>
<p>I wasn&rsquo;t able to attend in person, unfortunately, but the occasion provided an opportunity to create a poster that looks back on what I&rsquo;ve done since I started working with John over 30 years ago.</p>
<center>
<a href="/images/poster-for-jvg.pdf" style="border-width:2px; border-color:blue">
<img src="/images/poster-for-jvg.png" width="90%" class="image-shadow">
</a>
</center>

	

    
    <h2><a href="/congratulations-dr.-suri/">Congratulations, Dr. Suri!</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-07-18 00:00:00 &#43;0000 UTC" itemprop="datePublished">18 July 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/membership-inference">membership inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/phd-defense">PhD defense</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/alina-oprea">Alina Oprea</a>
    
  </span>
  
  
</div>


Congratulations to Anshuman Suri for successfully defending his PhD thesis!
Tianhao Wang, Dr. Anshuman Suri, Nando Fioretto, Cong Shen
On Screen: David Evans, Giuseppe Ateniese Inference Privacy in Machine Learning Using machine learning models comes at the risk of leaking information about data used in their training and deployment. This leakage can expose sensitive information about properties of the underlying data distribution, data from participating users, or even individual records in the training data.
<p class="text-right"><a href="/congratulations-dr.-suri/">Read More…</a></p>
	

    
    <h2><a href="/graduation-2024/">Graduation 2024</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-05-29 00:00:00 &#43;0000 UTC" itemprop="datePublished">29 May 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/josephine-lamp">Josephine Lamp</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/graudation">graudation</a>
    
  </span>
  
  
</div>


<center>
<a href="/images/graduation2024/PXL_20240517_175539174-fish.jpg""><img src="/images/graduation2024/PXL_20240517_175539174-fish.jpg" width="85%"></img></a>
</center>
<center>
<h1 id="congratulations-to-our-two-phd-graduates">Congratulations to our two PhD graduates!</h1>
</center>
<p>Suya will be joining the University of Tennessee at Knoxville as an Assistant Professor.</p>
<p>Josie will be building a medical analytics research group at Dexcom.</p>
<center>
<a href="/images/graduation2024/IMG_5973.png"><img src="/images/graduation2024/IMG_5973.png" width="85%"></img></a>
</center>
<h2 id="heading"></h2>
<center>
<a href="/images/graduation2024/IMG_5941.png"><img src="/images/graduation2024/IMG_5941.png" width="60%"></img></a>
&nbsp;<a href="/images/graduation2024/IMG_5951.png"><img src="/images/graduation2024/IMG_5951.png" width="60%"></img></a>&nbsp;
<a href="/images/graduation2024/IMG_5962.png"><img src="/images/graduation2024/IMG_5962.png" width="60%"></img></a>
</center>

	

    
    <h2><a href="/satml-talk-sok-pitfalls-in-evaluating-black-box-attacks/">SaTML Talk: SoK: Pitfalls in Evaluating Black-Box Attacks</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-04-22 00:00:00 &#43;0000 UTC" itemprop="datePublished">22 April 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/tingwei-zhang">Tingwei Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jingtao-hong">Jingtao Hong</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yuan-tian">Yuan Tian</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/satml">SaTML</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/black-box-adversarial-attacks">black-box adversarial attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/systemization-of-knowledge">systemization of knowledge</a>
    
  </span>
  
  
</div>


<p><a href="https://www.anshumansuri.com/">Anshuman Suri</a>&rsquo;s talk at <a href="https://satml.org/">IEEE Conference on Secure and Trustworthy Machine Learning</a> (SaTML) is now available:</p>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/ui4HMGe3aUs?si=M2A-uD77s4BdhXPR" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</center>
<p>See the <a href="https://uvasrg.github.io/sok-pitfalls-in-evaluating-black-box-attacks/">earlier blog post</a> for more on the work, and the paper at <a href="https://arxiv.org/abs/2310.17534">https://arxiv.org/abs/2310.17534</a>.</p>

	

    
    <h2><a href="/congratulations-dr.-lamp/">Congratulations, Dr. Lamp!</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-03-20 00:00:00 &#43;0000 UTC" itemprop="datePublished">20 March 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/josephine-lamp">Josephine Lamp</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/phd-defense">PhD defense</a>
    
  </span>
  
  
</div>


Tianhao Wang (Committee Chair), Miaomiao Zhang, Lu Feng (Co-Advisor), Dr. Josie Lamp, David Evans
On screen: Sula Mazimba, Rich Nguyen, Tingting Zhu Congratulations to Josephine Lamp for successfully defending her PhD thesis!
Trustworthy Clinical Decision Support Systems for Medical Trajectories The explosion of medical sensors and wearable devices has resulted in the collection of large amounts of medical trajectories. Medical trajectories are time series that provide a nuanced look into patient conditions and their changes over time, allowing for a more fine-grained understanding of patient health.
<p class="text-right"><a href="/congratulations-dr.-lamp/">Read More…</a></p>
	

    
    <h2><a href="/do-membership-inference-attacks-work-on-large-language-models/">Do Membership Inference Attacks Work on Large Language Models?</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-03-05 00:00:00 &#43;0000 UTC" itemprop="datePublished">5 March 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/llms">LLMs</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/michael-duan">Michael Duan</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/niloofar-mireshghallah">Niloofar Mireshghallah</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/sewon-min">Sewon Min</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/weijia-shi">Weijia Shi</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/luke-zettlemoyer">Luke Zettlemoyer</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yulia-tsvetkov">Yulia Tsvetkov</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yejin-choi">Yejin Choi</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/hannaneh-hajishirzi">Hannaneh Hajishirzi</a>
    
  </span>
  
  
</div>


MIMIR logo. Image credit: GPT-4 + DALL-E Paper Code Data Membership inference attacks (MIAs) attempt to predict whether a particular datapoint is a member of a target model&rsquo;s training data. Despite extensive research on traditional machine learning models, there has been limited work studying MIA on the pre-training data of large language models (LLMs).
We perform a large-scale evaluation of MIAs over a suite of language models (LMs) trained on the Pile, ranging from 160M to 12B parameters.
<p class="text-right"><a href="/do-membership-inference-attacks-work-on-large-language-models/">Read More…</a></p>
	

    
    <h2><a href="/sok-pitfalls-in-evaluating-black-box-attacks/">SoK: Pitfalls in Evaluating Black-Box Attacks</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-12-20 00:00:00 &#43;0000 UTC" itemprop="datePublished">20 December 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/tingwei-zhang">Tingwei Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jingtao-hong">Jingtao Hong</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yuan-tian">Yuan Tian</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/satml">SaTML</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/black-box-adversarial-attacks">black-box adversarial attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/systemization-of-knowledge">systemization of knowledge</a>
    
  </span>
  
  
</div>


Post by Anshuman Suri and Fnu Suya
Much research has studied black-box attacks on image classifiers, where adversaries generate adversarial examples against unknown target models without having access to their internal information. Our analysis of over 164 attacks (published in 102 major security, machine learning and security conferences) shows how these works make different assumptions about the adversary’s knowledge.
The current literature lacks cohesive organization centered around the threat model. Our SoK paper (to appear at IEEE SaTML 2024) introduces a taxonomy for systematizing these attacks and demonstrates the importance of careful evaluations that consider adversary resources and threat models.
<p class="text-right"><a href="/sok-pitfalls-in-evaluating-black-box-attacks/">Read More…</a></p>
	

    
    <h2><a href="/neurips-2023-what-distributions-are-robust-to-poisoning-attacks/">NeurIPS 2023: What Distributions are Robust to Poisoning Attacks?</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-12-07 00:00:00 &#43;0000 UTC" itemprop="datePublished">7 December 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/indiscriminate-poisoning-attacks">indiscriminate poisoning attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/susceptibility-variation">susceptibility variation</a>
    
  </span>
  
  
</div>


Post by Fnu Suya
Data poisoning attacks are recognized as a top concern in the industry [1]. We focus on conventional indiscriminate data poisoning attacks, where an adversary injects a few crafted examples into the training data with the goal of increasing the test error of the induced model. Despite recent advances, indiscriminate poisoning attacks on large neural networks remain challenging [2]. In this work (to be presented at NeurIPS 2023), we revisit the vulnerabilities of more extensively studied linear models under indiscriminate poisoning attacks.
<p class="text-right"><a href="/neurips-2023-what-distributions-are-robust-to-poisoning-attacks/">Read More…</a></p>
	

    
    <div class="row">
  <div class="column small-12">
    <ul class="pagination" role="navigation" aria-label="Pagination">
      
      <li class="arrow" aria-disabled="true"><a href="/post/page/2/">&laquo; <em>Older<span class="show-for-sr"> blog entries</span></em></a></li>
      
      <li><span>Page 1 of 9</span></li>      
      
    </ul>    
    All Posts by <a href="//uvasrg.github.io/categories">Category</a> or <a href="//uvasrg.github.io/tags">Tags</a>.

  </div>
</div>

</div>
</div>
</div>

    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
