<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Posts | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
        <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
	      
	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      

<div class="container">
<div class="content">
<div class="row">
    
    
    <h2><a href="/graduation-2024/">Graduation 2024</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-05-29 00:00:00 &#43;0000 UTC" itemprop="datePublished">29 May 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/josephine-lamp">Josephine Lamp</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/graudation">graudation</a>
    
  </span>
  
  
</div>


<center>
<a href="/images/graduation2024/PXL_20240517_175539174-fish.jpg""><img src="/images/graduation2024/PXL_20240517_175539174-fish.jpg" width="85%"></img></a>
</center>
<center>
<h1 id="congratulations-to-our-two-phd-graduates">Congratulations to our two PhD graduates!</h1>
</center>
<p>Suya will be joining the University of Tennessee at Knoxville as an Assistant Professor.</p>
<p>Josie will be building a medical analytics research group at Dexcom.</p>
<center>
<a href="/images/graduation2024/IMG_5973.png"><img src="/images/graduation2024/IMG_5973.png" width="85%"></img></a>
</center>
<h2 id="heading"></h2>
<center>
<a href="/images/graduation2024/IMG_5941.png"><img src="/images/graduation2024/IMG_5941.png" width="60%"></img></a>
&nbsp;<a href="/images/graduation2024/IMG_5951.png"><img src="/images/graduation2024/IMG_5951.png" width="60%"></img></a>&nbsp;
<a href="/images/graduation2024/IMG_5962.png"><img src="/images/graduation2024/IMG_5962.png" width="60%"></img></a>
</center>

	

    
    <h2><a href="/satml-talk-sok-pitfalls-in-evaluating-black-box-attacks/">SaTML Talk: SoK: Pitfalls in Evaluating Black-Box Attacks</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-04-22 00:00:00 &#43;0000 UTC" itemprop="datePublished">22 April 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/tingwei-zhang">Tingwei Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jingtao-hong">Jingtao Hong</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yuan-tian">Yuan Tian</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/satml">SaTML</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/black-box-adversarial-attacks">black-box adversarial attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/systemization-of-knowledge">systemization of knowledge</a>
    
  </span>
  
  
</div>


<p><a href="https://www.anshumansuri.me/">Anshuman Suri</a>&rsquo;s talk at <a href="https://satml.org/">IEEE Conference on Secure and Trustworthy Machine Learning</a> (SaTML) is now available:</p>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/ui4HMGe3aUs?si=M2A-uD77s4BdhXPR" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</center>
<p>See the <a href="https://uvasrg.github.io/sok-pitfalls-in-evaluating-black-box-attacks/">earlier blog post</a> for more on the work, and the paper at <a href="https://arxiv.org/abs/2310.17534">https://arxiv.org/abs/2310.17534</a>.</p>

	

    
    <h2><a href="/congratulations-dr.-lamp/">Congratulations, Dr. Lamp!</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-03-20 00:00:00 &#43;0000 UTC" itemprop="datePublished">20 March 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/josephine-lamp">Josephine Lamp</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/phd-defense">PhD defense</a>
    
  </span>
  
  
</div>


Tianhao Wang (Committee Chair), Miaomiao Zhang, Lu Feng (Co-Advisor), Dr. Josie Lamp, David Evans
On screen: Sula Mazimba, Rich Nguyen, Tingting Zhu Congratulations to Josephine Lamp for successfully defending her PhD thesis!
Trustworthy Clinical Decision Support Systems for Medical Trajectories The explosion of medical sensors and wearable devices has resulted in the collection of large amounts of medical trajectories. Medical trajectories are time series that provide a nuanced look into patient conditions and their changes over time, allowing for a more fine-grained understanding of patient health.
<p class="text-right"><a href="/congratulations-dr.-lamp/">Read More…</a></p>
	

    
    <h2><a href="/do-membership-inference-attacks-work-on-large-language-models/">Do Membership Inference Attacks Work on Large Language Models?</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-03-05 00:00:00 &#43;0000 UTC" itemprop="datePublished">5 March 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/llms">LLMs</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/michael-duan">Michael Duan</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/niloofar-mireshghallah">Niloofar Mireshghallah</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/sewon-min">Sewon Min</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/weijia-shi">Weijia Shi</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/luke-zettlemoyer">Luke Zettlemoyer</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yulia-tsvetkov">Yulia Tsvetkov</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yejin-choi">Yejin Choi</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/hannaneh-hajishirzi">Hannaneh Hajishirzi</a>
    
  </span>
  
  
</div>


MIMIR logo. Image credit: GPT-4 + DALL-E Paper Code Data Membership inference attacks (MIAs) attempt to predict whether a particular datapoint is a member of a target model&rsquo;s training data. Despite extensive research on traditional machine learning models, there has been limited work studying MIA on the pre-training data of large language models (LLMs).
We perform a large-scale evaluation of MIAs over a suite of language models (LMs) trained on the Pile, ranging from 160M to 12B parameters.
<p class="text-right"><a href="/do-membership-inference-attacks-work-on-large-language-models/">Read More…</a></p>
	

    
    <h2><a href="/sok-pitfalls-in-evaluating-black-box-attacks/">SoK: Pitfalls in Evaluating Black-Box Attacks</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-12-20 00:00:00 &#43;0000 UTC" itemprop="datePublished">20 December 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/tingwei-zhang">Tingwei Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jingtao-hong">Jingtao Hong</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yuan-tian">Yuan Tian</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/satml">SaTML</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/black-box-adversarial-attacks">black-box adversarial attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/systemization-of-knowledge">systemization of knowledge</a>
    
  </span>
  
  
</div>


Post by Anshuman Suri and Fnu Suya
Much research has studied black-box attacks on image classifiers, where adversaries generate adversarial examples against unknown target models without having access to their internal information. Our analysis of over 164 attacks (published in 102 major security, machine learning and security conferences) shows how these works make different assumptions about the adversary’s knowledge.
The current literature lacks cohesive organization centered around the threat model. Our SoK paper (to appear at IEEE SaTML 2024) introduces a taxonomy for systematizing these attacks and demonstrates the importance of careful evaluations that consider adversary resources and threat models.
<p class="text-right"><a href="/sok-pitfalls-in-evaluating-black-box-attacks/">Read More…</a></p>
	

    
    <h2><a href="/neurips-2023-what-distributions-are-robust-to-poisoning-attacks/">NeurIPS 2023: What Distributions are Robust to Poisoning Attacks?</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-12-07 00:00:00 &#43;0000 UTC" itemprop="datePublished">7 December 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/indiscriminate-poisoning-attacks">indiscriminate poisoning attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/susceptibility-variation">susceptibility variation</a>
    
  </span>
  
  
</div>


Post by Fnu Suya
Data poisoning attacks are recognized as a top concern in the industry [1]. We focus on conventional indiscriminate data poisoning attacks, where an adversary injects a few crafted examples into the training data with the goal of increasing the test error of the induced model. Despite recent advances, indiscriminate poisoning attacks on large neural networks remain challenging [2]. In this work (to be presented at NeurIPS 2023), we revisit the vulnerabilities of more extensively studied linear models under indiscriminate poisoning attacks.
<p class="text-right"><a href="/neurips-2023-what-distributions-are-robust-to-poisoning-attacks/">Read More…</a></p>
	

    
    <h2><a href="/adjectives-can-reveal-gender-biases-within-nlp-models/">Adjectives Can Reveal Gender Biases Within NLP Models</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-08-17 00:00:00 &#43;0000 UTC" itemprop="datePublished">17 August 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jason-briegel">Jason Briegel</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/hannah-chen">Hannah Chen</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/llms">LLMs</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/generative-ai">generative AI</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bias">bias</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/machine-learning">machine learning</a>
    
  </span>
  
  
</div>


Post by Jason Briegel and Hannah Chen
Because NLP models are trained with human corpora (and now, increasingly on text generated by other NLP models that were originally trained on human language), they are prone to inheriting common human stereotypes and biases. This is problematic, because with their growing prominence they may further propagate these stereotypes (Sun et al., 2019). For example, interest is growing in mitigating bias in the field of machine translation, where systems such as Google translate were observed to default to translating gender-neutral pronouns as male pronouns, even with feminine cues (Savoldi et al.
<p class="text-right"><a href="/adjectives-can-reveal-gender-biases-within-nlp-models/">Read More…</a></p>
	

    
    <h2><a href="/congratulations-dr.-suya/">Congratulations, Dr. Suya!</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-07-11 00:00:00 &#43;0000 UTC" itemprop="datePublished">11 July 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning">poisoning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/phd-defense">PhD defense</a>
    
  </span>
  
  
</div>


Congratulations to Fnu Suya for successfully defending his PhD thesis!
Suya will join the Unversity of Maryland as a MC2 Postdoctoral Fellow at the Maryland Cybersecurity Center this fall.
On the Limits of Data Poisoning Attacks Current machine learning models require large amounts of labeled training data, which are often collected from untrusted sources. Models trained on these potentially manipulated data points are prone to data poisoning attacks. My research aims to gain a deeper understanding on the limits of two types of data poisoning attacks: indiscriminate poisoning attacks, where the attacker aims to increase the test error on the entire dataset; and subpopulation poisoning attacks, where the attacker aims to increase the test error on a defined subset of the distribution.
<p class="text-right"><a href="/congratulations-dr.-suya/">Read More…</a></p>
	

    
    <h2><a href="/sok-let-the-privacy-games-begin-a-unified-treatment-of-data-inference-privacy-in-machine-learning/">SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-05-26 00:00:00 &#43;0000 UTC" itemprop="datePublished">26 May 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/ahmed-salem">Ahmed Salem</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/giovanni-cherubin">Giovanni Cherubin</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/boris-k%c3%b6pf">Boris Köpf</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/andrew-paverd">Andrew Paverd</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/shruti-tople">Shruti Tople</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/santiago-zanella-b%c3%a9guelin">Santiago Zanella-Béguelin</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/membership-inference">membership inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>
    
  </span>
  
  
</div>


<p>Our paper on the use of cryptographic-style games to model inference privacy is published in <a href="https://www.ieee-security.org/TC/SP2023/"><em>IEEE Symposium on Security and Privacy</em></a> (Oakland):</p>
<blockquote>
<a href="https://www.microsoft.com/en-us/research/people/t-salemahmed/>Ahmed Salem</a>, <a href="https://www.microsoft.com/en-us/research/people/gcherubin/">Giovanni Cherubin</a>, <a href="https://www.cs.virginia.edu/evans"/David Evans</a>, <a href="https://www.microsoft.com/en-us/research/people/bokoepf/">Boris Köpf</a>, <a href="https://www.microsoft.com/en-us/research/people/anpaverd/">Andrew Paverd</a>, <a href="https://www.anshumansuri.me/">Anshuman Suri</a>, <a href="https://www.microsoft.com/en-us/research/people/shtople/">Shruti Tople</a>, and <a href="https://www.microsoft.com/en-us/research/people/santiago/">Santiago Zanella-Béguelin</a>. <em>SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning</em>. IEEE Symposium on Security and Privacy, 2023. [<a href="https://arxiv.org/abs/2212.10986">Arxiv</a>]
</blockquote>
<h2 id="heading"></h2>
<center>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Tired of diverse definitions of machine learning privacy risks? Curious about game-based definitions? In our paper, we present privacy games as a tool for describing and analyzing privacy risks in machine learning. Join us on May 22nd, 11 AM <a href="https://twitter.com/IEEESSP?ref_src=twsrc%5Etfw">@IEEESSP</a> &#39;23 <a href="https://t.co/NbRuTmHyd2">https://t.co/NbRuTmHyd2</a> <a href="https://t.co/CIzsT7UY4b">pic.twitter.com/CIzsT7UY4b</a></p>&mdash; ahmed salem (@AhmedGaSalem) <a href="https://twitter.com/AhmedGaSalem/status/1658210153341001736?ref_src=twsrc%5Etfw">May 15, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

	

    
    <h2><a href="/cvpr-2023-manipulating-transfer-learning-for-property-inference/">CVPR 2023: Manipulating Transfer Learning for Property Inference</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-05-02 00:00:00 &#43;0000 UTC" itemprop="datePublished">2 May 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yulong-tian">Yulong Tian</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/transfer-learning">transfer learning</a>
    
  </span>
  
  
</div>


Manipulating Transfer Learning for Property Inference Transfer learning is a popular method to train deep learning models efficiently. By reusing parameters from upstream pre-trained models, the downstream trainer can use fewer computing resources to train downstream models, compared to training models from scratch.
The figure below shows the typical process of transfer learning for vision tasks:
However, the nature of transfer learning can be exploited by a malicious upstream trainer, leading to severe risks to the downstream trainer.
<p class="text-right"><a href="/cvpr-2023-manipulating-transfer-learning-for-property-inference/">Read More…</a></p>
	

    
    <div class="row">
  <div class="column small-12">
    <ul class="pagination" role="navigation" aria-label="Pagination">
      
      <li class="arrow" aria-disabled="true"><a href="/post/page/2/">&laquo; <em>Older<span class="show-for-sr"> blog entries</span></em></a></li>
      
      <li><span>Page 1 of 9</span></li>      
      
    </ul>    
    All Posts by <a href="//uvasrg.github.io/categories">Category</a> or <a href="//uvasrg.github.io/tags">Tags</a>.

  </div>
</div>

</div>
</div>
</div>

    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
