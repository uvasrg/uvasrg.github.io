<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Posts | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
        <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
	      
	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      

<div class="container">
<div class="content">
<div class="row">
    
    
    <h2><a href="/biml-what-machine-learnt-models-reveal/">BIML: What Machine Learnt Models Reveal</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-07-19 00:00:00 &#43;0000 UTC" itemprop="datePublished">19 July 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/biml">BIML</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/gary-mcgraw">Gary McGraw</a>
    
  </span>
  
  
</div>


<p>I gave a talk in the <a href="https://berryvilleiml.com/">Berryville Institute of Machine Learning in the Barn</a> series on <em>What Machine Learnt Models Reveal</em>, which is now available as an edited video:</p>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/zMM_y6VWSgA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>
<blockquote>
<p>David Evans, a professor of computer science researching security and privacy at the University of Virginia, talks about data leakage risk in ML systems and different approaches used to attack and secure models and datasets. Juxtaposing adversarial risks that target records and those aimed at attributes, David shows that differential privacy cannot capture all inference risks, and calls for more research based on privacy experiments aimed at both datasets and distributions.</p>
<p class="text-right"><a href="/biml-what-machine-learnt-models-reveal/">Read More…</a></p>
	

    
    <h2><a href="/congratulations-dr.-zhang/">Congratulations, Dr. Zhang!</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-07-19 00:00:00 &#43;0000 UTC" itemprop="datePublished">19 July 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/phd-defense">PhD defense</a>
    
  </span>
  
  
</div>


<p>Congratulations to Xiao Zhang for successfully defending his PhD thesis!</p>
<center>
<img src="/images/xiaozhangphd.png" width="75%">
<div class="caption">
Dr. Zhang and his PhD committee: Somesh Jha (University of Wisconsin), David Evans, Tom Fletcher; Tianxi&nbsp;Li&nbsp;(UVA&nbsp;Statistics), David&nbsp;Wu&nbsp;(UT&nbsp;Austin), Mohammad&nbsp;Mahmoody; Xiao&nbsp;Zhang.
</center>
<h2 id="heading"></h2>
<p>Xiao will join the <a href="https://cispa.de/en">CISPA Helmholtz Center for Information
Security</a> in Saarbrücken, Germany this fall as a
tenure-track faculty member.</p>
<h2 id="heading-1"></h2>
<center>
<em>From Characterizing Intrinsic Robustness to Adversarially Robust Machine Learning</em>
</center>
<h2 id="heading-2"></h2>
<p>The prevalence of adversarial examples raises questions about the
reliability of machine learning systems, especially for their
deployment in critical applications. Numerous defense mechanisms have
been proposed that aim to improve a machine learning system’s
robustness in the presence of adversarial examples. However, none of
these methods are able to produce satisfactorily robust models, even
for simple classification tasks on benchmarks. In addition to
empirical attempts to build robust models, recent studies have
identified intrinsic limitations for robust learning against
adversarial examples. My research aims to gain a deeper understanding
of why machine learning models fail in the presence of adversaries and
design ways to build better robust systems. In this dissertation, I
develop a concentration estimation framework to characterize the
intrinsic limits of robustness for typical classification tasks of
interest. The proposed framework leads to the discovery that compared
with the concentration of measure which was previously argued to be an
important factor, the existence of uncertain inputs may explain more
fundamentally the vulnerability of state-of-the-art
defenses. Moreover, to further advance our understanding of
adversarial examples, I introduce a notion of representation
robustness based on mutual information, which is shown to be related
to an intrinsic limit of model robustness for downstream
classification tasks. Finally in this dissertation, I advocate for a
need to rethink the current design goal of robustness and shed light
on ways to build better robust machine learning systems, potentially
escaping the intrinsic limits of robustness.</p>
<p class="text-right"><a href="/congratulations-dr.-zhang/">Read More…</a></p>
	

    
    <h2><a href="/iclr-2022-understanding-intrinsic-robustness-using-label-uncertainty/">ICLR 2022: Understanding Intrinsic Robustness Using Label Uncertainty</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-03-24 00:00:00 &#43;0000 UTC" itemprop="datePublished">24 March 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/iclr">ICLR</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/intrinsic-robustness">intrinsic robustness</a>
    
  </span>
  
  
</div>


<p>(Blog post written by <a href="https://xiao-zhang.net/">Xiao Zhang</a>)</p>
<p>Motivated by the empirical hardness of developing robust classifiers
against adversarial perturbations, researchers began asking the
question “<em>Does there even exist a robust classifier?</em>”. This is
formulated as the <strong><em>intrinsic robustness problem</em></strong> <a href="https://proceedings.neurips.cc/paper/2019/file/46f76a4bda9a9579eab38a8f6eabcda1-Paper.pdf">(Mahloujifar et
al.,
2019)</a>,
where the goal is to characterize the maximum adversarial robustness
possible for a given robust classification problem. Building upon the
connection between adversarial robustness and classifier’s error
region, it has been shown that if we restrict the search to the set of
imperfect classifiers, the intrinsic robustness problem can be reduced
to the <strong><em>concentration of measure problem</em></strong>.</p>
<p class="text-right"><a href="/iclr-2022-understanding-intrinsic-robustness-using-label-uncertainty/">Read More…</a></p>
	

    
    <h2><a href="/microsoft-research-summit-surprising-and-unsurprising-inference-risks-in-machine-learning/">Microsoft Research Summit: Surprising (and unsurprising) Inference Risks in Machine Learning</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2021-10-21 00:00:00 &#43;0000 UTC" itemprop="datePublished">21 October 2021</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bargav-jayaraman">Bargav Jayaraman</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/katherine-knipmeyer">Katherine Knipmeyer</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/microsoft">Microsoft</a>
    
  </span>
  
  
</div>


<p>Here are the slides for my talk at the <a href="https://www.microsoft.com/en-us/research/theme/confidential-computing/#workshops"><em>Practical and Theoretical Privacy of Machine Learning Training Pipelines</em></a>
Workshop at the Microsoft Research Summit (21 October 2021):</p>
   <center>
<a href="https://www.dropbox.com/s/1mfhbelv7qx4t3u/surprisinginferences.pdf?dl=0"><b>Surprising (and Unsurprising) Inference Risks in Machine Learning</b> [PDF]</a>
   </center>
<h2 id="heading"></h2>
<h2 id="heading-1"></h2>
<p>The work by Bargav Jayaraman (with Katherine Knipmeyer, Lingxiao Wang,
and Quanquan Gu) that I talked about on improving membership inference
attacks is described in more details here:</p>
<ul>
<li>
<p>Bargav Jayaraman, Lingxiao Wang, Katherine Knipmeyer, Quanquan Gu, David Evans. <a href="https://arxiv.org/abs/2005.10881"><em>Revisiting Membership Inference Under Realistic Assumptions</em></a> (PETS 2021).<br>
[<a href="/merlin-morgan-and-the-importance-of-thresholds-and-priors/">Blog</a>] [Code: <a href="https://github.com/bargavj/EvaluatingDPML"><em>https://github.com/bargavj/EvaluatingDPML</em></a>]</p>
<p class="text-right"><a href="/microsoft-research-summit-surprising-and-unsurprising-inference-risks-in-machine-learning/">Read More…</a></p>
	

    
    <h2><a href="/uva-news-article/">UVA News Article</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2021-09-28 00:00:00 &#43;0000 UTC" itemprop="datePublished">28 September 2021</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bargav-jayaraman">Bargav Jayaraman</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jack-prescott">Jack Prescott</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/katherine-knipmeyer">Katherine Knipmeyer</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>
    
  </span>
  
  
</div>


<p>UVA News has an article by Audra Book on our research on security and
privacy of machine learning (with some very nice quotes from several
students in the group, and me saying something positive about the
NSA!): <a href="https://engineering.virginia.edu/news/2021/09/computer-science-professor-david-evans-and-his-team-conduct-experiments-understand"><em>Computer science professor David Evans and his team conduct
experiments to understand security and privacy risks associated with
machine
learning</em></a>,
8 September 2021.</p>
<div class="articletext">
<p>David Evans, professor of computer science in the University of Virginia School of Engineering and Applied Science, is leading research to understand how machine learning models can be compromised.</p>
<p class="text-right"><a href="/uva-news-article/">Read More…</a></p>
	

    
    <h2><a href="/model-targeted-poisoning-attacks-with-provable-convergence/">Model-Targeted Poisoning Attacks with Provable Convergence</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2021-06-29 00:00:00 &#43;0000 UTC" itemprop="datePublished">29 June 2021</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/model-targeted-poisoning-attacks">model-targeted poisoning attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/saeed-mahloujifar">Saeed Mahloujifar</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yuan-tian">Yuan Tian</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/icml-2021">ICML 2021</a>
    
  </span>
  
  
</div>


<p>(Post by Sean Miller, using images adapted from Suya&rsquo;s talk slides)</p>
<h2 id="data-poisoning-attacks">Data Poisoning Attacks</h2>
<p>Machine learning models are often trained using data from untrusted
sources, leaving them open to poisoning attacks where adversaries use
their control over a small fraction of that training data to poison
the model in a particular way.</p>
<p>Most work on poisoning attacks is directly driven by an attacker&rsquo;s
objective, where the adversary chooses poisoning points that maximize
some target objective. Our work focuses on <em>model-targeted</em> poisoning
attacks, where the adversary splits the attack into choosing a target
model that satisfies the objective and then choosing poisoning points
that induce the target model.</p>
<p class="text-right"><a href="/model-targeted-poisoning-attacks-with-provable-convergence/">Read More…</a></p>
	

    
    <h2><a href="/on-the-risks-of-distribution-inference/">On the Risks of Distribution Inference</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2021-06-24 00:00:00 &#43;0000 UTC" itemprop="datePublished">24 June 2021</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/property-inference">property inference</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>
    
  </span>
  
  
</div>


<p>(Cross-post by <a href="https://www.anshumansuri.com/post/distr_infer">Anshuman Suri</a>)</p>
<p>Inference attacks seek to infer sensitive information about the training process of a revealed machine-learned model, most often about the training data.</p>
<p>Standard inference attacks (which we call “dataset inference attacks”)
aim to learn something about a particular record that may have been in
that training data. For example, in a membership inference attack
(Reza Shokri et al., <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7958568"><em>Membership Inference Attacks Against Machine
Learning
Models</em></a>, IEEE S&amp;P 2017),
the adversary aims to infer whether or not a particular record was
included in the training data.</p>
<p class="text-right"><a href="/on-the-risks-of-distribution-inference/">Read More…</a></p>
	

    
    <h2><a href="/chinese-translation-of-mpc-book/">Chinese Translation of MPC Book</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2021-06-07 00:00:00 &#43;0000 UTC" itemprop="datePublished">7 June 2021</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/secure-computation">secure computation</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/vladimir-kolesnikov">Vladimir Kolesnikov</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/mike-rosulek">Mike Rosulek</a>
    
  </span>
  
  
</div>


<a href="https://item.jd.com/13302742.html">
<img src="https://securecomputation.org/images/chinese-cover.png" width=25% align="right" style="margin-bottom: 12px; margin-left: 12px; margin-right: 20px; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);"></a>
<p>A Chinese translation of our <a href="//securecomputation.org"><em>A Pragmatic Introduction to Secure
Multi-Party Computation</em></a> book (by David
Evans, Vladimir Kolesnikov, and Mike Rosulek) is now available!</p>
<p>Thanks to Weiran Liu and Sengchao Ding for all the work they
did on the translation.</p>
<p>To order from JD.com: <a href="https://item.jd.com/13302742.html"><em>https://item.jd.com/13302742.html</em></a></p>
<p>(The English version of the book is still available for free download, from <a href="https://securecomputation.org"><em>https://securecomputation.org</em></a>.)</p>
<p class="text-right"><a href="/chinese-translation-of-mpc-book/">Read More…</a></p>
	

    
    <h2><a href="/iclr-dpml-2021-inference-risks-for-machine-learning/">ICLR DPML 2021: Inference Risks for Machine Learning</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2021-05-07 00:00:00 &#43;0000 UTC" itemprop="datePublished">7 May 2021</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bargav-jayaraman">Bargav Jayaraman</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/katherine-knipmeyer">Katherine Knipmeyer</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>
    
  </span>
  
  
</div>


<p>I gave an invited talk at the <a href="https://dp-ml.github.io/2021-workshop-ICLR/">Distributed and Private Machine Learning</a> (DPML) workshop at ICLR 2021 on <a href="https://iclr.cc/virtual/2021/workshop/2148#collapse3549"><em>Inference Risks for Machine Learning</em></a>.</p>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/zgSTsO1LKSs?controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></center>
<p>The talk mostly covers work by Bargav Jayaraman on evaluating privacy in
machine learning and connecting attribute inference and imputation, and recent work by Anshuman Suri on property inference.</p>

	

    
    <h2><a href="/how-to-hide-a-backdoor/">How to Hide a Backdoor</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2021-05-06 00:00:00 &#43;0000 UTC" itemprop="datePublished">6 May 2021</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/research">research</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yulong-tian">Yulong Tian</a>
    
  </span>
  
  
</div>


<p>The Register has an article on our recent work on <a href="https://arxiv.org/abs/2104.15129"><em>Stealthy Backdoors as Compression Artifacts</em></a>:
Thomas Claburn, <a href="https://www.theregister.com/AMP/2021/05/05/ai_backdoors/"><em>How to hide a backdoor in AI software — Neural networks can be aimed to misbehave when squeezed</em></a>, The Register, 5 May 2021.</p>

	

    
    <div class="row">
  <div class="column small-12">
    <ul class="pagination" role="navigation" aria-label="Pagination">
      
      <li class="arrow" aria-disabled="true"><a href="/post/page/5/">&laquo; <em>Older<span class="show-for-sr"> blog entries</span></em></a></li>
      
      <li><span>Page 4 of 10</span></li>      
      
      <li class="arrow" aria-disabled="true"><a href="/post/page/3/"><em>Newer<span class="show-for-sr"> blog entries</span></em>:&nbsp;&raquo;</a></li>
      
    </ul>    
    All Posts by <a href="//uvasrg.github.io/categories">Category</a> or <a href="//uvasrg.github.io/tags">Tags</a>.

  </div>
</div>

</div>
</div>
</div>

    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
