<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Posts | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
        <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
	      
	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      

<div class="container">
<div class="content">
<div class="row">
    
    
    <h2><a href="/congratulations-dr.-jayaraman/">Congratulations, Dr. Jayaraman!</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-12-02 00:00:00 &#43;0000 UTC" itemprop="datePublished">2 December 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bargav-jayaraman">Bargav Jayaraman</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/phd-defense">PhD defense</a>
    
  </span>
  
  
</div>


<p>Congratulations to Bargav Jayaraman for successfully <a href="https://engineering.virginia.edu/events/phd-defense-presentation-bargav-jayaraman">defending his PhD thesis</a>!</p>
<center>
<img src="/images/bargav-defense.jpg" width="65%">
<div class="caption"><center>
Dr. Jayaraman and his PhD committee: Mohammad&nbsp;Mahmoody, Quanquan&nbsp;Gu (UCLA Department of Computer Science, on screen), Yanjun&nbsp;Qi (Committee Chair, on screen), Denis&nbsp;Nekipelov (Department of Economics, on screen), and David Evans
</div>
</center>
<h2 id="heading"></h2>
<p>Bargav will join the Meta AI Lab in Menlo Park, CA as a post-doctoral researcher.</p>
<h2 id="heading-1"></h2>
<center>
<em>Analyzing the Leaky Cauldron: Inference Attacks on Machine Learning</em>
</center>
<h2 id="heading-2"></h2>
<p>Machine learning models have been shown to leak sensitive information about their training data. An adversary having access to the model can infer different types of sensitive information, such as learning if a particular individual&rsquo;s data is in the training set, extracting sensitive patterns like passwords in the training set, or predicting missing sensitive attribute values for partially known training records. This dissertation quantifies this privacy leakage. We explore inference attacks against machine learning models including membership inference, pattern extraction, and attribute inference. While our attacks give an empirical lower bound on the privacy leakage, we also provide a theoretical upper bound on the privacy leakage metrics. Our experiments across various real-world data sets show that the membership inference attacks can infer a subset of candidate training records with high attack precision, even in challenging cases where the adversary&rsquo;s candidate set is mostly non-training records. In our pattern extraction experiments, we show that an adversary is able to recover email ids, passwords and login credentials from large transformer-based language models. Our attribute inference adversary is able to use underlying training distribution information inferred from the model to confidently identify candidate records with sensitive attribute values. We further evaluate the privacy risk implication to individuals contributing their data for model training. Our findings suggest that different subsets of individuals are vulnerable to different membership inference attacks, and that some individuals are repeatedly identified across multiple runs of an attack. For attribute inference, we find that a subset of candidate records with a sensitive attribute value are correctly predicted by our white-box attribute inference attacks but would be misclassified by an imputation attack that does not have access to the target model. We explore different defense strategies to mitigate the inference risks, including approaches that avoid model overfitting such as early stopping and differential privacy, and approaches that remove sensitive data from the training. We find that differential privacy mechanisms can thwart membership inference and pattern extraction attacks, but even differential privacy fails to mitigate the attribute inference risks since the attribute inference attack relies on the distribution information leaked by the model whereas differential privacy provides no protection against leakage of distribution statistics.</p>
<p class="text-right"><a href="/congratulations-dr.-jayaraman/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/balancing-tradeoffs-between-fickleness-and-obstinacy-in-nlp-models/">Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-11-13 00:00:00 &#43;0000 UTC" itemprop="datePublished">13 November 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/hannah-chen">Hannah Chen</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yangfeng-ji">Yangfeng Ji</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-examples">adversarial examples</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/nlp">NLP</a>
    
  </span>
  
  
</div>


<p>Post by <a href="https://hannahxchen.github.io/">Hannah Chen</a>.</p>
<p>Our work on balanced adversarial training looks at how to train models
that are robust to two different types of adversarial examples:</p>
<p><a href="https://hannahxchen.github.io/">Hannah Chen</a>, <a href="http://yangfengji.net/">Yangfeng
Ji</a>, <a href="http://www.cs.virginia.edu/~evans/">David
Evans</a>. <em>Balanced Adversarial
Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP
Models</em>. In <a href="https://2022.emnlp.org/"><em>The 2022 Conference on Empirical Methods in Natural
Language Processing</em></a> (EMNLP), Abu Dhabi,
7-11 December 2022.  [<a href="https://arxiv.org/abs/2210.11498">ArXiv</a>]</p>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/xQH51lIVDyY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>
<h2 id="adversarial-examples">Adversarial Examples</h2>
<p>At the broadest level, an adversarial example is an input crafted intentionally to confuse a model. However, most work focus on the defintion as an input constructed by applying a small perturbation that preserves the ground truth label but changes model&rsquo;s output <a href="https://arxiv.org/abs/1412.6572">(Goodfellow et al., 2015)</a>. We refer it as a <strong>fickle adversarial example</strong>. On the other hand, attackers can target an opposite objective where the inputs are made with minimal changes that change the ground truth labels but retain model&rsquo;s predictions <a href="https://arxiv.org/abs/1811.00401">(Jacobsen et al., 2018)</a>. We refer these malicious inputs as <strong>obstinate adversarial examples</strong>.</p>
<p class="text-right"><a href="/balancing-tradeoffs-between-fickleness-and-obstinacy-in-nlp-models/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/best-submission-award-at-visxai-2022/">Best Submission Award at VISxAI 2022</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-10-18 00:00:00 &#43;0000 UTC" itemprop="datePublished">18 October 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/evan-rose">Evan Rose</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-syua">Fnu Syua</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning-attacks">poisoning attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>
    
  </span>
  
  
</div>


<p><a href="https://uvasrg.github.io/poisoning/"><em>Poisoning Attacks and Subpopulation Susceptibility</em></a> by Evan Rose, Fnu Suya, and David Evans won the Best Submission Award at the <a href="https://visxai.io/">5th Workshop on Visualization for AI Explainability</a>.</p>
<p>Undergraduate student Evan Rose led the work and presented it at VISxAI in Oklahoma City, 17 October 2022.</p>
<center>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Congratulations to <a href="https://twitter.com/hashtag/VISxAI?src=hash&amp;ref_src=twsrc%5Etfw">#VISxAI</a>&#39;s Best Submission Awards:<br><br>üèÜ K-Means Clustering: An Explorable Explainer by <a href="https://twitter.com/yizhe_ang?ref_src=twsrc%5Etfw">@yizhe_ang</a> <a href="https://t.co/BULW33WPzo">https://t.co/BULW33WPzo</a><br><br>üèÜ Poisoning Attacks and Subpopulation Susceptibility by Evan Rose, <a href="https://twitter.com/suyafnu?ref_src=twsrc%5Etfw">@suyafnu</a>, and <a href="https://twitter.com/UdacityDave?ref_src=twsrc%5Etfw">@UdacityDave</a> <a href="https://t.co/Z12D3PvfXu">https://t.co/Z12D3PvfXu</a><a href="https://twitter.com/hashtag/ieeevis?src=hash&amp;ref_src=twsrc%5Etfw">#ieeevis</a></p>&mdash; VISxAI (@VISxAI) <a href="https://twitter.com/VISxAI/status/1582085676857577473?ref_src=twsrc%5Etfw">October 17, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Next up is best submission award üèÖ winner, &quot;Poisoning Attacks and Subpopulation Susceptibility&quot; by Evan Rose, <a href="https://twitter.com/suyafnu?ref_src=twsrc%5Etfw">@suyafnu</a>, and <a href="https://twitter.com/UdacityDave?ref_src=twsrc%5Etfw">@UdacityDave</a>.<br><br>Tune in to learn why some data subpopulations are more vulnerable to attacks than others!<a href="https://t.co/Z12D3PvfXu">https://t.co/Z12D3PvfXu</a><a href="https://twitter.com/hashtag/ieeevis?src=hash&amp;ref_src=twsrc%5Etfw">#ieeevis</a> <a href="https://twitter.com/hashtag/VISxAI?src=hash&amp;ref_src=twsrc%5Etfw">#VISxAI</a> <a href="https://t.co/Gm2JBpWQSP">pic.twitter.com/Gm2JBpWQSP</a></p>
<p class="text-right"><a href="/best-submission-award-at-visxai-2022/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/visualizing-poisoning/">Visualizing Poisoning</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-07-26 00:00:00 &#43;0000 UTC" itemprop="datePublished">26 July 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/evan-rose">Evan Rose</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning-attacks">poisoning attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>
    
  </span>
  
  
</div>


<p><em>How does a poisoning attack work and why are some groups more
susceptible to being victimized by a poisoning attack?</em></p>
<p>We&rsquo;ve posted work that helps understand how poisoning attacks work
with some engaging visualizations:</p>
<p><a href="/poisoning">Poisoning Attacks and Subpopulation Susceptibility</a><br>
<em>An Experimental Exploration on the Effectiveness of Poisoning Attacks</em><br>
Evan Rose, Fnu Suya, and David Evans</p>
<center>
<a href="/poisoning"><img src="/images/visualizingpoisoning.png" width="85%"></a><br>
Follow <a href="/poisoning">the link</a> to try the interactive version!
</center>
<h1 id="heading"></h1>
<p>Machine learning is susceptible to poisoning attacks in which
adversaries inject maliciously crafted training data into the training
set to induce specific model behavior. We focus on subpopulation
attacks, in which the attacker&rsquo;s goal is to induce a model that
produces a targeted and incorrect output (label blue in our demos) for
a particular subset of the input space (colored orange). We study the
question, which subpopulations are the most vulnerable to an attack
and why?</p>
<p class="text-right"><a href="/visualizing-poisoning/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/biml-what-machine-learnt-models-reveal/">BIML: What Machine Learnt Models Reveal</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-07-19 00:00:00 &#43;0000 UTC" itemprop="datePublished">19 July 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/biml">BIML</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/gary-mcgraw">Gary McGraw</a>
    
  </span>
  
  
</div>


<p>I gave a talk in the <a href="https://berryvilleiml.com/">Berryville Institute of Machine Learning in the Barn</a> series on <em>What Machine Learnt Models Reveal</em>, which is now available as an edited video:</p>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/zMM_y6VWSgA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>
<blockquote>
<p>David Evans, a professor of computer science researching security and privacy at the University of Virginia, talks about data leakage risk in ML systems and different approaches used to attack and secure models and datasets. Juxtaposing adversarial risks that target records and those aimed at attributes, David shows that differential privacy cannot capture all inference risks, and calls for more research based on privacy experiments aimed at both datasets and distributions.</p>
<p class="text-right"><a href="/biml-what-machine-learnt-models-reveal/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/congratulations-dr.-zhang/">Congratulations, Dr. Zhang!</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-07-19 00:00:00 &#43;0000 UTC" itemprop="datePublished">19 July 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/phd-defense">PhD defense</a>
    
  </span>
  
  
</div>


<p>Congratulations to Xiao Zhang for successfully defending his PhD thesis!</p>
<center>
<img src="/images/xiaozhangphd.png" width="75%">
<div class="caption">
Dr. Zhang and his PhD committee: Somesh Jha (University of Wisconsin), David Evans, Tom Fletcher; Tianxi&nbsp;Li&nbsp;(UVA&nbsp;Statistics), David&nbsp;Wu&nbsp;(UT&nbsp;Austin), Mohammad&nbsp;Mahmoody; Xiao&nbsp;Zhang.
</center>
<h2 id="heading"></h2>
<p>Xiao will join the <a href="https://cispa.de/en">CISPA Helmholtz Center for Information
Security</a> in Saarbr√ºcken, Germany this fall as a
tenure-track faculty member.</p>
<h2 id="heading-1"></h2>
<center>
<em>From Characterizing Intrinsic Robustness to Adversarially Robust Machine Learning</em>
</center>
<h2 id="heading-2"></h2>
<p>The prevalence of adversarial examples raises questions about the
reliability of machine learning systems, especially for their
deployment in critical applications. Numerous defense mechanisms have
been proposed that aim to improve a machine learning system‚Äôs
robustness in the presence of adversarial examples. However, none of
these methods are able to produce satisfactorily robust models, even
for simple classification tasks on benchmarks. In addition to
empirical attempts to build robust models, recent studies have
identified intrinsic limitations for robust learning against
adversarial examples. My research aims to gain a deeper understanding
of why machine learning models fail in the presence of adversaries and
design ways to build better robust systems. In this dissertation, I
develop a concentration estimation framework to characterize the
intrinsic limits of robustness for typical classification tasks of
interest. The proposed framework leads to the discovery that compared
with the concentration of measure which was previously argued to be an
important factor, the existence of uncertain inputs may explain more
fundamentally the vulnerability of state-of-the-art
defenses. Moreover, to further advance our understanding of
adversarial examples, I introduce a notion of representation
robustness based on mutual information, which is shown to be related
to an intrinsic limit of model robustness for downstream
classification tasks. Finally in this dissertation, I advocate for a
need to rethink the current design goal of robustness and shed light
on ways to build better robust machine learning systems, potentially
escaping the intrinsic limits of robustness.</p>
<p class="text-right"><a href="/congratulations-dr.-zhang/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/iclr-2022-understanding-intrinsic-robustness-using-label-uncertainty/">ICLR 2022: Understanding Intrinsic Robustness Using Label Uncertainty</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-03-24 00:00:00 &#43;0000 UTC" itemprop="datePublished">24 March 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/iclr">ICLR</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/intrinsic-robustness">intrinsic robustness</a>
    
  </span>
  
  
</div>


<p>(Blog post written by <a href="https://xiao-zhang.net/">Xiao Zhang</a>)</p>
<p>Motivated by the empirical hardness of developing robust classifiers
against adversarial perturbations, researchers began asking the
question ‚Äú<em>Does there even exist a robust classifier?</em>‚Äù. This is
formulated as the <strong><em>intrinsic robustness problem</em></strong> <a href="https://proceedings.neurips.cc/paper/2019/file/46f76a4bda9a9579eab38a8f6eabcda1-Paper.pdf">(Mahloujifar et
al.,
2019)</a>,
where the goal is to characterize the maximum adversarial robustness
possible for a given robust classification problem. Building upon the
connection between adversarial robustness and classifier‚Äôs error
region, it has been shown that if we restrict the search to the set of
imperfect classifiers, the intrinsic robustness problem can be reduced
to the <strong><em>concentration of measure problem</em></strong>.</p>
<p class="text-right"><a href="/iclr-2022-understanding-intrinsic-robustness-using-label-uncertainty/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/microsoft-research-summit-surprising-and-unsurprising-inference-risks-in-machine-learning/">Microsoft Research Summit: Surprising (and unsurprising) Inference Risks in Machine Learning</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2021-10-21 00:00:00 &#43;0000 UTC" itemprop="datePublished">21 October 2021</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bargav-jayaraman">Bargav Jayaraman</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/katherine-knipmeyer">Katherine Knipmeyer</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/microsoft">Microsoft</a>
    
  </span>
  
  
</div>


<p>Here are the slides for my talk at the <a href="https://www.microsoft.com/en-us/research/theme/confidential-computing/#workshops"><em>Practical and Theoretical Privacy of Machine Learning Training Pipelines</em></a>
Workshop at the Microsoft Research Summit (21 October 2021):</p>
   <center>
<a href="https://www.dropbox.com/s/1mfhbelv7qx4t3u/surprisinginferences.pdf?dl=0"><b>Surprising (and Unsurprising) Inference Risks in Machine Learning</b> [PDF]</a>
   </center>
<h2 id="heading"></h2>
<h2 id="heading-1"></h2>
<p>The work by Bargav Jayaraman (with Katherine Knipmeyer, Lingxiao Wang,
and Quanquan Gu) that I talked about on improving membership inference
attacks is described in more details here:</p>
<ul>
<li>
<p>Bargav Jayaraman, Lingxiao Wang, Katherine Knipmeyer, Quanquan Gu, David Evans. <a href="https://arxiv.org/abs/2005.10881"><em>Revisiting Membership Inference Under Realistic Assumptions</em></a> (PETS 2021).<br>
[<a href="/merlin-morgan-and-the-importance-of-thresholds-and-priors/">Blog</a>] [Code: <a href="https://github.com/bargavj/EvaluatingDPML"><em>https://github.com/bargavj/EvaluatingDPML</em></a>]</p>
<p class="text-right"><a href="/microsoft-research-summit-surprising-and-unsurprising-inference-risks-in-machine-learning/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/uva-news-article/">UVA News Article</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2021-09-28 00:00:00 &#43;0000 UTC" itemprop="datePublished">28 September 2021</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bargav-jayaraman">Bargav Jayaraman</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jack-prescott">Jack Prescott</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/katherine-knipmeyer">Katherine Knipmeyer</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>
    
  </span>
  
  
</div>


<p>UVA News has an article by Audra Book on our research on security and
privacy of machine learning (with some very nice quotes from several
students in the group, and me saying something positive about the
NSA!): <a href="https://engineering.virginia.edu/news/2021/09/computer-science-professor-david-evans-and-his-team-conduct-experiments-understand"><em>Computer science professor David Evans and his team conduct
experiments to understand security and privacy risks associated with
machine
learning</em></a>,
8 September 2021.</p>
<div class="articletext">
<p>David Evans, professor of computer science in the University of Virginia School of Engineering and Applied Science, is leading research to understand how machine learning models can be compromised.</p>
<p class="text-right"><a href="/uva-news-article/">Read More‚Ä¶</a></p>
	

    
    <h2><a href="/model-targeted-poisoning-attacks-with-provable-convergence/">Model-Targeted Poisoning Attacks with Provable Convergence</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2021-06-29 00:00:00 &#43;0000 UTC" itemprop="datePublished">29 June 2021</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/model-targeted-poisoning-attacks">model-targeted poisoning attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/saeed-mahloujifar">Saeed Mahloujifar</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yuan-tian">Yuan Tian</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/icml-2021">ICML 2021</a>
    
  </span>
  
  
</div>


<p>(Post by Sean Miller, using images adapted from Suya&rsquo;s talk slides)</p>
<h2 id="data-poisoning-attacks">Data Poisoning Attacks</h2>
<p>Machine learning models are often trained using data from untrusted
sources, leaving them open to poisoning attacks where adversaries use
their control over a small fraction of that training data to poison
the model in a particular way.</p>
<p>Most work on poisoning attacks is directly driven by an attacker&rsquo;s
objective, where the adversary chooses poisoning points that maximize
some target objective. Our work focuses on <em>model-targeted</em> poisoning
attacks, where the adversary splits the attack into choosing a target
model that satisfies the objective and then choosing poisoning points
that induce the target model.</p>
<p class="text-right"><a href="/model-targeted-poisoning-attacks-with-provable-convergence/">Read More‚Ä¶</a></p>
	

    
    <div class="row">
  <div class="column small-12">
    <ul class="pagination" role="navigation" aria-label="Pagination">
      
      <li class="arrow" aria-disabled="true"><a href="/post/page/5/">&laquo; <em>Older<span class="show-for-sr"> blog entries</span></em></a></li>
      
      <li><span>Page 4 of 10</span></li>      
      
      <li class="arrow" aria-disabled="true"><a href="/post/page/3/"><em>Newer<span class="show-for-sr"> blog entries</span></em>:&nbsp;&raquo;</a></li>
      
    </ul>    
    All Posts by <a href="//uvasrg.github.io/categories">Category</a> or <a href="//uvasrg.github.io/tags">Tags</a>.

  </div>
</div>

</div>
</div>
</div>

    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
