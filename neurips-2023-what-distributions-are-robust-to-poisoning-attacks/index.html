<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>NeurIPS 2023: What Distributions are Robust to Poisoning Attacks? | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      



	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      
<div style="margin-top:16px; margin-left: auto; margin-right: auto; max-width: 800px;">
    <article class="article" itemscope itemtype="http://schema.org/Article">
      
      <h1 itemprop="name">NeurIPS 2023: What Distributions are Robust to Poisoning Attacks?</h1>
      <div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-12-07 00:00:00 &#43;0000 UTC" itemprop="datePublished">7 December 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/indiscriminate-poisoning-attacks">indiscriminate poisoning attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/susceptibility-variation">susceptibility variation</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Post by <strong><a href="https://fsuya.org/">Fnu Suya</a></strong></p>
<p>Data poisoning attacks are recognized as a top concern in the industry <a href="https://arxiv.org/abs/2002.05646">[1]</a>. We focus on conventional indiscriminate data poisoning attacks, where an adversary injects a few crafted examples into the training data with the goal of increasing the test error of the induced model. Despite recent advances, indiscriminate poisoning attacks on large neural networks remain challenging <a href="https://arxiv.org/abs/2303.03592">[2]</a>. In this work (to be presented at NeurIPS 2023), we revisit the vulnerabilities of more extensively studied linear models under indiscriminate poisoning attacks.</p>
<h2 id="understanding-vulnerabilities-across-different-datasets">Understanding Vulnerabilities Across Different Datasets</h2>
<p>We observed significant variations in the vulnerabilities of different datasets to poisoning attacks. Interestingly, certain datasets are robust against the best known attacks, even in the absence of any defensive measures.</p>
<p>The figure below illustrates the error rates (both before and after poisoning) of various datasets when assessed using the current best attacks with a 3% poisoning ratio under linear SVM model.</p>
<center>
<a href="/images/poisondistribution2023/thumbnail.png"><img src="/images/poisondistribution2023/thumbnail.png" width="80%"></a>
</center>
<h2 id="heading"></h2>
<p>Here, $\mathcal{S}_c$ represents the original training set (before poisoning), and $\mathcal{S}_c \cup \mathcal{S}_p$ represents the combination of the original clean training set and the poisoning set generated by the current best attacks (the poisoned model).  Different datasets exhibit widely varying vulnerability. For instance, datasets like MNIST 1-7 (with an error increase of &lt;3% at a 3% poisoning ratio) display resilience to current best attacks even without any defensive mechanisms. This leads to an important question: <em>Are datasets like MNIST 1-7 inherently robust to attacks, or are they merely resilient to current attack methods?</em></p>
<h2 id="why-some-datasets-resist-poisoning">Why Some Datasets Resist Poisoning</h2>
<p>To address this question, we conducted a series of theoretical analyses. Our findings indicate that ditributions, which are characterized by high class-wise separability (Sep) and low in-class variance (SD), as well as smaller sizes for the set containing all poisoning points (Size), inherently exhibit resistance to poisoning attacks.</p>
<!-- 1. **Theorem1**: under mild conditions, (practical) finite-sample indiscriminate poisoning attacks that generate poisoned dataset is an asymptotic estimator of the (theoretical) distributional indiscriminate poisoning attacks that generate poisoned distributions as the size of the clean training data is sufficiently large. **Therefore, it is useful study distributional attacks as they still connect to practical attacks.** 
2. **Theorem 2**: when studying distributional attacks at the $\epsilon$ poisoning ratio, under mild conditions, it is always beneficial to fully utilize the poisoning budget $\epsilon$ instead of budget less than that. **Therefore, we can study optimal poisoning attacks by using the maximum poisoraning ratio $\epsilon$**
3. With the Theorem 1 and Theorem 2, we conveniently study optimal distributinal poisoning attacks on 1-D Gaussian and showed that datasets with high class-wise separability (Sep) and low in-class variance (SD) are inherently robust to poisoning, and is even more robust when the size of the set conraining all poisoning points (Size) is small. We further show that, these three metrics also form upper bound to the performance of optimal attacks for generation distributions, and for distributions with noce properties, the optimal attack effectiveness is also limited (i.e., small upper bound).   -->
<p>Returning to the benchmark datasets, we observed a strong correlation between the identified metrics and the empirically observed vulnerabilities to current best attacks. This reaffirms our theoretical findings. Notably, we employed the ratios Sep/SD and Sep/Size for convenient comparison between datasets, as depicted in the results below:</p>
<center>
<a href="/images/poisondistribution2023/error_increase.png"><img src="/images/poisondistribution2023/error_increase.png" width="80%"></a>
</center>
<h2 id="heading-1"></h2>
<p>Datasets that are resistant to current attacks, like MNIST 1-7, exhibit larger Sep/SD and Sep/Size ratios. This suggests well-separated distributions with low variance and limited impact from poisoning points. Conversely, more vulnerable datasets, such as the spam email dataset Enron, display the opposite characteristics.</p>
<h2 id="implications">Implications</h2>
<p>While explaining the variations in vulnerabilities across datasets is valuable, our overriding goal is to improve robustness as much as possible. Our primary finding suggests that dataset robustness against poisoning attacks can be enhanced by leveraging favorable distributional properties.</p>
<p>In preliminary experiments, we demonstrate that employing improved feature extractors, such as deep models trained for an extended number of epochs, can achieve this objective.</p>
<p>We trained various feature extractors on the complete CIFAR-10 dataset and fine-tuned them on data labeled &ldquo;Truck&rdquo; and &ldquo;Ship&rdquo; for a downstream binary classification task. We utilized a deeper model, ResNet-18, trained for X epochs and denoted these models as R-X. Additionally, we included a straightforward CNN model trained until full convergence (LeNet). This approach allowed us to obtain a diverse set of pretrained models representing different potential feature representations for the downstream training data.</p>
<center>
<a href="/images/poisondistribution2023/feature.png"><img src="/images/poisondistribution2023/feature.png" width="60%"></a>
</center>
<h2 id="heading-2"></h2>
<p>The figure above shows that as we utilize the ResNet model and train it for a sufficient number of epochs, the quality of the feature representation improves, subsequently enhancing the robustness of downstream models against poisoning attacks. These preliminary findings highlight the exciting potential for future research aimed at leveraging enhanced features to bolster resilience against poisoning attacks. This serves as a strong motivation for further in-depth exploration in this direction.</p>
<h3 id="paper">Paper</h3>
<p>Fnu Suya, Xiao Zhang, Yuan Tian, David Evans. <a href="https://openreview.net/forum?id=yyLFUPNEiT"><em>What Distributions are Robust to Indiscriminate Poisoning Attacks for Linear Learners?</em></a>. In <a href="https://cvpr2023.thecvf.com/">Neural Information Processing Systems</a> (NeurIPS). New Orleans, 10â€“17 December 2023. <a href="https://arxiv.org/abs/2307.01073">[arXiv]</a></p>

      </div>

      <meta itemprop="wordCount" content="632">
      <meta itemprop="datePublished" content="2023-12-07">
      <meta itemprop="url" content="//uvasrg.github.io/neurips-2023-what-distributions-are-robust-to-poisoning-attacks/">
    </article>

    <ul class="pagination" role="navigation" aria-label="Pagination" style="margin-top:32px;">
      
      <li class="arrow" aria-disabled="true"><a href="//uvasrg.github.io/adjectives-can-reveal-gender-biases-within-nlp-models/">&laquo; <em>Previous<span class="show-for-sr"> page</span></em>: Adjectives Can Reveal Gender Biases Within NLP Models</a></li>
      
      
      <li class="arrow" aria-disabled="true"><a href="//uvasrg.github.io/sok-pitfalls-in-evaluating-black-box-attacks/"><em>Next<span class="show-for-sr"> page</span></em>: SoK: Pitfalls in Evaluating Black-Box Attacks&nbsp;&raquo;</a></li>
      
    </ul>
</div>

    </main>
    
    
<footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>


    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    

    <link rel="stylesheet" href="//uvasrg.github.io/katex/katex.min.css">

    
    <script defer src="//uvasrg.github.io/katex/katex.min.js"></script>

    
    <script defer src="//uvasrg.github.io/katex/contrib/auto-render.min.js"
            onload="renderMathInElement(document.body, { delimiters: [
                    {left: '$$', right: '$$', display: true },
                    { left: '$', right: '$', display: false },
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true} ]});"></script>

    
    
  </body>
</html>
