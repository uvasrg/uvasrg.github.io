<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Copilot on Security Research Group</title>
    <link>//uvasrg.github.io/tags/copilot/</link>
    <description>Recent content in Copilot on Security Research Group</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Wed, 21 Aug 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/tags/copilot/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Poisoning LLMs</title>
      <link>//uvasrg.github.io/poisoning-llms/</link>
      <pubDate>Wed, 21 Aug 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/poisoning-llms/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m quoted in this story by Rob Lemos about poisoning code models (the &lt;a href=&#34;https://www.usenix.org/conference/usenixsecurity24/presentation/yan&#34;&gt;CodeBreaker&lt;/a&gt; paper in USENIX Security 2024 by Shenao Yan, Shen Wang, Yue Duan, Hanbin Hong, Kiho Lee, Doowon Kim, and Yuan Hong), that considers a similar threat to our &lt;a href=&#34;https://uvasrg.github.io/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/&#34;&gt;TrojanPuzzle&lt;/a&gt; work:&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.darkreading.com/application-security/researchers-turn-code-completion-llms-into-attack-tools&#34;&gt;&lt;em&gt;Researchers Highlight How Poisoned LLMs Can Suggest Vulnerable Code&lt;/em&gt;&lt;/a&gt;&lt;br&gt;&#xA;Dark Reading, 20 August 2024&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;CodeBreaker uses code transformations to create vulnerable code that continues to function as expected, but that will not be detected by major static analysis security testing. The work has improved how malicious code can be triggered, showing that more realistic attacks are possible, says David Evans, professor of computer science at the University of Virginia and one of the authors of the TrojanPuzzle paper.&#xA;...&#xA;Developers can take more care as well, viewing code suggestions — whether from an AI or from the Internet — with a critical eye. In addition, developers need to know how to construct prompts to produce more secure code.  &#xA;&lt;p&gt;Yet, developers need their own tools to detect potentially malicious code, says the University of Virginia&amp;rsquo;s Evans.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Uh-oh, there&#39;s a new way to poison code models</title>
      <link>//uvasrg.github.io/uh-oh-theres-a-new-way-to-poison-code-models/</link>
      <pubDate>Mon, 16 Jan 2023 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/uh-oh-theres-a-new-way-to-poison-code-models/</guid>
      <description>&lt;p&gt;Jack Clark&amp;rsquo;s &lt;a href=&#34;https://mailchi.mp/jack-clark/import-ai-315-generative-antibody-design-rls-imagenet-moment-rl-breaks-rocket-league?e=545365c0e9&#34;&gt;Import AI, 16 Jan 2023&lt;/a&gt; includes a nice description of our work on TrojanPuzzle:&lt;/p&gt;&#xA;&lt;p style=&#34;margin: 10px 0;padding: 0;mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #202020;font-family: Helvetica;font-size: 16px;line-height: 150%;text-align: left;&#34;&gt;&lt;strong&gt;####################################################&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;!-- /wp:paragraph --&gt;&lt;!-- wp:paragraph --&gt;&#xA;&lt;p style=&#34;margin: 10px 0;padding: 0;mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #202020;font-family: Helvetica;font-size: 16px;line-height: 150%;text-align: left;&#34;&gt;&lt;strong&gt;Uh-oh, there&#39;s a new way to poison code models - and it&#39;s really hard to detect:&lt;/strong&gt;&lt;br&gt;&#xA;&lt;em&gt;…TROJANPUZZLE is a clever way to trick your code model into betraying you - if you can poison the undelrying dataset…&lt;/em&gt;&lt;br&gt;&#xA;Researchers with the University of California, Santa Barbara, Microsoft Corporation, and the University of Virginia have come up with some clever, subtle ways to poison the datasets used to train code models. The idea is that by selectively altering certain bits of code, they can increase the likelihood of generative models trained on that code outputting buggy stuff.&amp;nbsp;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Trojan Puzzle attack trains AI assistants into suggesting malicious code</title>
      <link>//uvasrg.github.io/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/</link>
      <pubDate>Tue, 10 Jan 2023 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/</guid>
      <description>&lt;p&gt;Bleeping Computer has a &lt;a href=&#34;https://www.bleepingcomputer.com/news/security/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/&#34;&gt;story on our work&lt;/a&gt; (in collaboration with Microsoft Research) on poisoning code suggestion models:&lt;/p&gt;&#xA;&lt;h1&gt;Trojan Puzzle attack trains AI assistants into suggesting malicious code&lt;/h1&gt;&#xA;&lt;p&gt;By &lt;b&gt;Bill Toulas&lt;/b&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;img alt=&#34;Person made of jigsaw puzzle pieces&#34; height=&#34;900&#34; src=&#34;https://www.bleepstatic.com/content/hl-images/2022/10/09/mystery-hacker.jpg&#34; width=&#34;80%&#34;&gt;&lt;/img&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;Researchers at the universities of California, Virginia, and Microsoft have devised a new poisoning attack that could trick AI-based coding assistants into suggesting dangerous code.&lt;/p&gt;&#xA;&lt;p&gt;Named &amp;lsquo;Trojan Puzzle,&amp;rsquo; the attack stands out for bypassing static detection and signature-based dataset cleansing models, resulting in the AI models being trained to learn how to reproduce dangerous payloads.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
