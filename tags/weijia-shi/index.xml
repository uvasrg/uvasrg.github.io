<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Weijia Shi on Security Research Group</title>
    <link>//uvasrg.github.io/tags/weijia-shi/</link>
    <description>Recent content in Weijia Shi on Security Research Group</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Tue, 05 Mar 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/tags/weijia-shi/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Do Membership Inference Attacks Work on Large Language Models?</title>
      <link>//uvasrg.github.io/do-membership-inference-attacks-work-on-large-language-models/</link>
      <pubDate>Tue, 05 Mar 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/do-membership-inference-attacks-work-on-large-language-models/</guid>
      <description>&lt;center&gt;&#xA;&lt;img src=&#34;https://iamgroot42.github.io/mimir.github.io/static/images/logo.png&#34; alt=&#34;&#34; style=&#34;width:25%;height:25%;&#34; class=&#34;center&#34;&gt;&#xA;&lt;figcaption style=&#34;font-size: small;&#34;&gt;&lt;a href=&#34;http://github.com/iamgroot42/mimir&#34;&gt;MIMIR&lt;/a&gt; logo. Image credit: &lt;a href=&#34;https://chat.openai.com/&#34;&gt;GPT-4 + DALL-E&lt;/a&gt;&#xA;&lt;/figcaption&gt;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&lt;div class=&#34;column has-text-centered&#34;&gt;&#xA;  &lt;div class=&#34;publication-links&#34;&gt;&#xA;  &lt;span class=&#34;link-block&#34;&gt;&#xA;  &lt;a href=&#34;https://arxiv.org/pdf/2402.07841.pdf&#34; class=&#34;external-link button is-normal is-rounded is-dark&#34;&gt;&#xA;  &lt;span class=&#34;icon&#34;&gt;&#xA;  &lt;i class=&#34;fas fa-file-pdf&#34;&gt;&lt;/i&gt;&#xA;                      &lt;/span&gt;&#xA;                      &lt;span&gt;Paper&lt;/span&gt;&#xA;                    &lt;/a&gt;&#xA;                  &lt;/span&gt;&#xA;  &lt;span class=&#34;link-block&#34;&gt;&#xA;                &lt;a href=&#34;http://github.com/iamgroot42/mimir&#34; class=&#34;external-link button is-normal is-rounded is-dark&#34;&gt;&#xA;                  &lt;span&gt;Code&lt;/span&gt;&#xA;                &lt;/a&gt;&#xA;              &lt;/span&gt;&#xA; &lt;span class=&#34;link-block&#34;&gt;&#xA;                  &lt;a href=&#34;https://huggingface.co/datasets/iamgroot42/mimir&#34;&#xA;                    class=&#34;external-link button is-normal is-rounded is-dark&#34;&gt;&#xA;                    &lt;span class=&#34;icon&#34;&gt;&#xA;                      &lt;i class=&#34;far fa-images&#34;&gt;&lt;/i&gt;&#xA;                    &lt;/span&gt;&#xA;                    &lt;span&gt;Data&lt;/span&gt;&#xA;                  &lt;/a&gt;&#xA;                &lt;/span&gt;&#xA;              &lt;/div&gt;&#xA;            &lt;/div&gt;&#xA;  &lt;/center&gt;&#xA;&lt;p&gt;Membership inference attacks (MIAs) attempt to predict whether a particular datapoint is a member of a target model&amp;rsquo;s training data. Despite extensive research on traditional machine learning models, there has been limited work studying MIA on the pre-training data of large language models (LLMs).&lt;/p&gt;&#xA;&lt;p&gt;We perform a large-scale evaluation of MIAs over a suite of language models (LMs) trained on the Pile, ranging from 160M to 12B parameters. We find that MIAs barely outperform random guessing for most settings across varying LLM sizes and domains. Our further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and few training iterations, and (2) an inherently fuzzy boundary between members and non-members.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
