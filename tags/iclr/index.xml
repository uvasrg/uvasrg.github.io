<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ICLR on Security Research Group</title>
    <link>//uvasrg.github.io/tags/iclr/</link>
    <description>Recent content in ICLR on Security Research Group</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Thu, 24 Mar 2022 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/tags/iclr/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ICLR 2022: Understanding Intrinsic Robustness Using Label Uncertainty</title>
      <link>//uvasrg.github.io/iclr-2022-understanding-intrinsic-robustness-using-label-uncertainty/</link>
      <pubDate>Thu, 24 Mar 2022 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/iclr-2022-understanding-intrinsic-robustness-using-label-uncertainty/</guid>
      <description>&lt;p&gt;(Blog post written by &lt;a href=&#34;https://xiao-zhang.net/&#34;&gt;Xiao Zhang&lt;/a&gt;)&lt;/p&gt;&#xA;&lt;p&gt;Motivated by the empirical hardness of developing robust classifiers&#xA;against adversarial perturbations, researchers began asking the&#xA;question “&lt;em&gt;Does there even exist a robust classifier?&lt;/em&gt;”. This is&#xA;formulated as the &lt;strong&gt;&lt;em&gt;intrinsic robustness problem&lt;/em&gt;&lt;/strong&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/file/46f76a4bda9a9579eab38a8f6eabcda1-Paper.pdf&#34;&gt;(Mahloujifar et&#xA;al.,&#xA;2019)&lt;/a&gt;,&#xA;where the goal is to characterize the maximum adversarial robustness&#xA;possible for a given robust classification problem. Building upon the&#xA;connection between adversarial robustness and classifier’s error&#xA;region, it has been shown that if we restrict the search to the set of&#xA;imperfect classifiers, the intrinsic robustness problem can be reduced&#xA;to the &lt;strong&gt;&lt;em&gt;concentration of measure problem&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Improved Estimation of Concentration (ICLR 2021)</title>
      <link>//uvasrg.github.io/improved-estimation-of-concentration-iclr-2021/</link>
      <pubDate>Fri, 02 Apr 2021 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/improved-estimation-of-concentration-iclr-2021/</guid>
      <description>&lt;p&gt;Our paper on &lt;a href=&#34;https://openreview.net/forum?id=BUlyHkzjgmA&#34;&gt;&lt;em&gt;Improved Estimation of Concentration Under ℓ&lt;sub&gt;p&lt;/sub&gt;-Norm Distance Metrics Using Half Spaces&lt;/em&gt;&lt;/a&gt; (Jack Prescott, &lt;a href=&#34;https://people.virginia.edu/~xz7bc/&#34;&gt;Xiao Zhang&lt;/a&gt;, and David Evans) will be presented at ICLR 2021.&lt;/p&gt;&#xA;&lt;p&gt;&lt;b&gt;Abstract:&lt;/b&gt; Concentration of measure has been argued to be the&#xA;fundamental cause of adversarial vulnerability. Mahloujifar et&#xA;al. (2019) presented an empirical way to measure the concentration of&#xA;a data distribution using samples, and employed it to find lower&#xA;bounds on intrinsic robustness for several benchmark&#xA;datasets. However, it remains unclear whether these lower bounds are&#xA;tight enough to provide a useful approximation for the intrinsic&#xA;robustness of a dataset. To gain a deeper understanding of the&#xA;concentration of measure phenomenon, we first extend the Gaussian&#xA;Isoperimetric Inequality to non-spherical Gaussian measures and&#xA;arbitrary ℓ&lt;sub&gt;p&lt;/sub&gt;-norms (&lt;em&gt;p&lt;/em&gt; ≥ 2). We leverage these&#xA;theoretical insights to design a method that uses half-spaces to&#xA;estimate the concentration of any empirical dataset under&#xA;ℓ&lt;sub&gt;p&lt;/sub&gt;-norm distance metrics. Our proposed algorithm is more&#xA;efficient than Mahloujifar et al. (2019)&amp;rsquo;s, and experiments on&#xA;synthetic datasets and image benchmarks demonstrate that it is able to&#xA;find much tighter intrinsic robustness bounds. These tighter estimates&#xA;provide further evidence that rules out intrinsic dataset&#xA;concentration as a possible explanation for the adversarial&#xA;vulnerability of state-of-the-art classifiers.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cost-Sensitive Adversarial Robustness at ICLR 2019</title>
      <link>//uvasrg.github.io/cost-sensitive-adversarial-robustness-at-iclr-2019/</link>
      <pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/cost-sensitive-adversarial-robustness-at-iclr-2019/</guid>
      <description>&lt;p&gt;Xiao Zhang will present &lt;a href=&#34;https://openreview.net/forum?id=BygANhA9tQ&amp;amp;noteId=BJe7cKRWeN&#34;&gt;&lt;em&gt;Cost-Sensitive Robustness against Adversarial Examples&lt;/em&gt;&lt;/a&gt; on May 7 (4:30-6:30pm) at &lt;a href=&#34;https://iclr.cc/Conferences/2019/&#34;&gt;ICLR 2019 in New Orleans.&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/docs/cost-sensitive-poster.pdf&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/docs/cost-sensitive-poster-small.png&#34; width=&#34;90%&#34; align=&#34;center&#34;&gt;&lt;/a&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;Paper: &lt;a href=&#34;https://evademl.org/docs/cost-sensitive-robustness.pdf&#34;&gt;[PDF]&lt;/a&gt; [&lt;a href=&#34;https://openreview.net/forum?id=BygANhA9tQ&amp;amp;noteId=BJe7cKRWeN&#34;&gt;OpenReview&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/1810.09225&#34;&gt;ArXiv&lt;/a&gt;]&lt;/p&gt;</description>
    </item>
    <item>
      <title>Empirically Measuring Concentration</title>
      <link>//uvasrg.github.io/empirically-measuring-concentration/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/empirically-measuring-concentration/</guid>
      <description>&lt;p&gt;Xiao Zhang and Saeed Mahloujifar will present our work on &lt;em&gt;Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness&lt;/em&gt; at two workshops May 6 at ICLR 2019 in New Orleans: &lt;a href=&#34;https://debug-ml-iclr2019.github.io/&#34;&gt;&lt;em&gt;Debugging Machine Learning Models&lt;/em&gt;&lt;/a&gt; and &lt;a href=&#34;https://sites.google.com/view/safeml-iclr2019&#34;&gt;&lt;em&gt;Safe Machine Learning:&#xA;Specification, Robustness and Assurance&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Paper: &lt;a href=&#34;//uvasrg.github.io/docs/concentration-robustness.pdf&#34;&gt;[PDF]&lt;/a&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/docs/concentration-robustness-poster.pdf&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/docs/concentration-robustness-poster-small.png&#34; width=&#34;90%&#34; align=&#34;center&#34;&gt;&lt;/a&gt;&#xA;&lt;/center&gt;</description>
    </item>
    <item>
      <title>ICLR 2019: Cost-Sensitive Robustness against Adversarial Examples</title>
      <link>//uvasrg.github.io/iclr-2019-cost-sensitive-robustness-against-adversarial-examples/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/iclr-2019-cost-sensitive-robustness-against-adversarial-examples/</guid>
      <description>&lt;p&gt;Xiao Zhang and my paper on &lt;a href=&#34;https://openreview.net/forum?id=BygANhA9tQ&amp;amp;noteId=BJe7cKRWeN&#34;&gt;&lt;em&gt;Cost-Sensitive Robustness against Adversarial Examples&lt;/em&gt;&lt;/a&gt; has been accepted to ICLR 2019.&lt;/p&gt;&#xA;&lt;p&gt;Several recent works have developed methods for training classifiers&#xA;that are certifiably robust against norm-bounded adversarial&#xA;perturbations. However, these methods assume that all the adversarial&#xA;transformations provide equal value for adversaries, which is seldom&#xA;the case in real-world applications. We advocate for cost-sensitive&#xA;robustness as the criteria for measuring the classifier&amp;rsquo;s performance&#xA;for specific tasks. We encode the potential harm of different&#xA;adversarial transformations in a cost matrix, and propose a general&#xA;objective function to adapt the robust training method of Wong &amp;amp;&#xA;Kolter (2018) to optimize for cost-sensitive robustness. Our&#xA;experiments on simple MNIST and CIFAR10 models and a variety of cost&#xA;matrices show that the proposed approach can produce models with&#xA;substantially reduced cost-sensitive robust error, while maintaining&#xA;classification accuracy.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
