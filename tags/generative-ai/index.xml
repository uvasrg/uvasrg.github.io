<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Generative AI on Security Research Group</title>
    <link>//uvasrg.github.io/tags/generative-ai/</link>
    <description>Recent content in Generative AI on Security Research Group</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Thu, 17 Aug 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/tags/generative-ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Adjectives Can Reveal Gender Biases Within NLP Models</title>
      <link>//uvasrg.github.io/adjectives-can-reveal-gender-biases-within-nlp-models/</link>
      <pubDate>Thu, 17 Aug 2023 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/adjectives-can-reveal-gender-biases-within-nlp-models/</guid>
      <description>&lt;p&gt;Post by &lt;strong&gt;Jason Briegel&lt;/strong&gt; and &lt;a href=&#34;https://hannahxchen.github.io/&#34;&gt;&lt;strong&gt;Hannah Chen&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Because NLP models are trained with human corpora (and now,&#xA;increasingly on text generated by other NLP models that were&#xA;originally trained on human language), they are prone to inheriting&#xA;common human stereotypes and biases. This is problematic, because with&#xA;their growing prominence they may further propagate these stereotypes&#xA;&lt;a href=&#34;https://arxiv.org/abs/1906.08976&#34;&gt;(Sun et al., 2019)&lt;/a&gt;. For example,&#xA;interest is growing in mitigating bias in the field of machine&#xA;translation, where systems such as Google translate were observed to&#xA;default to translating gender-neutral pronouns as male pronouns, even&#xA;with feminine cues &lt;a href=&#34;https://doi.org/10.1162/tacl_a_00401&#34;&gt;(Savoldi et al.,&#xA;2021)&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
