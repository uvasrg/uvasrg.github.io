<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Intrinsic Robustness on Security Research Group</title>
    <link>//uvasrg.github.io/tags/intrinsic-robustness/</link>
    <description>Recent content in Intrinsic Robustness on Security Research Group</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Thu, 24 Mar 2022 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/tags/intrinsic-robustness/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ICLR 2022: Understanding Intrinsic Robustness Using Label Uncertainty</title>
      <link>//uvasrg.github.io/iclr-2022-understanding-intrinsic-robustness-using-label-uncertainty/</link>
      <pubDate>Thu, 24 Mar 2022 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/iclr-2022-understanding-intrinsic-robustness-using-label-uncertainty/</guid>
      <description>&lt;p&gt;(Blog post written by &lt;a href=&#34;https://xiao-zhang.net/&#34;&gt;Xiao Zhang&lt;/a&gt;)&lt;/p&gt;&#xA;&lt;p&gt;Motivated by the empirical hardness of developing robust classifiers&#xA;against adversarial perturbations, researchers began asking the&#xA;question “&lt;em&gt;Does there even exist a robust classifier?&lt;/em&gt;”. This is&#xA;formulated as the &lt;strong&gt;&lt;em&gt;intrinsic robustness problem&lt;/em&gt;&lt;/strong&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/file/46f76a4bda9a9579eab38a8f6eabcda1-Paper.pdf&#34;&gt;(Mahloujifar et&#xA;al.,&#xA;2019)&lt;/a&gt;,&#xA;where the goal is to characterize the maximum adversarial robustness&#xA;possible for a given robust classification problem. Building upon the&#xA;connection between adversarial robustness and classifier’s error&#xA;region, it has been shown that if we restrict the search to the set of&#xA;imperfect classifiers, the intrinsic robustness problem can be reduced&#xA;to the &lt;strong&gt;&lt;em&gt;concentration of measure problem&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Improved Estimation of Concentration (ICLR 2021)</title>
      <link>//uvasrg.github.io/improved-estimation-of-concentration-iclr-2021/</link>
      <pubDate>Fri, 02 Apr 2021 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/improved-estimation-of-concentration-iclr-2021/</guid>
      <description>&lt;p&gt;Our paper on &lt;a href=&#34;https://openreview.net/forum?id=BUlyHkzjgmA&#34;&gt;&lt;em&gt;Improved Estimation of Concentration Under ℓ&lt;sub&gt;p&lt;/sub&gt;-Norm Distance Metrics Using Half Spaces&lt;/em&gt;&lt;/a&gt; (Jack Prescott, &lt;a href=&#34;https://people.virginia.edu/~xz7bc/&#34;&gt;Xiao Zhang&lt;/a&gt;, and David Evans) will be presented at ICLR 2021.&lt;/p&gt;&#xA;&lt;p&gt;&lt;b&gt;Abstract:&lt;/b&gt; Concentration of measure has been argued to be the&#xA;fundamental cause of adversarial vulnerability. Mahloujifar et&#xA;al. (2019) presented an empirical way to measure the concentration of&#xA;a data distribution using samples, and employed it to find lower&#xA;bounds on intrinsic robustness for several benchmark&#xA;datasets. However, it remains unclear whether these lower bounds are&#xA;tight enough to provide a useful approximation for the intrinsic&#xA;robustness of a dataset. To gain a deeper understanding of the&#xA;concentration of measure phenomenon, we first extend the Gaussian&#xA;Isoperimetric Inequality to non-spherical Gaussian measures and&#xA;arbitrary ℓ&lt;sub&gt;p&lt;/sub&gt;-norms (&lt;em&gt;p&lt;/em&gt; ≥ 2). We leverage these&#xA;theoretical insights to design a method that uses half-spaces to&#xA;estimate the concentration of any empirical dataset under&#xA;ℓ&lt;sub&gt;p&lt;/sub&gt;-norm distance metrics. Our proposed algorithm is more&#xA;efficient than Mahloujifar et al. (2019)&amp;rsquo;s, and experiments on&#xA;synthetic datasets and image benchmarks demonstrate that it is able to&#xA;find much tighter intrinsic robustness bounds. These tighter estimates&#xA;provide further evidence that rules out intrinsic dataset&#xA;concentration as a possible explanation for the adversarial&#xA;vulnerability of state-of-the-art classifiers.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
