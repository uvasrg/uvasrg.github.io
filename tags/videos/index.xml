<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Videos on Security Research Group</title>
    <link>//uvasrg.github.io/tags/videos/</link>
    <description>Recent content in Videos on Security Research Group</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Mon, 05 Apr 2021 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/tags/videos/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CrySP Talk: When Models Learn Too Much</title>
      <link>//uvasrg.github.io/crysp-talk-when-models-learn-too-much/</link>
      <pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/crysp-talk-when-models-learn-too-much/</guid>
      <description>&lt;p&gt;I gave a talk on &lt;a href=&#34;https://crysp.uwaterloo.ca/speakers/20210329-Evans&#34;&gt;&lt;em&gt;When Models Learn Too Much&lt;/em&gt;&lt;/a&gt; at the University of Waterloo (virtually) in the CrySP&#xA;Speaker Series on Privacy (29 March 2021):&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;iframe width=&#34;900&#34; height=&#34;315&#34; src=&#34;https://www.youtube-nocookie.com/embed/LM_-N76_KIw&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&#xA;&lt;/center&gt;&#xA;&lt;center&gt;&#xA;&lt;b&gt;Abstract&lt;/b&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;Statistical machine learning uses training data to produce models that&#xA;capture patterns in that data. When models are trained on private&#xA;data, such as medical records or personal emails, there is a risk that&#xA;those models not only learn the hoped-for patterns, but will also&#xA;learn and expose sensitive information about their training&#xA;data. Several different types of inference attacks on machine learning&#xA;models have been found, and methods have been proposed to mitigate the&#xA;risks of exposing sensitive aspects of training data. Differential&#xA;privacy provides formal guarantees bounding certain types of inference&#xA;risk, but, at least with state-of-the-art methods, providing&#xA;substantive differential privacy guarantees requires adding so much&#xA;noise to the training process for complex models that the resulting&#xA;models are useless. Experimental evidence, however, suggests that&#xA;inference attacks have limited power, and in many cases a very small&#xA;amount of privacy noise seems to be enough to defuse inference&#xA;attacks. In this talk, I will give an overview of a variety of&#xA;different inference risks for machine learning models, talk about&#xA;strategies for evaluating model inference risks, and report on some&#xA;experiments by our research group to better understand the power of&#xA;inference attacks in more realistic settings, and explore some broader&#xA;the connections between privacy, fairness, and adversarial robustness.&lt;/p&gt;</description>
    </item>
    <item>
      <title>DLS Keynote: Is &#39;adversarial examples&#39; an Adversarial Example?</title>
      <link>//uvasrg.github.io/dls-keynote-is-adversarial-examples-an-adversarial-example/</link>
      <pubDate>Tue, 29 May 2018 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/dls-keynote-is-adversarial-examples-an-adversarial-example/</guid>
      <description>&lt;p&gt;I gave a keynote talk at the &lt;a href=&#34;https://www.ieee-security.org/TC/SPW2018/DLS/#&#34;&gt;&lt;em&gt;1st Deep Learning and Security Workshop&lt;/em&gt;&lt;/a&gt; (co-located with the 39th &lt;em&gt;IEEE Symposium on Security and Privacy&lt;/em&gt;). San Francisco, California. 24 May 2018&lt;/p&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;center&gt;&lt;br /&gt;&#xA;&lt;iframe width=&#34;640&#34; height=&#34;360&#34; src=&#34;https://www.youtube-nocookie.com/embed/sFhD6ABghf8?rel=0&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;&#xA;&lt;script async class=&#34;speakerdeck-embed&#34;&#xA;&#x9;data-id=&#34;9d2c5bf9b3444a8a992762f5cd6ea7fe&#34;&#xA;&#x9;data-ratio=&#34;1.77777777777778&#34; src=&#34;http://speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;&lt;br /&gt;&#xA;&lt;/center&gt;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;&#xA;&lt;center&gt;&lt;br /&gt;&#xA;&lt;b&gt;Abstract&lt;/b&gt;&lt;br /&gt;&#xA;&lt;/center&gt;&lt;/p&gt;&#xA;&lt;p&gt;&#xA;Over the past few years, there has been an explosion of research in security of machine learning and on adversarial examples in particular. Although this is in many ways a new and immature research area, the general problem of adversarial examples has been a core problem in information security for thousands of years. In this talk, I&amp;#8217;ll look at some of the long-forgotten lessons from that quest and attempt to understand what, if anything, has changed now we are in the era of deep learning classifiers. I will survey the prevailing definitions for &amp;#8220;adversarial examples&amp;#8221;, argue that those definitions are unlikely to be the right ones, and raise questions about whether those definitions are leading us astray.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
