<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Differential Privacy on Security Research Group</title>
    <link>//uvasrg.github.io/tags/differential-privacy/</link>
    <description>Recent content in Differential Privacy on Security Research Group</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Mon, 05 Apr 2021 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/tags/differential-privacy/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CrySP Talk: When Models Learn Too Much</title>
      <link>//uvasrg.github.io/crysp-talk-when-models-learn-too-much/</link>
      <pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/crysp-talk-when-models-learn-too-much/</guid>
      <description>&lt;p&gt;I gave a talk on &lt;a href=&#34;https://crysp.uwaterloo.ca/speakers/20210329-Evans&#34;&gt;&lt;em&gt;When Models Learn Too Much&lt;/em&gt;&lt;/a&gt; at the University of Waterloo (virtually) in the CrySP&#xA;Speaker Series on Privacy (29 March 2021):&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;iframe width=&#34;900&#34; height=&#34;315&#34; src=&#34;https://www.youtube-nocookie.com/embed/LM_-N76_KIw&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&#xA;&lt;/center&gt;&#xA;&lt;center&gt;&#xA;&lt;b&gt;Abstract&lt;/b&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;Statistical machine learning uses training data to produce models that&#xA;capture patterns in that data. When models are trained on private&#xA;data, such as medical records or personal emails, there is a risk that&#xA;those models not only learn the hoped-for patterns, but will also&#xA;learn and expose sensitive information about their training&#xA;data. Several different types of inference attacks on machine learning&#xA;models have been found, and methods have been proposed to mitigate the&#xA;risks of exposing sensitive aspects of training data. Differential&#xA;privacy provides formal guarantees bounding certain types of inference&#xA;risk, but, at least with state-of-the-art methods, providing&#xA;substantive differential privacy guarantees requires adding so much&#xA;noise to the training process for complex models that the resulting&#xA;models are useless. Experimental evidence, however, suggests that&#xA;inference attacks have limited power, and in many cases a very small&#xA;amount of privacy noise seems to be enough to defuse inference&#xA;attacks. In this talk, I will give an overview of a variety of&#xA;different inference risks for machine learning models, talk about&#xA;strategies for evaluating model inference risks, and report on some&#xA;experiments by our research group to better understand the power of&#xA;inference attacks in more realistic settings, and explore some broader&#xA;the connections between privacy, fairness, and adversarial robustness.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Evaluating Differentially Private Machine Learning in Practice</title>
      <link>//uvasrg.github.io/evaluating-differentially-private-machine-learning-in-practice/</link>
      <pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/evaluating-differentially-private-machine-learning-in-practice/</guid>
      <description>&lt;p&gt;(Cross-post by &lt;a href=&#34;https://bargavjayaraman.github.io/post/evaluating-dpml-results/&#34;&gt;Bargav Jayaraman&lt;/a&gt;)&lt;/p&gt;&#xA;&lt;p&gt;With the recent advances in composition of differential private&#xA;mechanisms, the research community has been able to achieve meaningful&#xA;deep learning with privacy budgets in single digits. RÃ¨nyi&#xA;differential privacy (RDP) is one mechanism that provides tighter&#xA;composition which is widely used because of its implementation in&#xA;TensorFlow Privacy (recently, Gaussian differential privacy (GDP) has&#xA;shown a tighter analysis for low privacy budgets, but it was not yet&#xA;available when we did this work). But the central question that&#xA;remains to be answered is: &lt;em&gt;how private are these methods in&#xA;practice?&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>How AI could save lives without spilling medical secrets</title>
      <link>//uvasrg.github.io/how-ai-could-save-lives-without-spilling-medical-secrets/</link>
      <pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/how-ai-could-save-lives-without-spilling-medical-secrets/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m quoted in this article by Will Knight focused on the work Oasis Labs (Dawn Song&amp;rsquo;s company) is doing on privacy-preserving medical data analysis: &lt;a href=&#34;https://www.technologyreview.com/s/613520/how-ai-could-save-lives-without-spilling-secrets/&#34;&gt;&lt;em&gt;How AI could save lives without spilling medical secrets&lt;/em&gt;&lt;/a&gt;, MIT Technology Review, 14 May 2019.&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&amp;ldquo;The whole notion of doing computation while keeping data secret is an incredibly powerful one,&amp;rdquo; says David Evans, who specializes in machine learning and security at the University of Virginia. When applied across hospitals and patient populations, for instance, machine learning might unlock completely new ways of tying disease to genomics, test results, and other patient information.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
