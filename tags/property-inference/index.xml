<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Property Inference on Security Research Group</title>
    <link>//uvasrg.github.io/tags/property-inference/</link>
    <description>Recent content in Property Inference on Security Research Group</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Fri, 16 Dec 2022 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/tags/property-inference/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Dissecting Distribution Inference</title>
      <link>//uvasrg.github.io/dissecting-distribution-inference/</link>
      <pubDate>Fri, 16 Dec 2022 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/dissecting-distribution-inference/</guid>
      <description>&lt;p&gt;(Cross-post by &lt;a href=&#34;https://www.anshumansuri.com/post/dissecting&#34;&gt;Anshuman Suri&lt;/a&gt;)&lt;/p&gt;&#xA;&lt;p&gt;Distribution inference attacks aims to infer statistical properties of data used to train machine learning models.&#xA;These attacks are sometimes surprisingly potent, as we demonstrated in&#xA;&lt;a href=&#34;https://uvasrg.github.io/on-the-risks-of-distribution-inference/&#34;&gt;previous work&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;!-- However, the factors that impact this inference risk are not well understood, and demonstrated attacks often&#xA;rely on strong and unrealistic assumptions such as full knowledge of training environments&#xA;even in supposedly black-box threat scenarios. --&gt;&#xA;&lt;!-- In this work, we develop a new black-box attack, the KL Divergence Attack (KL), and use it to evaluate inference risk while relaxing&#xA;a number of implicit assumptions based on the adversary&#39;s knowledge in black-box scenarios. We also evaluate several noise-based defenses, a&#xA;standard approach while trying to preserve privacy in machine learning, along with some intuitive defenses based on resampling. --&gt;&#xA;&lt;h2 id=&#34;kl-divergence-attack&#34;&gt;KL Divergence Attack&lt;/h2&gt;&#xA;&lt;p&gt;Most attacks against distribution inference involve training a meta-classifier, either using model parameters in white-box settings (Ganju et al., &lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3243734.3243834&#34;&gt;Property Inference Attacks on Fully Connected Neural Networks using Permutation Invariant Representations&lt;/a&gt;, CCS 2018), or using model&#xA;predictions in black-box scenarios (Zhang et al., &lt;a href=&#34;https://www.usenix.org/system/files/sec21-zhang-wanrong.pdf&#34;&gt;Leakage of Dataset Properties in Multi-Party Machine Learning&lt;/a&gt;, USENIX 2021). While other black-box were proposed in our prior work, they are not as accurate as meta-classifier-based methods, and require training shadow models nonetheless (Suri and Evans, &lt;a href=&#34;https://arxiv.org/pdf/2109.06024.pdf&#34;&gt;Formalizing and Estimating Distribution Inference Risks&lt;/a&gt;, PETS 2022).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cray Distinguished Speaker: On Leaky Models and Unintended Inferences</title>
      <link>//uvasrg.github.io/cray-distinguished-speaker-on-leaky-models-and-unintended-inferences/</link>
      <pubDate>Tue, 13 Dec 2022 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/cray-distinguished-speaker-on-leaky-models-and-unintended-inferences/</guid>
      <description>&lt;p&gt;Here&amp;rsquo;s the slides from my &lt;a href=&#34;https://cse.umn.edu/cs/cray&#34;&gt;Cray Distinguished Speaker&lt;/a&gt; talk on &lt;a href=&#34;https://cse.umn.edu/cs/events/cse-colloquium-leaky-models-and-unintended-inferences&#34;&gt;&lt;em&gt;On Leaky Models and Unintended Inferences&lt;/em&gt;&lt;/a&gt;: [&lt;a href=&#34;https://www.dropbox.com/s/5gi766dqezsitw4/cray2022.pdf?dl=0&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;https://www.dropbox.com/s/5gi766dqezsitw4/cray2022.pdf?dl=0&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/cray2022-title.png&#34; width=&#34;65%&#34; alt=&#34;Leaky Models and Unintended Inferences&#34;&gt;&lt;/a&gt; &#xA;&lt;/center&gt;&#xA;&lt;/br&gt;&#xA;&lt;p&gt;The chatGPT limerick version of my talk abstract is much better than mine:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;A machine learning model, oh so grand&lt;br&gt;&#xA;With data sets that it held in its hand&lt;br&gt;&#xA;It performed quite well&lt;br&gt;&#xA;But secrets to tell&lt;br&gt;&#xA;And an adversary&amp;rsquo;s tricks it could not withstand.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;Thanks to Stephen McCamant and Kangjie Lu for hosting my visit, and everyone at University of Minnesota. Also great to catch up with UVA BSCS alumn, &lt;a href=&#34;https://www-users.cse.umn.edu/~sjguy/&#34;&gt;Stephen J. Guy&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>On the Risks of Distribution Inference</title>
      <link>//uvasrg.github.io/on-the-risks-of-distribution-inference/</link>
      <pubDate>Thu, 24 Jun 2021 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/on-the-risks-of-distribution-inference/</guid>
      <description>&lt;p&gt;(Cross-post by &lt;a href=&#34;https://www.anshumansuri.com/post/distr_infer&#34;&gt;Anshuman Suri&lt;/a&gt;)&lt;/p&gt;&#xA;&lt;p&gt;Inference attacks seek to infer sensitive information about the training process of a revealed machine-learned model, most often about the training data.&lt;/p&gt;&#xA;&lt;p&gt;Standard inference attacks (which we call “dataset inference attacks”)&#xA;aim to learn something about a particular record that may have been in&#xA;that training data. For example, in a membership inference attack&#xA;(Reza Shokri et al., &lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7958568&#34;&gt;&lt;em&gt;Membership Inference Attacks Against Machine&#xA;Learning&#xA;Models&lt;/em&gt;&lt;/a&gt;, IEEE S&amp;amp;P 2017),&#xA;the adversary aims to infer whether or not a particular record was&#xA;included in the training data.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
