<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Fnu Suya on Security Research Group</title>
    <link>//uvasrg.github.io/tags/fnu-suya/</link>
    <description>Recent content in Fnu Suya on Security Research Group</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Thu, 10 Oct 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/tags/fnu-suya/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Meet Professor Suya!</title>
      <link>//uvasrg.github.io/meet-professor-suya/</link>
      <pubDate>Thu, 10 Oct 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/meet-professor-suya/</guid>
      <description>&lt;center&gt;&#xA;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Meet Assistant Professor Fnu Suya. His research interests include the application of machine learning techniques to security-critical applications and the vulnerabilities of machine learning models in the presence of adversaries, generally known as trustworthy machine learning. &lt;a href=&#34;https://t.co/8R63QSN8aO&#34;&gt;pic.twitter.com/8R63QSN8aO&lt;/a&gt;&lt;/p&gt;&amp;mdash; EECS (@EECS_UTK) &lt;a href=&#34;https://twitter.com/EECS_UTK/status/1843293955158593547?ref_src=twsrc%5Etfw&#34;&gt;October 7, 2024&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&#xA;&lt;/center&gt;</description>
    </item>
    <item>
      <title>Graduation 2024</title>
      <link>//uvasrg.github.io/graduation-2024/</link>
      <pubDate>Wed, 29 May 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/graduation-2024/</guid>
      <description>&lt;center&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/graduation2024/PXL_20240517_175539174-fish.jpg&#34;&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/graduation2024/PXL_20240517_175539174-fish.jpg&#34; width=&#34;85%&#34;&gt;&lt;/img&gt;&lt;/a&gt;&#xA;&lt;/center&gt;&#xA;&lt;center&gt;&#xA;&lt;h1 id=&#34;congratulations-to-our-two-phd-graduates&#34;&gt;Congratulations to our two PhD graduates!&lt;/h1&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;Suya will be joining the University of Tennessee at Knoxville as an Assistant Professor.&lt;/p&gt;&#xA;&lt;p&gt;Josie will be building a medical analytics research group at Dexcom.&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/graduation2024/IMG_5973.png&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/graduation2024/IMG_5973.png&#34; width=&#34;85%&#34;&gt;&lt;/img&gt;&lt;/a&gt;&#xA;&lt;/center&gt;&#xA;&lt;h2 id=&#34;heading&#34;&gt;&lt;/h2&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/graduation2024/IMG_5941.png&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/graduation2024/IMG_5941.png&#34; width=&#34;60%&#34;&gt;&lt;/img&gt;&lt;/a&gt;&#xA;&amp;nbsp;&lt;a href=&#34;//uvasrg.github.io/images/graduation2024/IMG_5951.png&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/graduation2024/IMG_5951.png&#34; width=&#34;60%&#34;&gt;&lt;/img&gt;&lt;/a&gt;&amp;nbsp;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/graduation2024/IMG_5962.png&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/graduation2024/IMG_5962.png&#34; width=&#34;60%&#34;&gt;&lt;/img&gt;&lt;/a&gt;&#xA;&lt;/center&gt;</description>
    </item>
    <item>
      <title>SaTML Talk: SoK: Pitfalls in Evaluating Black-Box Attacks</title>
      <link>//uvasrg.github.io/satml-talk-sok-pitfalls-in-evaluating-black-box-attacks/</link>
      <pubDate>Mon, 22 Apr 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/satml-talk-sok-pitfalls-in-evaluating-black-box-attacks/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.anshumansuri.com/&#34;&gt;Anshuman Suri&lt;/a&gt;&amp;rsquo;s talk at &lt;a href=&#34;https://satml.org/&#34;&gt;IEEE Conference on Secure and Trustworthy Machine Learning&lt;/a&gt; (SaTML) is now available:&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube-nocookie.com/embed/ui4HMGe3aUs?si=M2A-uD77s4BdhXPR&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; allowfullscreen&gt;&lt;/iframe&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;See the &lt;a href=&#34;https://uvasrg.github.io/sok-pitfalls-in-evaluating-black-box-attacks/&#34;&gt;earlier blog post&lt;/a&gt; for more on the work, and the paper at &lt;a href=&#34;https://arxiv.org/abs/2310.17534&#34;&gt;https://arxiv.org/abs/2310.17534&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SoK: Pitfalls in Evaluating Black-Box Attacks</title>
      <link>//uvasrg.github.io/sok-pitfalls-in-evaluating-black-box-attacks/</link>
      <pubDate>Wed, 20 Dec 2023 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/sok-pitfalls-in-evaluating-black-box-attacks/</guid>
      <description>&lt;p&gt;Post by &lt;strong&gt;&lt;a href=&#34;https://www.anshumansuri.com/&#34;&gt;Anshuman Suri&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&#34;https://fsuya.org/&#34;&gt;Fnu Suya&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Much research has studied black-box attacks on image classifiers,&#xA;where adversaries generate adversarial examples against unknown target&#xA;models without having access to their internal information. Our&#xA;analysis of over 164 attacks (published in 102 major security, machine&#xA;learning and security conferences) shows how these works make&#xA;different assumptions about the adversary’s knowledge.&lt;/p&gt;&#xA;&lt;p&gt;The current literature lacks cohesive organization centered around the&#xA;threat model. Our &lt;a href=&#34;https://arxiv.org/abs/2310.17534&#34;&gt;SoK paper&lt;/a&gt; (to&#xA;appear at &lt;a href=&#34;https://satml.org/&#34;&gt;IEEE SaTML 2024&lt;/a&gt;) introduces a taxonomy&#xA;for systematizing these attacks and demonstrates the importance of&#xA;careful evaluations that consider adversary resources and threat&#xA;models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>NeurIPS 2023: What Distributions are Robust to Poisoning Attacks?</title>
      <link>//uvasrg.github.io/neurips-2023-what-distributions-are-robust-to-poisoning-attacks/</link>
      <pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/neurips-2023-what-distributions-are-robust-to-poisoning-attacks/</guid>
      <description>&lt;p&gt;Post by &lt;strong&gt;&lt;a href=&#34;https://fsuya.org/&#34;&gt;Fnu Suya&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Data poisoning attacks are recognized as a top concern in the industry &lt;a href=&#34;https://arxiv.org/abs/2002.05646&#34;&gt;[1]&lt;/a&gt;. We focus on conventional indiscriminate data poisoning attacks, where an adversary injects a few crafted examples into the training data with the goal of increasing the test error of the induced model. Despite recent advances, indiscriminate poisoning attacks on large neural networks remain challenging &lt;a href=&#34;https://arxiv.org/abs/2303.03592&#34;&gt;[2]&lt;/a&gt;. In this work (to be presented at NeurIPS 2023), we revisit the vulnerabilities of more extensively studied linear models under indiscriminate poisoning attacks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Congratulations, Dr. Suya!</title>
      <link>//uvasrg.github.io/congratulations-dr.-suya/</link>
      <pubDate>Tue, 11 Jul 2023 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/congratulations-dr.-suya/</guid>
      <description>&lt;p&gt;Congratulations to &lt;a href=&#34;fsuya.org&#34;&gt;Fnu Suya&lt;/a&gt; for successfully defending&#xA;his PhD thesis!&lt;/p&gt;&#xA;&lt;p&gt;Suya will join the Unversity of Maryland as a MC2 Postdoctoral Fellow&#xA;at the &lt;a href=&#34;https://cyber.umd.edu/about&#34;&gt;Maryland Cybersecurity Center&lt;/a&gt; this fall.&lt;/p&gt;&#xA;&lt;h2 id=&#34;heading&#34;&gt;&lt;/h2&gt;&#xA;&lt;center&gt;&#xA;&lt;em&gt;&#xA;On the Limits of Data Poisoning Attacks&#xA;&lt;/em&gt;&#xA;&lt;/center&gt;&#xA;&lt;h2 id=&#34;heading-1&#34;&gt;&lt;/h2&gt;&#xA;&lt;p&gt;Current machine learning models require large amounts of labeled training data, which are often collected from untrusted sources. Models trained on these potentially manipulated data points are prone to data poisoning attacks. My research aims to gain a deeper understanding on the limits of two types of data poisoning attacks: indiscriminate poisoning attacks, where the attacker aims to increase the test error on the entire dataset; and subpopulation poisoning attacks, where the attacker aims to increase the test error on a defined subset of the distribution. We first present an empirical poisoning attack that encodes the attack objectives into target models and then generates poisoning points that induce the target models (and hence the encoded objectives) with provable convergence. This attack achieves state-of-the-art performance for a diverse set of attack objectives and quantifies a lower bound to the performance of best possible poisoning attacks. In the broader sense, because the attack guarantees convergence to the target model which encodes the desired attack objective, our attack can also be applied to objectives related to other trustworthy aspects (e.g., privacy, fairness) of machine learning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>CVPR 2023: Manipulating Transfer Learning for Property Inference</title>
      <link>//uvasrg.github.io/cvpr-2023-manipulating-transfer-learning-for-property-inference/</link>
      <pubDate>Tue, 02 May 2023 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/cvpr-2023-manipulating-transfer-learning-for-property-inference/</guid>
      <description>&lt;h1 id=&#34;manipulating-transfer-learning-for-property-inference&#34;&gt;Manipulating Transfer Learning for Property Inference&lt;/h1&gt;&#xA;&lt;p&gt;Transfer learning is a popular method to train deep learning models&#xA;efficiently. By reusing parameters from upstream pre-trained models,&#xA;the downstream trainer can use fewer computing resources to train&#xA;downstream models, compared to training models from scratch.&lt;/p&gt;&#xA;&lt;p&gt;The figure below shows the typical process of transfer learning for&#xA;vision tasks:&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/mtlpi/fig1.png&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/mtlpi/fig1.png&#34; width=&#34;80%&#34;&gt;&lt;/a&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;However, the nature of transfer learning can be exploited by a&#xA;malicious upstream trainer, leading to severe risks to the downstream&#xA;trainer.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Visualizing Poisoning</title>
      <link>//uvasrg.github.io/visualizing-poisoning/</link>
      <pubDate>Tue, 26 Jul 2022 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/visualizing-poisoning/</guid>
      <description>&lt;p&gt;&lt;em&gt;How does a poisoning attack work and why are some groups more&#xA;susceptible to being victimized by a poisoning attack?&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;We&amp;rsquo;ve posted work that helps understand how poisoning attacks work&#xA;with some engaging visualizations:&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;//uvasrg.github.io/poisoning&#34;&gt;Poisoning Attacks and Subpopulation Susceptibility&lt;/a&gt;&lt;br&gt;&#xA;&lt;em&gt;An Experimental Exploration on the Effectiveness of Poisoning Attacks&lt;/em&gt;&lt;br&gt;&#xA;Evan Rose, Fnu Suya, and David Evans&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/poisoning&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/visualizingpoisoning.png&#34; width=&#34;85%&#34;&gt;&lt;/a&gt;&lt;br&gt;&#xA;Follow &lt;a href=&#34;//uvasrg.github.io/poisoning&#34;&gt;the link&lt;/a&gt; to try the interactive version!&#xA;&lt;/center&gt;&#xA;&lt;h1 id=&#34;heading&#34;&gt;&lt;/h1&gt;&#xA;&lt;p&gt;Machine learning is susceptible to poisoning attacks in which&#xA;adversaries inject maliciously crafted training data into the training&#xA;set to induce specific model behavior. We focus on subpopulation&#xA;attacks, in which the attacker&amp;rsquo;s goal is to induce a model that&#xA;produces a targeted and incorrect output (label blue in our demos) for&#xA;a particular subset of the input space (colored orange). We study the&#xA;question, which subpopulations are the most vulnerable to an attack&#xA;and why?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Model-Targeted Poisoning Attacks with Provable Convergence</title>
      <link>//uvasrg.github.io/model-targeted-poisoning-attacks-with-provable-convergence/</link>
      <pubDate>Tue, 29 Jun 2021 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/model-targeted-poisoning-attacks-with-provable-convergence/</guid>
      <description>&lt;p&gt;(Post by Sean Miller, using images adapted from Suya&amp;rsquo;s talk slides)&lt;/p&gt;&#xA;&lt;h2 id=&#34;data-poisoning-attacks&#34;&gt;Data Poisoning Attacks&lt;/h2&gt;&#xA;&lt;p&gt;Machine learning models are often trained using data from untrusted&#xA;sources, leaving them open to poisoning attacks where adversaries use&#xA;their control over a small fraction of that training data to poison&#xA;the model in a particular way.&lt;/p&gt;&#xA;&lt;p&gt;Most work on poisoning attacks is directly driven by an attacker&amp;rsquo;s&#xA;objective, where the adversary chooses poisoning points that maximize&#xA;some target objective. Our work focuses on &lt;em&gt;model-targeted&lt;/em&gt; poisoning&#xA;attacks, where the adversary splits the attack into choosing a target&#xA;model that satisfies the objective and then choosing poisoning points&#xA;that induce the target model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to Hide a Backdoor</title>
      <link>//uvasrg.github.io/how-to-hide-a-backdoor/</link>
      <pubDate>Thu, 06 May 2021 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/how-to-hide-a-backdoor/</guid>
      <description>&lt;p&gt;The Register has an article on our recent work on &lt;a href=&#34;https://arxiv.org/abs/2104.15129&#34;&gt;&lt;em&gt;Stealthy Backdoors as Compression Artifacts&lt;/em&gt;&lt;/a&gt;:&#xA;Thomas Claburn, &lt;a href=&#34;https://www.theregister.com/AMP/2021/05/05/ai_backdoors/&#34;&gt;&lt;em&gt;How to hide a backdoor in AI software — Neural networks can be aimed to misbehave when squeezed&lt;/em&gt;&lt;/a&gt;, The Register, 5 May 2021.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hybrid Batch Attacks at USENIX Security 2020</title>
      <link>//uvasrg.github.io/hybrid-batch-attacks-at-usenix-security-2020/</link>
      <pubDate>Thu, 13 Aug 2020 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/hybrid-batch-attacks-at-usenix-security-2020/</guid>
      <description>&lt;p&gt;Here&amp;rsquo;s the video for Suya&amp;rsquo;s presentation on &lt;a href=&#34;//uvasrg.github.io/usenix-security-2020-hybrid-batch-attacks&#34;&gt;Hybrid Batch Attacks&lt;/a&gt; at USENIX Security 2020:&lt;/p&gt;&#xA;&lt;center&gt;&#xA;  &lt;video width=&#34;90%&#34; id=&#34;usenix-media-video-1&#34; data-setup=&#34;{}&#34; poster=&#34;&#34; class=&#34;video-js vjs-default-skin vjs-big-play-centered&#34; preload=&#34;auto&#34; controls&gt;&#xA;    &lt;source src=&#39;https://2459d6dc103cb5933875-c0245c5c937c5dedcca3f1764ecc9b2f.ssl.cf2.rackcdn.com/sec20/videos/0813/s5_machine_learning_1/3_sec20summer-paper412-presentation-video.mp4&#39; type=&#39;video/mp4; codecs=&#34;avc1.42E01E, mp4a.40.2&#34;&#39;&gt;&#xA;  &lt;/video&gt;&lt;br&gt; &#xA;&lt;a href=&#34;https://2459d6dc103cb5933875-c0245c5c937c5dedcca3f1764ecc9b2f.ssl.cf2.rackcdn.com/sec20/videos/0813/s5_machine_learning_1/3_sec20summer-paper412-presentation-video.mp4&#34;&gt;Download Video [mp4]&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;a href=&#34;//uvasrg.github.io/usenix-security-2020-hybrid-batch-attacks&#34;&gt;Blog Post&lt;/a&gt;&lt;br&gt;&#xA;Paper: [&lt;a href=&#34;//uvasrg.github.io/docs/hybrid_attack.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/1908.07000&#34;&gt;arXiv&lt;/a&gt;]&lt;/p&gt;</description>
    </item>
    <item>
      <title>USENIX Security 2020: Hybrid Batch Attacks</title>
      <link>//uvasrg.github.io/usenix-security-2020-hybrid-batch-attacks/</link>
      <pubDate>Sat, 14 Dec 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/usenix-security-2020-hybrid-batch-attacks/</guid>
      <description>&lt;p&gt;&lt;b&gt;&lt;font color=&#34;red&#34;&gt;New:&lt;/font&gt; &lt;a href=&#34;//uvasrg.github.io/hybrid-batch-attacks-at-usenix-security-2020/&#34;&gt;Video Presentation&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;finding-black-box-adversarial-examples-with-limited-queries&#34;&gt;Finding Black-box Adversarial Examples with Limited Queries&lt;/h2&gt;&#xA;&lt;p&gt;Black-box attacks generate adversarial examples (AEs) against deep&#xA;neural networks with only API access to the victim model.&lt;/p&gt;&#xA;&lt;p&gt;Existing black-box attacks can be grouped into two main categories:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Transfer Attacks&lt;/strong&gt; use white-box attacks on local models to find&#xA;candidate adversarial examples that transfer to the target model.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Optimization Attacks&lt;/strong&gt; use queries to the target model and apply&#xA;optimization techniques to search for adversarial examples.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Research Symposium Posters</title>
      <link>//uvasrg.github.io/research-symposium-posters/</link>
      <pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/research-symposium-posters/</guid>
      <description>&lt;p&gt;Five students from our group presented posters at the department&amp;rsquo;s&#xA;&lt;a href=&#34;https://engineering.virginia.edu/cs-research-symposium-fall-2019&#34;&gt;Fall Research&#xA;Symposium&lt;/a&gt;:&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;iframe src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vQcaahIxPyCHIMJti6tRB9HM_RreRhZkGH4wCN7YjTwiHSqcHod9v3hDFj-ZS1TtXp9OtBEBCV8OPH4/embed?start=false&amp;loop=false&amp;delayms=3000&#34; frameborder=&#34;0&#34; width=&#34;764&#34; height=&#34;453&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;&lt;br&gt;&#xA;Anshuman Suri&#39;s Overview Talk&#xA;&lt;/center&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/evaluatingdpml-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt; &lt;br&gt;&#xA;Bargav Jayaraman, &lt;em&gt;Evaluating Differentially Private Machine Learning In Practice&lt;/em&gt; &#xA;[&lt;a href=&#34;&#xA;&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/evaluatingdpml-poster.pdf&#34;&gt;Poster&lt;/a&gt;]&lt;br&gt;&#xA;[&lt;a href=&#34;https://www.cs.virginia.edu/~evans/pubs/usenix2019/&#34;&gt;Paper&lt;/a&gt; (USENIX Security 2019)]&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/pretrainedvulnerable-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt;&lt;br&gt;&#xA;Hannah Chen [&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/pretrainedvulnerable-poster.pdf&#34;&gt;Poster&lt;/a&gt;]&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/measuringconcentration-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt;&lt;br&gt;&#xA;Xiao Zhang [&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/measuringconcentration-poster.pdf&#34;&gt;Poster&lt;a&gt;]&lt;br&gt;&#xA;[&lt;a href=&#34;https://arxiv.org/abs/1905.12202&#34;&gt;Paper&lt;/a&gt; (NeurIPS 2019)]&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/diversemodels-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt;&lt;br&gt;&#xA;Mainudding Jonas [&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/diversemodels-poster.pdf&#34;&gt;Poster&lt;/a&gt;]&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/hybridbatch-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt;&lt;br&gt;&#xA;Fnu Suya [&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/hybridbatch-poster.pdf&#34;&gt;Poster&lt;a&gt;]&lt;br&gt;&#xA;[&lt;a href=&#34;https://arxiv.org/abs/1908.07000&#34;&gt;Paper&lt;/a&gt; (USENIX Security 2020)]&#xA;&lt;/center&gt;</description>
    </item>
    <item>
      <title>Wahoos at Oakland</title>
      <link>//uvasrg.github.io/wahoos-at-oakland/</link>
      <pubDate>Tue, 29 May 2018 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/wahoos-at-oakland/</guid>
      <description>&lt;h2 id=&#34;uva-group-dinner-at-ieee-security-and-privacy-2018&#34;&gt;UVA Group Dinner at IEEE Security and Privacy 2018&lt;/h2&gt;&#xA;&lt;p&gt;Including our newest faculty member, &lt;a href=&#34;https://www.cs.purdue.edu/homes/kwon58/#summary&#34;&gt;Yongwhi Kwon&lt;/a&gt;, joining UVA in Fall 2018!&lt;br /&gt;&lt;/p&gt;&#xA;&lt;center&gt;&lt;br /&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/srg2018/ORG_DSC07202.jpg&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/srg2018/ORG_DSC07202.jpg&#34; width=&#34;680&#34;&gt;&lt;/a&gt;&lt;br /&gt;&#xA;&lt;small&gt;Yuan Tian, Fnu Suya, Mainuddin Jonas, Yongwhi Kwon, David Evans, Weihang Wang, Aihua&amp;nbsp;Chen,&amp;nbsp;Weilin&amp;nbsp;Xu&lt;/small&gt;&lt;br /&gt;&#xA;&lt;/center&gt;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;&#xA;## Poster Session&#xA;&lt;table width=&#34;100%&#34;&gt;&#xA;&lt;tr valign=&#34;top&#34;&gt;&#xA;&lt;td width=&#34;50%&#34; align=&#34;center&#34;&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/srg2018/IMG_20180521_193906.jpg&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/srg2018/IMG_20180521_193906-3.jpg&#34; height=&#34;360&#34;&gt;&lt;/a&gt;&lt;br /&gt;&#xA;Fnu Suya (with Yuan Tian and David Evans), &lt;em&gt;Adversaries Don’t Care About Averages: Batch Attacks on Black-Box Classifiers&lt;/em&gt; &lt;a href=&#34;https://www.ieee-security.org/TC/SP2018/poster-abstracts/oakland2018-paper37-poster-abstract.pdf&#34;&gt;[PDF]&lt;/a&gt;&#xA;&lt;/td&gt;&#xA;&lt;td width=&#34;50%&#34; align=&#34;center&#34;&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/srg2018/IMG_20180521_193914.jpg&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/srg2018/IMG_20180521_193914-2.jpg&#34; height=&#34;360&#34;&gt;&lt;/a&gt;&lt;br /&gt;&#xA;Mainuddin Jonas (with David Evans), &lt;em&gt;Enhancing Adversarial Example Defenses Using Internal Layers&lt;/em&gt; &lt;a href=&#34;https://www.ieee-security.org/TC/SP2018/poster-abstracts/oakland2018-paper29-poster-abstract.pdf&#34;&gt;[PDF]&lt;/a&gt;&#xA;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;tr valign=&#34;top&#34;&gt;&#xA;&lt;td width=&#34;50%&#34; align=&#34;center&#34;&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/srg2018/IMG_20180522_153017.jpg&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/srg2018/IMG_20180522_153017-2.jpg&#34; height=&#34;300&#34;&gt;&lt;/a&gt;&#xA;&lt;/td&gt;&#xA;&lt;td width=&#34;50%&#34; align=&#34;center&#34;&gt;&#xA;&lt;a href=&#34;//uvasrg.github.io/images/srg2018/IMG_20180522_153109.jpg&#34;&gt;&lt;img src=&#34;//uvasrg.github.io/images/srg2018/IMG_20180522_153109-2.jpg&#34; height=&#34;300&#34;&gt;&lt;/a&gt;&#xA;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/table&gt;</description>
    </item>
  </channel>
</rss>
