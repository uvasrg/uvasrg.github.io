<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hannah Chen on Security Research Group</title>
    <link>//uvasrg.github.io/tags/hannah-chen/</link>
    <description>Recent content in Hannah Chen on Security Research Group</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Sun, 11 Aug 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/tags/hannah-chen/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The Mismeasure of Man and Models</title>
      <link>//uvasrg.github.io/the-mismeasure-of-man-and-models/</link>
      <pubDate>Sun, 11 Aug 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/the-mismeasure-of-man-and-models/</guid>
      <description>&lt;h1 id=&#34;evaluating-allocational-harms-in-large-language-models&#34;&gt;Evaluating Allocational Harms in Large Language Models&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;Blog post written by &lt;a href=&#34;https://hannahxchen.github.io/&#34;&gt;Hannah Chen&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Our work considers &lt;i&gt;allocational harms&lt;/i&gt; that arise when model predictions are used to distribute scarce resources or opportunities.&lt;/p&gt;&#xA;&lt;h2 id=&#34;current-bias-metrics-do-not-reliably-reflect-allocation-disparities&#34;&gt;Current Bias Metrics Do Not Reliably Reflect Allocation Disparities&lt;/h2&gt;&#xA;&lt;p&gt;Several methods have been proposed to audit large language models (LLMs) for bias when used in critical decision-making, such as resume screening for hiring. Yet, these methods focus on &lt;i&gt;predictions&lt;/i&gt;, without considering how the predictions are used to make &lt;i&gt;decisions&lt;/i&gt;. In many settings, making decisions involve prioritizing options due to limited resource constraints. We find that prediction-based evaluation methods, which measure bias as the &lt;i&gt;average performance gap&lt;/i&gt; (Î´) in prediction outcomes, do not reliably reflect disparities in allocation decision outcomes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Adjectives Can Reveal Gender Biases Within NLP Models</title>
      <link>//uvasrg.github.io/adjectives-can-reveal-gender-biases-within-nlp-models/</link>
      <pubDate>Thu, 17 Aug 2023 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/adjectives-can-reveal-gender-biases-within-nlp-models/</guid>
      <description>&lt;p&gt;Post by &lt;strong&gt;Jason Briegel&lt;/strong&gt; and &lt;a href=&#34;https://hannahxchen.github.io/&#34;&gt;&lt;strong&gt;Hannah Chen&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Because NLP models are trained with human corpora (and now,&#xA;increasingly on text generated by other NLP models that were&#xA;originally trained on human language), they are prone to inheriting&#xA;common human stereotypes and biases. This is problematic, because with&#xA;their growing prominence they may further propagate these stereotypes&#xA;&lt;a href=&#34;https://arxiv.org/abs/1906.08976&#34;&gt;(Sun et al., 2019)&lt;/a&gt;. For example,&#xA;interest is growing in mitigating bias in the field of machine&#xA;translation, where systems such as Google translate were observed to&#xA;default to translating gender-neutral pronouns as male pronouns, even&#xA;with feminine cues &lt;a href=&#34;https://doi.org/10.1162/tacl_a_00401&#34;&gt;(Savoldi et al.,&#xA;2021)&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models</title>
      <link>//uvasrg.github.io/balancing-tradeoffs-between-fickleness-and-obstinacy-in-nlp-models/</link>
      <pubDate>Sun, 13 Nov 2022 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/balancing-tradeoffs-between-fickleness-and-obstinacy-in-nlp-models/</guid>
      <description>&lt;p&gt;Post by &lt;a href=&#34;https://hannahxchen.github.io/&#34;&gt;Hannah Chen&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Our work on balanced adversarial training looks at how to train models&#xA;that are robust to two different types of adversarial examples:&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://hannahxchen.github.io/&#34;&gt;Hannah Chen&lt;/a&gt;, &lt;a href=&#34;http://yangfengji.net/&#34;&gt;Yangfeng&#xA;Ji&lt;/a&gt;, &lt;a href=&#34;http://www.cs.virginia.edu/~evans/&#34;&gt;David&#xA;Evans&lt;/a&gt;. &lt;em&gt;Balanced Adversarial&#xA;Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP&#xA;Models&lt;/em&gt;. In &lt;a href=&#34;https://2022.emnlp.org/&#34;&gt;&lt;em&gt;The 2022 Conference on Empirical Methods in Natural&#xA;Language Processing&lt;/em&gt;&lt;/a&gt; (EMNLP), Abu Dhabi,&#xA;7-11 December 2022.  [&lt;a href=&#34;https://arxiv.org/abs/2210.11498&#34;&gt;ArXiv&lt;/a&gt;]&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube-nocookie.com/embed/xQH51lIVDyY&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&#xA;&lt;/center&gt;&#xA;&lt;h2 id=&#34;adversarial-examples&#34;&gt;Adversarial Examples&lt;/h2&gt;&#xA;&lt;p&gt;At the broadest level, an adversarial example is an input crafted intentionally to confuse a model. However, most work focus on the defintion as an input constructed by applying a small perturbation that preserves the ground truth label but changes model&amp;rsquo;s output &lt;a href=&#34;https://arxiv.org/abs/1412.6572&#34;&gt;(Goodfellow et al., 2015)&lt;/a&gt;. We refer it as a &lt;strong&gt;fickle adversarial example&lt;/strong&gt;. On the other hand, attackers can target an opposite objective where the inputs are made with minimal changes that change the ground truth labels but retain model&amp;rsquo;s predictions &lt;a href=&#34;https://arxiv.org/abs/1811.00401&#34;&gt;(Jacobsen et al., 2018)&lt;/a&gt;. We refer these malicious inputs as &lt;strong&gt;obstinate adversarial examples&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pointwise Paraphrase Appraisal is Potentially Problematic</title>
      <link>//uvasrg.github.io/pointwise-paraphrase-appraisal-is-potentially-problematic/</link>
      <pubDate>Tue, 07 Jul 2020 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/pointwise-paraphrase-appraisal-is-potentially-problematic/</guid>
      <description>&lt;p&gt;Hannah Chen presented her paper on &lt;em&gt;Pointwise Paraphrase Appraisal is&#xA;Potentially Problematic&lt;/em&gt; at the &lt;a href=&#34;https://sites.google.com/view/acl20studentresearchworkshop/&#34;&gt;ACL 2020 Student Research&#xA;Workshop&lt;/a&gt;:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;The prevailing approach for training and evaluating paraphrase&#xA;identification models is constructed as a binary classification&#xA;problem: the model is given a pair of sentences, and is judged by how&#xA;accurately it classifies pairs as either paraphrases or&#xA;non-paraphrases. This pointwise-based evaluation method does not match&#xA;well the objective of most real world applications, so the goal of our&#xA;work is to understand how models which perform well under pointwise&#xA;evaluation may fail in practice and find better methods for evaluating&#xA;paraphrase identification models. As a first step towards that goal,&#xA;we show that although the standard way of fine-tuning BERT for&#xA;paraphrase identification by pairing two sentences as one sequence&#xA;results in a model with state-of-the-art performance, that model may&#xA;perform poorly on simple tasks like identifying pairs with two&#xA;identical sentences. Moreover, we show that these models may even&#xA;predict a pair of randomly-selected sentences with higher paraphrase&#xA;score than a pair of identical ones.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Research Symposium Posters</title>
      <link>//uvasrg.github.io/research-symposium-posters/</link>
      <pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/research-symposium-posters/</guid>
      <description>&lt;p&gt;Five students from our group presented posters at the department&amp;rsquo;s&#xA;&lt;a href=&#34;https://engineering.virginia.edu/cs-research-symposium-fall-2019&#34;&gt;Fall Research&#xA;Symposium&lt;/a&gt;:&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;iframe src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vQcaahIxPyCHIMJti6tRB9HM_RreRhZkGH4wCN7YjTwiHSqcHod9v3hDFj-ZS1TtXp9OtBEBCV8OPH4/embed?start=false&amp;loop=false&amp;delayms=3000&#34; frameborder=&#34;0&#34; width=&#34;764&#34; height=&#34;453&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;&lt;br&gt;&#xA;Anshuman Suri&#39;s Overview Talk&#xA;&lt;/center&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/evaluatingdpml-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt; &lt;br&gt;&#xA;Bargav Jayaraman, &lt;em&gt;Evaluating Differentially Private Machine Learning In Practice&lt;/em&gt; &#xA;[&lt;a href=&#34;&#xA;&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/evaluatingdpml-poster.pdf&#34;&gt;Poster&lt;/a&gt;]&lt;br&gt;&#xA;[&lt;a href=&#34;https://www.cs.virginia.edu/~evans/pubs/usenix2019/&#34;&gt;Paper&lt;/a&gt; (USENIX Security 2019)]&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/pretrainedvulnerable-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt;&lt;br&gt;&#xA;Hannah Chen [&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/pretrainedvulnerable-poster.pdf&#34;&gt;Poster&lt;/a&gt;]&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/measuringconcentration-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt;&lt;br&gt;&#xA;Xiao Zhang [&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/measuringconcentration-poster.pdf&#34;&gt;Poster&lt;a&gt;]&lt;br&gt;&#xA;[&lt;a href=&#34;https://arxiv.org/abs/1905.12202&#34;&gt;Paper&lt;/a&gt; (NeurIPS 2019)]&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/diversemodels-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt;&lt;br&gt;&#xA;Mainudding Jonas [&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/diversemodels-poster.pdf&#34;&gt;Poster&lt;/a&gt;]&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;/br&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA;&lt;embed src=&#34;//uvasrg.github.io/docs/symposters2019/hybridbatch-poster.pdf&#34; width=&#34;95%&#34; height=&#34;300&#34; type=&#34;application/pdf&#34;&gt;&lt;br&gt;&#xA;Fnu Suya [&lt;a href=&#34;//uvasrg.github.io/docs/symposters2019/hybridbatch-poster.pdf&#34;&gt;Poster&lt;a&gt;]&lt;br&gt;&#xA;[&lt;a href=&#34;https://arxiv.org/abs/1908.07000&#34;&gt;Paper&lt;/a&gt; (USENIX Security 2020)]&#xA;&lt;/center&gt;</description>
    </item>
  </channel>
</rss>
