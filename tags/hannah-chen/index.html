<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Hannah Chen | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
        <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
	      
	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      
<div style="margin-top:16px; margin-left: auto; margin-right: auto; max-width: 800px;">
<div class="row">
  <div class="column small-12">
    
    
    <h2><a href="/the-mismeasure-of-man-and-models/">The Mismeasure of Man and Models</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-08-11 00:00:00 &#43;0000 UTC" itemprop="datePublished">11 August 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/hannah-chen">Hannah Chen</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yangfeng-ji">Yangfeng Ji</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/auditing">auditing</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/llms">LLMs</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bias">bias</a>
    
  </span>
  
  
</div>


<h1 id="evaluating-allocational-harms-in-large-language-models">Evaluating Allocational Harms in Large Language Models</h1>
<p><strong>Blog post written by <a href="https://hannahxchen.github.io/">Hannah Chen</a></strong></p>
<p>Our work considers <i>allocational harms</i> that arise when model predictions are used to distribute scarce resources or opportunities.</p>
<h2 id="current-bias-metrics-do-not-reliably-reflect-allocation-disparities">Current Bias Metrics Do Not Reliably Reflect Allocation Disparities</h2>
<p>Several methods have been proposed to audit large language models (LLMs) for bias when used in critical decision-making, such as resume screening for hiring. Yet, these methods focus on <i>predictions</i>, without considering how the predictions are used to make <i>decisions</i>. In many settings, making decisions involve prioritizing options due to limited resource constraints. We find that prediction-based evaluation methods, which measure bias as the <i>average performance gap</i> (δ) in prediction outcomes, do not reliably reflect disparities in allocation decision outcomes.</p>
<p class="text-right"><a href="/the-mismeasure-of-man-and-models/">Read More…</a></p>
	

    
    <h2><a href="/adjectives-can-reveal-gender-biases-within-nlp-models/">Adjectives Can Reveal Gender Biases Within NLP Models</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-08-17 00:00:00 &#43;0000 UTC" itemprop="datePublished">17 August 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jason-briegel">Jason Briegel</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/hannah-chen">Hannah Chen</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/llms">LLMs</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/generative-ai">generative AI</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bias">bias</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/machine-learning">machine learning</a>
    
  </span>
  
  
</div>


<p>Post by <strong>Jason Briegel</strong> and <a href="https://hannahxchen.github.io/"><strong>Hannah Chen</strong></a></p>
<p>Because NLP models are trained with human corpora (and now,
increasingly on text generated by other NLP models that were
originally trained on human language), they are prone to inheriting
common human stereotypes and biases. This is problematic, because with
their growing prominence they may further propagate these stereotypes
<a href="https://arxiv.org/abs/1906.08976">(Sun et al., 2019)</a>. For example,
interest is growing in mitigating bias in the field of machine
translation, where systems such as Google translate were observed to
default to translating gender-neutral pronouns as male pronouns, even
with feminine cues <a href="https://doi.org/10.1162/tacl_a_00401">(Savoldi et al.,
2021)</a>.</p>
<p class="text-right"><a href="/adjectives-can-reveal-gender-biases-within-nlp-models/">Read More…</a></p>
	

    
    <h2><a href="/balancing-tradeoffs-between-fickleness-and-obstinacy-in-nlp-models/">Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-11-13 00:00:00 &#43;0000 UTC" itemprop="datePublished">13 November 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/hannah-chen">Hannah Chen</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yangfeng-ji">Yangfeng Ji</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-examples">adversarial examples</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/nlp">NLP</a>
    
  </span>
  
  
</div>


<p>Post by <a href="https://hannahxchen.github.io/">Hannah Chen</a>.</p>
<p>Our work on balanced adversarial training looks at how to train models
that are robust to two different types of adversarial examples:</p>
<p><a href="https://hannahxchen.github.io/">Hannah Chen</a>, <a href="http://yangfengji.net/">Yangfeng
Ji</a>, <a href="http://www.cs.virginia.edu/~evans/">David
Evans</a>. <em>Balanced Adversarial
Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP
Models</em>. In <a href="https://2022.emnlp.org/"><em>The 2022 Conference on Empirical Methods in Natural
Language Processing</em></a> (EMNLP), Abu Dhabi,
7-11 December 2022.  [<a href="https://arxiv.org/abs/2210.11498">ArXiv</a>]</p>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/xQH51lIVDyY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>
<h2 id="adversarial-examples">Adversarial Examples</h2>
<p>At the broadest level, an adversarial example is an input crafted intentionally to confuse a model. However, most work focus on the defintion as an input constructed by applying a small perturbation that preserves the ground truth label but changes model&rsquo;s output <a href="https://arxiv.org/abs/1412.6572">(Goodfellow et al., 2015)</a>. We refer it as a <strong>fickle adversarial example</strong>. On the other hand, attackers can target an opposite objective where the inputs are made with minimal changes that change the ground truth labels but retain model&rsquo;s predictions <a href="https://arxiv.org/abs/1811.00401">(Jacobsen et al., 2018)</a>. We refer these malicious inputs as <strong>obstinate adversarial examples</strong>.</p>
<p class="text-right"><a href="/balancing-tradeoffs-between-fickleness-and-obstinacy-in-nlp-models/">Read More…</a></p>
	

    
    <h2><a href="/pointwise-paraphrase-appraisal-is-potentially-problematic/">Pointwise Paraphrase Appraisal is Potentially Problematic</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2020-07-07 00:00:00 &#43;0000 UTC" itemprop="datePublished">7 July 2020</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/natural-language-processing">natural language processing</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/hannah-chen">Hannah Chen</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yangfeng-ji">Yangfeng Ji</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/acl">ACL</a>
    
  </span>
  
  
</div>


<p>Hannah Chen presented her paper on <em>Pointwise Paraphrase Appraisal is
Potentially Problematic</em> at the <a href="https://sites.google.com/view/acl20studentresearchworkshop/">ACL 2020 Student Research
Workshop</a>:</p>
<blockquote>
<p>The prevailing approach for training and evaluating paraphrase
identification models is constructed as a binary classification
problem: the model is given a pair of sentences, and is judged by how
accurately it classifies pairs as either paraphrases or
non-paraphrases. This pointwise-based evaluation method does not match
well the objective of most real world applications, so the goal of our
work is to understand how models which perform well under pointwise
evaluation may fail in practice and find better methods for evaluating
paraphrase identification models. As a first step towards that goal,
we show that although the standard way of fine-tuning BERT for
paraphrase identification by pairing two sentences as one sequence
results in a model with state-of-the-art performance, that model may
perform poorly on simple tasks like identifying pairs with two
identical sentences. Moreover, we show that these models may even
predict a pair of randomly-selected sentences with higher paraphrase
score than a pair of identical ones.</p>
<p class="text-right"><a href="/pointwise-paraphrase-appraisal-is-potentially-problematic/">Read More…</a></p>
	

    
    <h2><a href="/research-symposium-posters/">Research Symposium Posters</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-10-08 00:00:00 &#43;0000 UTC" itemprop="datePublished">8 October 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/research">research</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/mainuddin-jonas">Mainuddin Jonas</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bargav-jayaraman">Bargav Jayaraman</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/hannah-chen">Hannah Chen</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>
    
  </span>
  
  
</div>


<p>Five students from our group presented posters at the department&rsquo;s
<a href="https://engineering.virginia.edu/cs-research-symposium-fall-2019">Fall Research
Symposium</a>:</p>
<center>
<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vQcaahIxPyCHIMJti6tRB9HM_RreRhZkGH4wCN7YjTwiHSqcHod9v3hDFj-ZS1TtXp9OtBEBCV8OPH4/embed?start=false&loop=false&delayms=3000" frameborder="0" width="764" height="453" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe><br>
Anshuman Suri's Overview Talk
</center>
<center>
<embed src="/docs/symposters2019/evaluatingdpml-poster.pdf" width="95%" height="300" type="application/pdf"> <br>
Bargav Jayaraman, <em>Evaluating Differentially Private Machine Learning In Practice</em> 
[<a href="
<a href="/docs/symposters2019/evaluatingdpml-poster.pdf">Poster</a>]<br>
[<a href="https://www.cs.virginia.edu/~evans/pubs/usenix2019/">Paper</a> (USENIX Security 2019)]
</center>
<p><br></br></p>
<center>
<embed src="/docs/symposters2019/pretrainedvulnerable-poster.pdf" width="95%" height="300" type="application/pdf"><br>
Hannah Chen [<a href="/docs/symposters2019/pretrainedvulnerable-poster.pdf">Poster</a>]
</center>
<p><br></br></p>
<center>
<embed src="/docs/symposters2019/measuringconcentration-poster.pdf" width="95%" height="300" type="application/pdf"><br>
Xiao Zhang [<a href="/docs/symposters2019/measuringconcentration-poster.pdf">Poster<a>]<br>
[<a href="https://arxiv.org/abs/1905.12202">Paper</a> (NeurIPS 2019)]
</center>
<p><br></br></p>
<center>
<embed src="/docs/symposters2019/diversemodels-poster.pdf" width="95%" height="300" type="application/pdf"><br>
Mainudding Jonas [<a href="/docs/symposters2019/diversemodels-poster.pdf">Poster</a>]
</center>
<p><br></br></p>
<center>
<embed src="/docs/symposters2019/hybridbatch-poster.pdf" width="95%" height="300" type="application/pdf"><br>
Fnu Suya [<a href="/docs/symposters2019/hybridbatch-poster.pdf">Poster<a>]<br>
[<a href="https://arxiv.org/abs/1908.07000">Paper</a> (USENIX Security 2020)]
</center>

	

    
    <div class="row">
  <div class="column small-12">
    <ul class="pagination" role="navigation" aria-label="Pagination">
      
      <li><span>Page 1 of 1</span></li>      
      
    </ul>    
    All Posts by <a href="//uvasrg.github.io/categories">Category</a> or <a href="//uvasrg.github.io/tags">Tags</a>.

  </div>
</div>

  </div>
</div>
</div>

    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
