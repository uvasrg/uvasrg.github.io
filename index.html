<!doctype html>
<html class="no-js" lang="en-us">
  <head>
	<meta name="generator" content="Hugo 0.71.0" />
    <meta charset="utf-8">
    <title>Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
	      
	    </div>
	  </div>
	    
	  
	    <div class="top-bar" id="site-menu" >	      
	      <div class="top-bar-title show-for-medium site-title">
		<a href="//uvasrg.github.io/">Security Research Group</a>
	      </div>
	      <div class="top-bar-left">
		<ul class="menu vertical medium-horizontal">
		  
		  
		</ul>
	      </div>
	      <div class="top-bar-right show-for-medium">
		
	         <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
		
	      </div>
	    </div>
	  
	</nav>
      
    </header>
    
    <main>
      





<div class="container">
 <div>

    <div class="column small-18 medium-9">
      
    <div class="content">

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
           <h1 id="centersecurity-and-privacy-research-at-the-university-of-virginiacenter"><center>Security and Privacy Research at the University of Virginia</center></h1>
<p></p>
<div class="row">
<div class="column small-10 medium-6">
<p>Our research seeks to empower individuals and organizations to control
how their data is used.  We use techniques from cryptography,
programming languages, machine learning, operating systems, and other
areas to both understand and improve the privacy and security of
computing as practiced today, and as envisioned in the future. A major
current focus is on <em>adversarial machine learning</em>.</p>
</p> 
 <p>
<p>Everyone is welcome at our research group meetings. To get
announcements, join our <a
href="https://teams.microsoft.com/l/team/19%3aWdkw2xYq6taXh-0OftqQdt8SQ2vyvUI_Z0ZL39APghY1%40thread.tacv2/conversations?groupId=58076b41-c835-4a07-abaa-705bc7cca101&tenantId=7b3480c7-3707-4873-8b77-e216733a65ac">Teams Group</a> (any
<em>@virginia.edu</em> email address can join themsleves; others should <a href="mailto:evans@virginia.edu">email me</a> to request an invitation). </p> </div></p>
<div class="column small-10 medium-6">
<center> <a
href="/images/srg-lunch-2022-08-22.png"><img
src="/images/srg-lunch-2022-08-22-small.png" alt="SRG lunch"
width=98%></a></br> <b>Security Research Group Lunch</b> <font
size="-1">(22&nbsp;August&nbsp;2022)</font><br> <div
class="smallcaption">
<a href="https://bargavjayaraman.github.io/">Bargav&nbsp;Jayaraman</a>,
<a href="https://www.josephinelamp.com/">Josephine&nbsp;Lamp</a>,
<a href="https://hannahxchen.github.io/">Hannah&nbsp;Chen</a>,
<A href="https://www.linkedin.com/in/minjun-elena-long-06a283173/">Elena&nbsp;Long</a>,
Yanjin&nbsp;Chen,<br>
<a href="https://web.archive.org/web/20190909071143/http://www.cs.virginia.edu:80/~sza4uq/">Samee&nbsp;Zahur</a>&nbsp;(PhD&nbsp;2016),
<a href="https://sites.google.com/virginia.edu/anshuman/home">Anshuman&nbsp;Suri</a>,
<A href="https://fsuya.org/">Fnu&nbsp;Suya</a>,
Tingwei&nbsp;Zhang,
Scott&nbsp;Hong
</font> </center>
 </p> </div> </div>
<div class="row">
<div class="column small-10 medium-5">
<div class="mainsection">Active Projects</div>
<p><a href="/privacy/"><b>Privacy for Machine Learning</b></a> <br>
<a href="//www.evademl.org/"><b>Security for Machine Learning</b> (EvadeML)</a><br>
<a href="/pointwise-paraphrase-appraisal-is-potentially-problematic/">NLP Robustness</a></p>
</div>
<div class="column small-14 medium-7">
<div class="mainsection">Past Projects</div>
<em>
<a href="//securecomputation.org">Secure Multi-Party Computation</a></em>:
<a href="//www.oblivc.org/">Obliv-C</a> &middot; <a href="//www.mightbeevil.org/">MightBeEvil</a><br>
<p><em>Web and Mobile Security</em>: <a href="http://www.scriptinspector.org/">ScriptInspector</a> 路
<a href="http://www.ssoscan.org/">SSOScan</a><br>
<em>Program Analysis</em>: <a href="//www.splint.org/">Splint</a> 路 <a href="//www.cs.virginia.edu/perracotta">Perracotta</a><br>
<a href="//www.cs.virginia.edu/nvariant/">N-Variant Systems</a> 路
<a href="//www.cs.virginia.edu/physicrypt/">Physicrypt</a> 路
<a href="//www.cs.virginia.edu/evans/research.html">More&hellip;</a></p>
</p>
</div>
</div>

        
    
  

    <div class="mainsection">Recent Posts</div>

    
    <h2><a href="/balancing-tradeoffs-between-fickleness-and-obstinacy-in-nlp-models/">Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-11-13 00:00:00 &#43;0000 UTC" itemprop="datePublished">13 November 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/hannah-chen">Hannah Chen</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yangfeng-ji">Yangfeng Ji</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-examples">adversarial examples</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/nlp">NLP</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Our work on balanced adversarial training looks at how to train models
that are robust to two different types of adversarial examples:</p>
<p><a href="https://hannahxchen.github.io/">Hannah Chen</a>, <a href="http://yangfengji.net/">Yangfeng
Ji</a>, <a href="http://www.cs.virginia.edu/~evans/">David
Evans</a>. <em>Balanced Adversarial
Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP
Models</em>. In <a href="https://2022.emnlp.org/"><em>The 2022 Conference on Empirical Methods in Natural
Language Processing</em></a> (EMNLP), Abu Dhabi,
7-11 December 2022.  [<a href="https://arxiv.org/abs/2210.11498">ArXiv</a>]</p>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/xQH51lIVDyY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>
<h2 id="adversarial-examples">Adversarial Examples</h2>
<p>At the broadest level, an adversarial example is an input crafted intentionally to confuse a model. However, most work focus on the defintion as an input constructed by applying a small perturbation that preserves the ground truth label but changes model&rsquo;s output <a href="https://arxiv.org/abs/1412.6572">(Goodfellow et al., 2015)</a>. We refer it as a <strong>fickle adversarial example</strong>. On the other hand, attackers can target an opposite objective where the inputs are made with minimal changes that change the ground truth labels but retain model&rsquo;s predictions <a href="https://arxiv.org/abs/1811.00401">(Jacobsen et al., 2018)</a>. We refer these malicious inputs as <strong>obstinate adversarial examples</strong>.</p>
<p>
    <center>
    <a href="/images/bat/image_AEs.png"><img src="/images/bat/image_AEs.png" width="60%" align="center"></a>
    <br>
    <em>Adversarial examples for images</em>
    </center>
</p>
<p>
    <center>
    <a href="/images/bat/nlp_AEs.png"><img src="/images/bat/nlp_AEs.png" width="50%" align="center"></a>
    <br>
    <em>Adversarial examples for texts (<span style="color:red;">Red</span>: synonym substitution, <span style="color:blue;">Blue</span>: antonym substitution)</em>
    </center>
</p>
<h2 id="distance-oracle-misalignment">Distance-Oracle Misalignment</h2>
<p>Previous work from <a href="https://arxiv.org/abs/2002.04599">(Tramer et al., 2020)</a> show that for image classification models, increasing robustness against fickle adversarial examples may also increase vulnerability to obstinate adversarial attacks. They suggested the reason behind this is may be the <em>distance-oracle misalignment</em> during fickle adversarial training. The norm bounded perturbation used for certified robust training may not align with the ground truth decision boundary. We hypothesize that this phenomenon may also exist in NLP models since the automatically-generated adversarial examples for NLP models can be imperfect sometimes, e.g., synonym word substitutions for constructing fickle adversarial examples may not preserve the ground truth label of the input.</p>
<center>
<a href="/images/bat/distance_misalignment.png"><img src="/images/bat/distance_misalignment.png" width="50%" align="center"></a>
</center>
<h2 id="robustness-tradeoffs">Robustness Tradeoffs</h2>
<p>To test our hypothsis, we perform obstinate adversarial attacks on models trained with normal training and fickle adversarial training. We use antonym word substitution for obstinate attack and SAFER <a href="https://arxiv.org/abs/2005.14424">(Ye et al., 2020)</a>, a certified robust training for NLP models, as the fickle adversarial defense. We visualize the antonym attack success rate on models trained with SAFER at each training epoch. We found that as the synonym attack success rate decreases over the course of training, the antonym attack success rate increases as well. The antonym attack success rate is also higher than the normal training baseline. This results prove our hypothesis that optimizing only fickle adversarial robustness can result in models being more vulnerable to obstinate adversarial examples.</p>
<center>
<a href="/images/bat/robustness-tradeoffs.png"><img src="/images/bat/robustness-tradeoffs.png" width="80%" align="center"></a>
</center>
<h2 id="balanced-adversarial-training-bat">Balanced Adversarial Training (BAT)</h2>
<p>We adapt constrastive learning by pairing fickle adversarial examples with the original examples as positive pairs and obstinate adversarial examples with the original examples as negative pairs. The goal of training is to minimize the distance between the postive pairs and maximize the distance between the negative pairs. We propose BAT-Pairwise and BAT-Triplet, where each combines a normal training objective with a pairwise or triplet loss.</p>
<center>
<a href="/images/bat/bat.png"><img src="/images/bat/bat.png" width="80%" align="center"></a>
</center>
<br>
<p>We evaluate BAT based on synonym (fickle) and antonym (obstinate) attack success rate and compare it with normal training, and two fickle adversarial defenses, A2T (vanilla adversarial training) <a href="https://arxiv.org/abs/2109.00544">(Yoo and Qi, 2021)</a> and SAFER (certified robust training). We show that both BAT-Pairwise and BAT-Triple result in better robustness against antonym attacks compared to other training baselines and are more robust against synonym attacks than the normal training method. While fickle adversarial defenses (A2T and SAFER) perform best when evaluated solely based on fickleness robustness, they have worse obstinacy robustness. Our proposed method gives a better balance between the two types of robustness.</p>
<center>
<a href="/images/bat/bat-results.png"><img src="/images/bat/bat-results.png" width="80%" align="center"></a>
</center>
<br>
<p>We compare the learned representations of models trained with BAT and other training baselines. We project the embeddings to 2 dimensional space with t-SNE. We see that boh fickle and obstinate examples are close to the original examples when the model is trained with normal training or SAFER. With BAT-Pairwise and BAT-Triplet, only the fickle examples and the original examples are close to each other while the obstinate examples are further away from them. This results match with BAT&rsquo;s training goal and show that BAT can mitigate the distance-oracle misalignment.</p>
<center>
<a href="/images/bat/tsne.png"><img src="/images/bat/tsne.png" width="80%" align="center"></a>
</center>
<h2 id="summary">Summary</h2>
<p>We show that robustness tradeoffs between ficklenss and obstinacy exist in NLP models. To counter this, we propose Balanced Adversarial Training (BAT) and show that it helps increase robustness against both fickle and obstinate adversarial examples.</p>
<p><b>Paper:</b> [<a href="https://arxiv.org/abs/2210.11498">ArXiv</a>]</p>
<p><b>Code:</b> <a href="https://github.com/hannahxchen/balanced-adversarial-training">https://github.com/hannahxchen/balanced-adversarial-training</a></p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/best-submission-award-at-visxai-2022/">Best Submission Award at VISxAI 2022</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-10-18 00:00:00 &#43;0000 UTC" itemprop="datePublished">18 October 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/evan-rose">Evan Rose</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-syua">Fnu Syua</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning-attacks">poisoning attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p><a href="https://uvasrg.github.io/poisoning/"><em>Poisoning Attacks and Subpopulation Susceptibility</em></a> by Evan Rose, Fnu Suya, and David Evans won the Best Submission Award at the <a href="https://visxai.io/">5th Workshop on Visualization for AI Explainability</a>.</p>
<p>Undergraduate student Evan Rose led the work and presented it at VISxAI in Oklahoma City, 17 October 2022.</p>
<center>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Congratulations to <a href="https://twitter.com/hashtag/VISxAI?src=hash&amp;ref_src=twsrc%5Etfw">#VISxAI</a>&#39;s Best Submission Awards:<br><br> K-Means Clustering: An Explorable Explainer by <a href="https://twitter.com/yizhe_ang?ref_src=twsrc%5Etfw">@yizhe_ang</a> <a href="https://t.co/BULW33WPzo">https://t.co/BULW33WPzo</a><br><br> Poisoning Attacks and Subpopulation Susceptibility by Evan Rose, <a href="https://twitter.com/suyafnu?ref_src=twsrc%5Etfw">@suyafnu</a>, and <a href="https://twitter.com/UdacityDave?ref_src=twsrc%5Etfw">@UdacityDave</a> <a href="https://t.co/Z12D3PvfXu">https://t.co/Z12D3PvfXu</a><a href="https://twitter.com/hashtag/ieeevis?src=hash&amp;ref_src=twsrc%5Etfw">#ieeevis</a></p>&mdash; VISxAI (@VISxAI) <a href="https://twitter.com/VISxAI/status/1582085676857577473?ref_src=twsrc%5Etfw">October 17, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Next up is best submission award  winner, &quot;Poisoning Attacks and Subpopulation Susceptibility&quot; by Evan Rose, <a href="https://twitter.com/suyafnu?ref_src=twsrc%5Etfw">@suyafnu</a>, and <a href="https://twitter.com/UdacityDave?ref_src=twsrc%5Etfw">@UdacityDave</a>.<br><br>Tune in to learn why some data subpopulations are more vulnerable to attacks than others!<a href="https://t.co/Z12D3PvfXu">https://t.co/Z12D3PvfXu</a><a href="https://twitter.com/hashtag/ieeevis?src=hash&amp;ref_src=twsrc%5Etfw">#ieeevis</a> <a href="https://twitter.com/hashtag/VISxAI?src=hash&amp;ref_src=twsrc%5Etfw">#VISxAI</a> <a href="https://t.co/Gm2JBpWQSP">pic.twitter.com/Gm2JBpWQSP</a></p>&mdash; VISxAI (@VISxAI) <a href="https://twitter.com/VISxAI/status/1582117943889969153?ref_src=twsrc%5Etfw">October 17, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>
      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/visualizing-poisoning/">Visualizing Poisoning</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-07-26 00:00:00 &#43;0000 UTC" itemprop="datePublished">26 July 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/evan-rose">Evan Rose</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning-attacks">poisoning attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p><em>How does a poisoning attack work and why are some groups more
susceptible to being victimized by a poisoning attack?</em></p>
<p>We&rsquo;ve posted work that helps understand how poisoning attacks work
with some engaging visualizations:</p>
<p><a href="/poisoning">Poisoning Attacks and Subpopulation Susceptibility</a><br>
<em>An Experimental Exploration on the Effectiveness of Poisoning Attacks</em><br>
Evan Rose, Fnu Suya, and David Evans</p>
<center>
<a href="/poisoning"><img src="/images/visualizingpoisoning.png" width="85%"></a><br>
Follow <a href="/poisoning">the link</a> to try the interactive version!
</center>
<h1 id="heading"></h1>
<p>Machine learning is susceptible to poisoning attacks in which
adversaries inject maliciously crafted training data into the training
set to induce specific model behavior. We focus on subpopulation
attacks, in which the attacker&rsquo;s goal is to induce a model that
produces a targeted and incorrect output (label blue in our demos) for
a particular subset of the input space (colored orange). We study the
question, which subpopulations are the most vulnerable to an attack
and why?</p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/congratulations-dr.-zhang/">Congratulations, Dr. Zhang</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-07-22 00:00:00 &#43;0000 UTC" itemprop="datePublished">22 July 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversary-machine-learning">adversary machine learning</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Congratulations to Xiao Zhang for successfully defending his PhD thesis!</p>
<center>
<img src="/images/xiaozhangphd.png" width="75%">
<div class="caption">
Dr. Zhang and his PhD committee: Somesh Jha (University of Wisconsin), David Evans, Tom Fletcher; Tianxi&nbsp;Li&nbsp;(UVA&nbsp;Statistics), David&nbsp;Wu&nbsp;(UT&nbsp;Austin), Mohammad&nbsp;Mahmoody; Xiao&nbsp;Zhang.
</center>
<h2 id="heading"></h2>
<p>Xiao will join the <a href="https://cispa.de/en">CISPA Helmholtz Center for Information
Security</a> in Saarbr眉cken, Germany this fall as a
tenure-track faculty member.</p>
<h2 id="heading-1"></h2>
<center>
<em>From Characterizing Intrinsic Robustness to Adversarially Robust Machine Learning</em>
</center>
<h2 id="heading-2"></h2>
<p>The prevalence of adversarial examples raises questions about the
reliability of machine learning systems, especially for their
deployment in critical applications. Numerous defense mechanisms have
been proposed that aim to improve a machine learning systems
robustness in the presence of adversarial examples. However, none of
these methods are able to produce satisfactorily robust models, even
for simple classification tasks on benchmarks. In addition to
empirical attempts to build robust models, recent studies have
identified intrinsic limitations for robust learning against
adversarial examples. My research aims to gain a deeper understanding
of why machine learning models fail in the presence of adversaries and
design ways to build better robust systems. In this dissertation, I
develop a concentration estimation framework to characterize the
intrinsic limits of robustness for typical classification tasks of
interest. The proposed framework leads to the discovery that compared
with the concentration of measure which was previously argued to be an
important factor, the existence of uncertain inputs may explain more
fundamentally the vulnerability of state-of-the-art
defenses. Moreover, to further advance our understanding of
adversarial examples, I introduce a notion of representation
robustness based on mutual information, which is shown to be related
to an intrinsic limit of model robustness for downstream
classification tasks. Finally in this dissertation, I advocate for a
need to rethink the current design goal of robustness and shed light
on ways to build better robust machine learning systems, potentially
escaping the intrinsic limits of robustness.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/biml-what-machine-learnt-models-reveal/">BIML: What Machine Learnt Models Reveal</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-07-19 00:00:00 &#43;0000 UTC" itemprop="datePublished">19 July 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/biml">BIML</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/gary-mcgraw">Gary McGraw</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>I gave a talk in the <a href="https://berryvilleiml.com/">Berryville Institute of Machine Learning in the Barn</a> series on <em>What Machine Learnt Models Reveal</em>, which is now available as an edited video:</p>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/zMM_y6VWSgA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>
<blockquote>
<p>David Evans, a professor of computer science researching security and privacy at the University of Virginia, talks about data leakage risk in ML systems and different approaches used to attack and secure models and datasets. Juxtaposing adversarial risks that target records and those aimed at attributes, David shows that differential privacy cannot capture all inference risks, and calls for more research based on privacy experiments aimed at both datasets and distributions.</p>
</blockquote>
<p>The talk is mostly about inference privacy work done by Anshuman Suri and Bargav Jayaraman.</p>

      </div>
<hr class="post-separator"></hr>

    


    <footer>
      <nav>
	<a href="/post/" class="button hollow primary">All Posts</a>
      </nav>
    </footer>
   </div>
    </div>

    <div class="column small=7 medium-3">
    <div class="sidebar">
<center>                  <img src="/images/srg-logo-scaled.png" width=200 height=200 alt="SRG Logo">
      University of Virginia <br>
Security Research Group
</center>

</p>
   <p>
   <a href="/team"><b>Team</b></a></br>
   <a href="//www.cs.virginia.edu/evans/pubs"><b>Publications</b></a><br>
      <a href="/videos"><b>Videos</b></a><br>
   </p>
<p class="nogap">
     <p>
   <a href="https://teams.microsoft.com/l/team/19%3aWdkw2xYq6taXh-0OftqQdt8SQ2vyvUI_Z0ZL39APghY1%40thread.tacv2/conversations?groupId=58076b41-c835-4a07-abaa-705bc7cca101&tenantId=7b3480c7-3707-4873-8b77-e216733a65ac"><b>Join Teams Group</b></a>
   </p>

  <a href="/studygroups/"><b>Study Groups</b></a>

  

<p class="nogap"></p>
  <p>
   <b><a href="/post/">Recent News</a></b>
   </p>
   
   <div class="posttitle">
      <a href="/balancing-tradeoffs-between-fickleness-and-obstinacy-in-nlp-models/">Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models</a>


   </div>
   
   <div class="posttitle">
      <a href="/best-submission-award-at-visxai-2022/">Best Submission Award at VISxAI 2022</a>


   </div>
   
   <div class="posttitle">
      <a href="/visualizing-poisoning/">Visualizing Poisoning</a>


   </div>
   
   <div class="posttitle">
      <a href="/congratulations-dr.-zhang/">Congratulations, Dr. Zhang</a>


   </div>
   
   <div class="posttitle">
      <a href="/biml-what-machine-learnt-models-reveal/">BIML: What Machine Learnt Models Reveal</a>


   </div>
   
   <div class="posttitle">
      <a href="/iclr-2022-understanding-intrinsic-robustness-using-label-uncertainty/">ICLR 2022: Understanding Intrinsic Robustness Using Label Uncertainty</a>


   </div>
   
   <div class="posttitle">
      <a href="/microsoft-research-summit-surprising-and-unsurprising-inference-risks-in-machine-learning/">Microsoft Research Summit: Surprising (and unsurprising) Inference Risks in Machine Learning</a>


   </div>
   
   <div class="posttitle">
      <a href="/uva-news-article/">UVA News Article</a>


   </div>
   
   <div class="posttitle">
      <a href="/model-targeted-poisoning-attacks-with-provable-convergence/">Model-Targeted Poisoning Attacks with Provable Convergence</a>


   </div>
   
   <div class="posttitle">
      <a href="/on-the-risks-of-distribution-inference/">On the Risks of Distribution Inference</a>


   </div>
   
   <div class="posttitle">
      <a href="/chinese-translation-of-mpc-book/">Chinese Translation of MPC Book</a>


   </div>
   
   <div class="posttitle">
      <a href="/iclr-dpml-2021-inference-risks-for-machine-learning/">ICLR DPML 2021: Inference Risks for Machine Learning</a>


   </div>
   
   <div class="posttitle">
      <a href="/how-to-hide-a-backdoor/">How to Hide a Backdoor</a>


   </div>
   
   <div class="posttitle">
      <a href="/codaspy-2021-keynote-when-models-learn-too-much/">Codaspy 2021 Keynote: When Models Learn Too Much</a>


   </div>
   
   <div class="posttitle">
      <a href="/crysp-talk-when-models-learn-too-much/">CrySP Talk: When Models Learn Too Much</a>


   </div>
   
   <div class="posttitle">
      <a href="/improved-estimation-of-concentration-iclr-2021/">Improved Estimation of Concentration (ICLR 2021)</a>


   </div>
   
   <div class="posttitle">
      <a href="/virginia-consumer-data-protection-act/">Virginia Consumer Data Protection Act</a>


   </div>
   
   <div class="posttitle">
      <a href="/algorithmic-accountability-and-the-law/">Algorithmic Accountability and the Law</a>


   </div>
   
   <div class="posttitle">
      <a href="/microsoft-security-data-science-colloquium-inference-privacy-in-theory-and-practice/">Microsoft Security Data Science Colloquium: Inference Privacy in Theory and Practice</a>


   </div>
   
   <div class="posttitle">
      <a href="/fact-checking-donald-trumps-tweet-firing-christopher-krebs/">Fact-checking Donald Trumps tweet firing Christopher Krebs</a>


   </div>
   
   <div class="posttitle">
      <a href="/voting-security/">Voting Security</a>


   </div>
   
   <div class="posttitle">
      <a href="/merlin-morgan-and-the-importance-of-thresholds-and-priors/">Merlin, Morgan, and the Importance of Thresholds and Priors</a>


   </div>
   
   <div class="posttitle">
      <a href="/intrinsic-robustness-using-conditional-gans/">Intrinsic Robustness using Conditional GANs</a>


   </div>
   
   <div class="posttitle">
      <a href="/robustrepresentations/">Adversarially Robust Representations</a>


   </div>
   
   <div class="posttitle">
      <a href="/hybrid-batch-attacks-at-usenix-security-2020/">Hybrid Batch Attacks at USENIX Security 2020</a>


   </div>
   
<p></p>
   <div class="posttitle"><a href="/post/">Older Posts</a></div>
  <div class="posttitle"><a href="/tags">Posts by Tag</a></div>
  <div class="posttitle"><a href="/categories/">Posts by Category</a></div>
  <div class="posttitle"><a href="2017.html">Old Blog</a></div>
<p></p>
<p>
<a href="/awards/"><b>Awards</b></a>
</p>

   <p>
Director: <a href="//www.cs.virginia.edu/evans">David Evans</a>
   </p>

<p>

  </p>

<p><br></br></p>

   <p>
     <center>
       <img src="/images/uva_primary_rgb_white.png" width="80%">
       </center>
</p>

    </div>
</div>

   </div>


    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
