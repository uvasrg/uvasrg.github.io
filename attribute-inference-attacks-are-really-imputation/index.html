<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Attribute Inference attacks are really Imputation | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      



	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      
<div style="margin-top:16px; margin-left: auto; margin-right: auto; max-width: 800px;">
    <article class="article" itemscope itemtype="http://schema.org/Article">
      
      <h1 itemprop="name">Attribute Inference attacks are really Imputation</h1>
      <div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-12-07 00:00:00 &#43;0000 UTC" itemprop="datePublished">7 December 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bargav-jayaraman">Bargav Jayaraman</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/attribute-inference">attribute inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p><strong>Post by Bargav Jayaraman</strong></p>
<p><em>Attribute inference</em> attacks have been shown by prior works to pose privacy threat against ML models. However, these works assume the knowledge of the training distribution and we show that in such cases these attacks do no better than a data imputataion attack that does not have access to the model. We explore the attribute inference risks in the cases where the adversary has limited or no prior knowledge of the training distribution and show that our white-box attribute inference attack (that uses neuron activations to infer the unknown sensitive attribute) surpasses imputation in these data constrained cases. This attack uses the training distribution information leaked by the model, and thus poses privacy risk when the distribution is private.</p>
<center>
<a href="/images/ai/ai.pdf"><img alt="" src="/images/ai/ai.png" width="60%"></a>
</center>
<br>
<h2 id="prior-attribute-inference-attacks-do-not-pose-privacy-risk">Prior Attribute Inference Attacks Do Not Pose Privacy Risk</h2>
<p>Prior works in attribute inference have mainly considered black-box
access to the machine learning model and show successful attribute
inference (in terms of attack accuracy) in the case where the
adversary has access to the underlying training distribution. Our
experiments show that in such cases even an imputation adversary,
without access to the model, can achieve high inference accuracy, as
shown in the table below:</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th>           </th>
          <th style="text-align: center">Census (Race)</th>
          <th>           </th>
          <th style="text-align: center">Texas-100X (Ethnicity)</th>
          <th>             </th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Predict Most Common</td>
          <td></td>
          <td style="text-align: center">0.78</td>
          <td></td>
          <td style="text-align: center">0.72</td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">Imputation Attack</td>
          <td></td>
          <td style="text-align: center">0.82</td>
          <td></td>
          <td style="text-align: center">0.72</td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">Yeom et al. Attack</td>
          <td></td>
          <td style="text-align: center">0.65</td>
          <td></td>
          <td style="text-align: center">0.58</td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">Mehnaz et al. Attack</td>
          <td></td>
          <td style="text-align: center">0.06</td>
          <td></td>
          <td style="text-align: center">0.60</td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">WCAI (Our version of Yeom)</td>
          <td></td>
          <td style="text-align: center">0.83</td>
          <td></td>
          <td style="text-align: center">0.74</td>
          <td></td>
      </tr>
  </tbody>
</table>
<center>Comparing accuracy of attribute inference attacks</center>
</br>
<p>Thus, these attribute inference attacks seem to not pose any
significant privacy risk as the adversary can have similar attack
success without access to the model.</p>
<h2 id="sensitive-value-inference">Sensitive Value Inference</h2>
<p>Attribute inference risk is inherently asymmetric &mdash; identifying a record with minority attribute value (such as <em>Hispanic</em> ethnicity) does not have the same risk as identifying a record with majority attribute value (such as <em>Non-Hispanic</em> ethnicity). Accuracy metric does not capture this. Moreover, attribute inference definition considered by prior works also fails to distinguish these cases. We propose studying a fine-grained version of attribute inference, called <em>sensitive value inference</em>, that considers the attack success in inferring a particular sensitive attribute outcome.</p>
<center>
<a href="/images/ai/svi.pdf"><img alt="Sensitive Value Inference" src="/images/ai/svi.png" width="60%"></a>
</center>
<p>We measure the attack success by evaluating the positive predictive value (PPV) of the inference attack in predicting the top-k candidate records with the sensitive outcome. The PPV values are between 0 and 1, where a higher value denotes a greater attack precision.</p>
<h2 id="the-neuron-output-attack">The Neuron Output Attack</h2>
<p>Our novel neuron output based white-box attack finds the neurons that are most correlated with the sensitive value. For this attack, the adversary selects records from a hold-out set, sets the unknown target attribute to the sensitive value, and queries the model. The adversary then identifies the set of neurons that have higher activations on average for the records with the sensitive value as the ground-truth. The adversary then uses the aggregate output of these neurons to identify the candidate records with sensitive value.</p>
<center>
<A href="/images/ai/wb.pdf"><img alt="" src="/images/ai/wb.png" width="55%"></a>
</center>
</br>
<h2 id="model-leaks-distribution-information">Model Leaks Distribution Information</h2>
<p>In our experiments, we vary the distribution available to the adversary and also the amount of data from the respective distribution the adversary has to train the inference attack. When the adversary has access to &gt;5000 records from the training distribition (not the same as the training set records), imputataion outperforms all the attribute inference attacks (incuding our white-box neuron output attack). As we decrease the known set size to 500 and 50, the imputation PPV decreases drastically whereas our neuron output attack continues to achieve high PPV. Thus the attack is able to take advantage of the training distribution information leaked by the model. The figure below depicts the case where the adversary has 500 records from the training distribution, and as shown, the neuron output attack surpasses the imputataion.</p>
<center>
<A href="/images/ai/img2.pdf"><img alt="" src="/images/ai/img2.png" width="80%"></a>
<div class="caption"><center>
Neurons correlated to Hispanic ethnicity for a neural network model trained on Texas-100X data set.
</center></div>
</center>
</br>
<p>We observe similar trend across different distribution settings and across different data sets. Detailed results can be found in the paper.</p>
<h2 id="differential-privacy-doesnt-mitigate-the-risk">Differential Privacy Doesn&rsquo;t Mitigate the Risk</h2>
<p>Prior works have claimed that attribute inference attacks cannot work in the cases where membership inference attacks do not succeed. Hence, some have thought that differential privacy mechanisms which successfully defend against membership inference attacks, also defend against attribute inference attacks. This is based on the <em>attribute advantage</em> metric of Yeom et al. that shows that the difference between the <em>accuracy</em> of inference attack across training and non-training set is bounded by differential privacy. We agree that this is true, as we shown in our experiment results in Table 2 below where the PPV of the neuron output attack is similar across both train and test sets. However, our <em>attribute advantage</em> metric measures the gap between the attack PPV when the adversary has access to the model (i.e., neuron output attack) versus when the adversary does not have model access (i.e., imputataion). As shown in the table below, this is not bounded by differential privacy as the neuron output attack PPV remains more or less the same with or without differential privacy.</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th>      </th>
          <th style="text-align: center">Without DP</th>
          <th style="text-align: center">With DP</th>
          <th>      </th>
          <th style="text-align: center">Train Set</th>
          <th style="text-align: center">Test Set</th>
          <th>           </th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Imputation Attack</td>
          <td></td>
          <td style="text-align: center">0.62</td>
          <td style="text-align: center">0.62</td>
          <td></td>
          <td style="text-align: center">0.62</td>
          <td style="text-align: center">0.63</td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">Neuron Output Attack</td>
          <td></td>
          <td style="text-align: center">0.49</td>
          <td style="text-align: center">0.49</td>
          <td></td>
          <td style="text-align: center">0.49</td>
          <td style="text-align: center">0.48</td>
          <td></td>
      </tr>
  </tbody>
</table>
<center>Impact of Differential Privacy (DP) on the PPV of attacks (see table in paper for error margins). <br> Results show the PPV of attacks in predicting top-100 candidate records.</center>
</br>
<p>Since the risk is due to the model leaking distribution information, it is not mitigated by differential privacy noise.</p>
<h2 id="conclusion">Conclusion</h2>
<p>We show that the attribute inference attacks take advantage of the model leaking sensitive information about the underlying training distribution as opposed to leaking information about individual training records. While this is often considered by researchers to be <strong>not</strong> a privacy risk since the distribution statistics are supposed to be public knowledge, we argue that when the distribution itself is a private information then any such disclosure poses a severe privacy risk. Existing defenses, such as training the model with differential privacy mechanisms, does not mitigate this distribution privacy risk.</p>
<p><strong>Full paper:</strong> Bargav Jayaraman and David Evans. <a href="https://arxiv.org/abs/2209.01292"><em>Are Attribute Inference Attacks Just Imputation?</em></a> (<a href="https://arxiv.org/abs/2209.01292">arXiv</a>). In <a href="https://www.sigsac.org/ccs/CCS2022/"><em>ACM Conference on Computer and Communications Security</em></a> (CCS 2022).</p>
<p><strong>Code:</strong> <a href="https://github.com/bargavj/EvaluatingDPML"><em>https://github.com/bargavj/EvaluatingDPML</em></a></p>
<center><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/iLy0C5DK2T8?controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></center>
<p><strong>Talk Video:</strong> <a href="https://youtu.be/iLy0C5DK2T8"><em>https://youtu.be/iLy0C5DK2T8</em></a></p>

      </div>

      <meta itemprop="wordCount" content="1013">
      <meta itemprop="datePublished" content="2022-12-07">
      <meta itemprop="url" content="//uvasrg.github.io/attribute-inference-attacks-are-really-imputation/">
    </article>

    <ul class="pagination" role="navigation" aria-label="Pagination" style="margin-top:32px;">
      
      <li class="arrow" aria-disabled="true"><a href="//uvasrg.github.io/congratulations-dr.-jayaraman/">&laquo; <em>Previous<span class="show-for-sr"> page</span></em>: Congratulations, Dr. Jayaraman!</a></li>
      
      
      <li class="arrow" aria-disabled="true"><a href="//uvasrg.github.io/cray-distinguished-speaker-on-leaky-models-and-unintended-inferences/"><em>Next<span class="show-for-sr"> page</span></em>: Cray Distinguished Speaker: On Leaky Models and Unintended Inferences&nbsp;&raquo;</a></li>
      
    </ul>
</div>

    </main>
    
    
<footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>


    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
